{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#splunk-connect-for-snmp","title":"Splunk Connect for SNMP","text":"<p>Splunk welcomes your experimentation and feedback. Let your account team know you are testing Splunk Connect for SNMP.</p> <p>Splunk Connect for SNMP is an edge-deployed, containerized, and highly available solution for collecting SNMP data for Splunk Enterprise, Splunk Enterprise Cloud and Splunk Infrastructure Monitoring.</p> <p>SC4SNMP provides context-full information. It not only forwards SNMP data to Splunk, but also integrates the data into meaningful objects. For example, you don\u2019t need to write queries in order to gather information about interfaces of the device, because SC4SNMP does that automatically:</p> <p></p> <p>What makes it easy to visualize the data in Analytics of Splunk:</p> <p></p> <p>Here is a short presentation of how to browse SNMP data in Splunk:</p> <p></p> <p>SC4SNMP can also easily monitor trap events sent by different SNMP devices. Trap events are JSON formatted, and are being stored under <code>netops</code> index.</p> <p></p>"},{"location":"bestpractices/","title":"Troubleshooting","text":""},{"location":"bestpractices/#debug-splunk-connect-for-snmp","title":"Debug Splunk Connect for SNMP","text":""},{"location":"bestpractices/#pieces-of-advice","title":"Pieces of Advice","text":""},{"location":"bestpractices/#check-when-snmp-walk-was-executed-last-time-for-the-device","title":"Check when SNMP WALK was executed last time for the device","text":"<ol> <li>Configure Splunk OpenTelemetry Collector for Kubernetes</li> <li>Go to your Splunk and execute search: <code>index=\"em_logs\"   \"Sending due task\" \"sc4snmp;&lt;IP_ADDRESS&gt;;walk\"</code>  and replace  with the pertinent IP Address. </li> </ol>"},{"location":"bestpractices/#installing-splunk-connect-for-snmp-on-linux-redhat","title":"Installing Splunk Connect for SNMP on Linux RedHat","text":"<p>Installation of RedHat may be blocking ports required by microk8s. Installing microk8s on RedHat  requires checking to see if the firewall is not blocking any of required microk8s ports. </p>"},{"location":"bestpractices/#issues","title":"Issues","text":""},{"location":"bestpractices/#empty-snmp-response-message-problem","title":"\u201cEmpty SNMP response message\u201d problem","text":"<p>In case you see the following line in the worker\u2019s logs:</p> <pre><code>[2022-01-04 11:44:22,553: INFO/ForkPoolWorker-1] Task splunk_connect_for_snmp.snmp.tasks.walk[8e62fc62-569c-473f-a765-ff92577774e5] retry: Retry in 3489s: SnmpActionError('An error of SNMP isWalk=True for a host 192.168.10.20 occurred: Empty SNMP response message')\n</code></pre> <p>that causes infinite retry of walk operation, add <code>worker.ignoreEmptyVarbinds</code> parameter to <code>values.yaml</code> and set it to true.</p> <p>An example configuration for a worker in <code>values.yaml</code> is:</p> <pre><code>worker:\nignoreEmptyVarbinds: true\n</code></pre>"},{"location":"bestpractices/#oid-not-increasing-problem","title":"\u201cOID not increasing\u201d problem","text":"<p>In case you see the following line in worker\u2019s logs:</p> <pre><code>[2022-01-04 11:44:22,553: INFO/ForkPoolWorker-1] Task splunk_connect_for_snmp.snmp.tasks.walk[8e62fc62-569c-473f-a765-ff92577774e5] retry: Retry in 3489s: SnmpActionError('An error of SNMP isWalk=True for a host 192.168.10.20 occurred: OID not increasing')\n</code></pre> <p>that causes infinite retry of walk operation, add <code>worker.ignoreNotIncreasingOid</code> array to <code>values.yaml</code> and fill with the addresses of hosts where the problem appears.</p> <p>An example configuration for a worker in <code>values.yaml</code> is:</p> <pre><code>worker:\nignoreNotIncreasingOid:\n- \"127.0.0.1:164\"\n- \"127.0.0.6\"\n</code></pre> <p>If you put only IP address (ex. <code>127.0.0.1</code>), then errors will be ignored for all of its devices (like <code>127.0.0.1:161</code>,  <code>127.0.0.1:163</code>\u2026). If you put IP address and host structured as <code>{host}:{port}</code>, that means the error will be ignored only for this device.</p>"},{"location":"bestpractices/#walking-a-device-takes-too-much-time","title":"Walking a device takes too much time","text":"<p>Enable small walk functionality with the following instruction: Configure small walk profile. </p>"},{"location":"bestpractices/#an-error-of-snmp-iswalktrue-blocks-traffic-on-sc4snmp-instance","title":"An error of SNMP isWalk=True blocks traffic on SC4SNMP instance","text":"<p>If you see many <code>An error of SNMP isWalk=True</code> errors in logs, that means that there is a connection problem with the hosts you\u2019re polling from. Walk will try to retry multiple times, which will eventually cause a worker to be blocked for the retries time. In this case, you might want to limit the maximum retries time. You can do this by setting the variable <code>worker.walkRetryMaxInterval</code>, for example:</p> <pre><code>worker:\nwalkRetryMaxInterval: 60\n</code></pre> <p>With the configuration from the above, \u2018walk\u2019 will retry exponentially until it reaches 60 seconds.</p>"},{"location":"bestpractices/#snmp-rollover","title":"SNMP Rollover","text":"<p>The Rollover problem is due to the integer value stored (especially when the value is 32-bit) being finite.  When it reaches its maximum, it gets rolled down to 0 again. This causes a strange drop in Analytics data. The most common case of this issue is interface speed on high speed ports. As a solution to this problem, SNMPv2 SMI defined a new object type, counter64, for 64-bit counters (read more about it). Not all the devices support it, but if they do, poll the counter64 type OID instead of the counter32 one.  For example, use <code>ifHCInOctets</code> instead of <code>ifInOctets</code>.</p> <p>If 64-bit counter are not supported on your device, you can write your own Splunk queries that calculate the shift based on maximum integer value + current state. The same works for values big enough that they\u2019re not fitting a 64-bit value. An example for a SPLUNK query like that (interface counter), would be:</p> <pre><code>| streamstats current=f last(ifInOctets) as p_ifInOctets last(ifOutOctets) as p_ifOutOctets by ifAlias             \n| eval in_delta=(ifInOctets - p_ifInOctets)\n| eval out_delta=(ifOutOctets - p_ifOutOctets)\n| eval max=pow(2,64)\n| eval out = if(out_delta&lt;0,((max+out_delta)*8/(5*60*1000*1000*1000)),(out_delta)*8/(5*60*1000*1000*1000))\n| timechart span=5m avg(in) AS in, avg(out) AS out by ifAlias\n</code></pre>"},{"location":"bestpractices/#field-is-immutable-error-during-helm-upgrade","title":"\u201cField is immutable\u201d error during helm upgrade","text":"<pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/charts/splunk-connect-for-snmp/ --namespace=sc4snmp --create-namespace\nError: UPGRADE FAILED: cannot patch \"snmp-splunk-connect-for-snmp-inventory\" with kind Job: Job.batch \"snmp-splunk-connect-for-snmp-inventory\" is invalid: (...) : field is immutable\n</code></pre> <p>The immutable error is due to the limitation placed on an inventory job. As the SC4SNMP requires several checks before applying updates, it is designed to allow changes in the inventory task after 5 minutes. </p> <p>The status of the inventory can be checked with a command:</p> <pre><code>microk8s kubectl -n sc4snmp get pods | grep inventory\n</code></pre> <p>If the command is not empty, wait and execute it again after the inventory job finishes (no longer visible in the output).</p> <p>If the changes are required to be applied immedietly, the previous inventory job can be deleted with the command:</p> <pre><code>microk8s kubectl delete job/snmp-splunk-connect-for-snmp-inventory -n sc4snmp\n</code></pre> <p>The upgrade command can be executed again. </p>"},{"location":"ha/","title":"High Availability","text":""},{"location":"ha/#high-availability-considerations","title":"High Availability Considerations","text":"<p>The SNMP protocol uses UDP as the transport protocol. Network reliability is a constraint. Consider network architecture when designing for high availability:</p> <ul> <li>When using a single node collector, ensure automatic recovery from virtual infrastructure (i.e. VMware, Openstack, etc).</li> <li>When using a multi-node cluster, ensure nodes are not located such that a simple majority of nodes can be lost. For example, consider row, rack, network, power, and storage.</li> <li>When determining the placement of clusters, the closest location by the number of network hops should be utilized.</li> <li>For \u201cdata center\u201d applications, collection should be local to the data center.</li> <li>Consider IP Anycast.</li> </ul>"},{"location":"mib-request/","title":"Request MIB","text":""},{"location":"mib-request/#mib-submission-process","title":"MIB submission process","text":"<p>To achieve human-readable OIDs, the corresponding MIB files are necessary. They are being stored in one of the components of SC4SNMP - the MIB server. </p> <p>The list of currently available MIBs is here: https://pysnmp.github.io/mibs/index.csv</p> <p>An alternative way to check if the MIB you\u2019re interested in is being served is to check the link: <code>https://pysnmp.github.io/mibs/asn1/@mib@</code> where <code>@mib@</code> is the name of MIB (for example <code>IF-MIB</code>). If the file  is downloading, that means the MIB file exists in the mib server.</p>"},{"location":"mib-request/#submit-a-new-mib-file","title":"Submit a new MIB file","text":"<p>In case you want to add a new MIB file to the MIB server, follow the steps:</p> <ol> <li> <p>Create a fork of the https://github.com/pysnmp/mibs repository </p> </li> <li> <p>Put MIB file/s under <code>src/vendor/@vendor_name@</code> where <code>@vendor_name@</code> is the name of the MIB file\u2019s vendor (in case there is no directory of vendors you need, create it by yourself)</p> </li> <li> <p>Create a pull request to a <code>main</code> branch</p> </li> <li> <p>Name the pull request the following way: <code>feat: add @vendor_name@ MIB files</code></p> </li> </ol> <p>An alternative way of adding MIBs to the MIB server is to create an issue on  https://github.com/pysnmp/mibs repository, attaching the files and information about  the vendor.</p>"},{"location":"mib-request/#update-your-instance-of-sc4snmp-with-the-newest-mib-server","title":"Update your instance of SC4SNMP with the newest MIB server","text":"<p>Usually SC4SNMP is released with the newest version of MIB server every time the new MIB files were added. But, if you want to use the newest MIB server right after its released, you can do it manually via the <code>values.yaml</code> file.</p> <ol> <li>Append <code>mibserver</code> config to the values.yaml, with the <code>mibserver.image.tag</code> of a value of the newest <code>mibserver</code>, for ex.:</li> </ol> <pre><code>mibserver:\nimage:\ntag: \"1.14.5\"\n</code></pre> <p>Check all the MIB server releases here.</p> <ol> <li> <p>Run <code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace</code></p> </li> <li> <p>Restart worker-trap and worker-poller deployments:</p> </li> </ol> <pre><code>microk8s kubectl rollout restart deployment snmp-splunk-connect-for-snmp-worker-trap -n sc4snmp\nmicrok8s kubectl rollout restart deployment snmp-splunk-connect-for-snmp-worker-poller -n sc4snmp\n</code></pre>"},{"location":"mib-request/#beta-use-mib-server-with-local-mibs","title":"Beta: use MIB server with local MIBs","text":"<p>From the <code>1.15.0</code> version of the MIB server, there is a way to use local MIB files. This may be useful when your MIB  files are proprietary, or you use SC4SNMP offline - this way you can update necessary MIBs by yourself, without a need of going through the MIB request procedure.</p> <p>In order to add your MIB files to the MIB server in standalone SC4SNMP installation:</p> <ol> <li>Create/Choose a directory on the machine where SC4SNMP is installed. For example: <code>/home/user/local_mibs</code>.</li> <li>Create vendor directories inside. For example, if you have MIB files from <code>VENDOR1</code> and <code>VENDOR2</code>, create <code>/home/user/local_mibs/VENDOR1</code> and <code>/home/user/local_mibs/VENDOR2</code> and put files inside accordingly. Putting wrong  vendor names won\u2019t make compilation fail, this is more for the logging purposes. Segregating your files will make  troubleshooting easier.</li> <li>MIB files should be named the same as the contained MIB module. The MIB module name is specified at the beginning of the MIB file before <code>::= BEGIN</code> keyword.</li> <li>Add following config to the <code>values.yaml</code>:</li> </ol> <pre><code>mibserver:\nlocalMibs:\npathToMibs: \"/home/user/local_mibs\"\n</code></pre> <p>To verify if the process of compilation was completed successfully, check the mibserver logs with:</p> <pre><code>microk8s kubectl logs -f deployments/snmp-mibserver -n sc4snmp\n</code></pre> <p>It creates a Kubernetes pvc with MIB files inside and maps it to MIB server pod. Also, you can change the storageClass and size of persistence according to the <code>mibserver</code> schema: check here. The default persistence size is 1 Gibibyte, so consider reducing or expanding it to the amount you actually need. Whenever you add new MIB files, rollout restart MIB server pods to compile them again:</p> <pre><code>microk8s kubectl rollout restart deployment snmp-mibserver -n sc4snmp\n</code></pre> <p>NOTE: In case of multi-node Kubernetes installation, create pvc beforehand, copy files onto it and add to the MIB server using <code>persistence.existingClaim</code>. If you go with <code>localMibs.pathToMibs</code> solution in case of multi-node installation (with <code>nodeSelector</code> set up to schedule MIB server pods on the same node where the MIB files are), it will work - but when the Node with hostPath mapped fails, you\u2019ll use access to the MIB files on another node.</p>"},{"location":"planning/","title":"Planning","text":""},{"location":"planning/#planning","title":"Planning","text":"<p>Splunk Connect for SNMP (SC4SNMP) is a solution that allows the customer to \"get\" data from network devices and appliances when a more feature-complete solution, such as the Splunk Universal Forwarder, is not available.</p>"},{"location":"planning/#architecture","title":"Architecture","text":"<p>SC4SNMP is deployed using a Kubernetes distribution, typically MicroK8s, that\u2019s designed to be a low-touch experience for integration with sensitive edge network devices. It will typically be deployed in the same network management zone as the monitored devices and separated from Splunk by an existing firewall.</p> <p></p>"},{"location":"planning/#requirements","title":"Requirements","text":"<ul> <li> <p>A supported deployment of MicroK8s</p> </li> <li> <p>16 Core/32 threads x64 architecture server or vm (single instance)     12 GB ram</p> </li> <li> <p>HA Requires 3 or more instances (odd numbers) 8 core/16 thread 16 GB     ram</p> </li> <li> <p>50 GB root mount</p> </li> <li> <p>HTTP access (non-proxy) allowed for the HTTP(s) connection from     SC4SNMP to the Splunk destination.</p> </li> <li> <p>Splunk Enterprise/Cloud 8.x or newer and/or Splunk Infrastructure Monitoring     (SignalFx)</p> </li> </ul>"},{"location":"planning/#planning-infrastructure","title":"Planning Infrastructure","text":"<p>A single installation of Splunk Connect for SNMP (SC4SNMP) on a machine with 16 Core/32 threads x64 and 64 GB RAM will be able to handle up to 1500 SNMP TRAPs per second.</p> <p>A single installation of Splunk Connect for SNMP (SC4SNMP) on a machine with 16 Core/32 threads x64 and 64 GB RAM is able to handle up to 2750 SNMP varbinds per second.  As for events per second visible in Splunk, please remember that a single SC4SNMP event can contain more than one varbind inside - auto aggregation/grouping feature (varbinds which are describing same thing ie. network interface will be grouped in one event).  That is why, depending on configuration, the number of events per second may vary. </p> <p>When planning infrastructure for Splunk Connect for SNMP, (SC4SNMP) note the limitations highlighted above.</p>"},{"location":"releases/","title":"Releases","text":""},{"location":"releases/#base-information","title":"Base Information","text":""},{"location":"releases/#known-issues","title":"Known Issues","text":"<p>List of open known issues is available under Known issue link</p>"},{"location":"releases/#open-issues-to-the-product","title":"Open issues to the product","text":"<p>To open issue for Splunk Connect for SNMP go to github SC4SNMP project and open issue.   </p>"},{"location":"releases/#releases","title":"Releases","text":"<p>To check Splunk Connect for SNMP releases please visit: SC4SNMP Releases</p>"},{"location":"security/","title":"Security","text":""},{"location":"security/#security-considerations","title":"Security Considerations","text":"<p>The SC4SNMP solution implements SNMP in a compatible mode for current and legacy network device gear. SNMP is a protocol widely considered to be risky and requires threat mitigation at the network level.</p> <ul> <li>Do not expose SNMP endpoints to untrusted connections such as the internet or general LAN network of a typical enterprise.</li> <li>Do not allow SNMPv1 or SNMPv2 connections to cross a network zone where a man in the middle interception is possible.</li> <li>Be aware many SNMPv3 devices rely on insecure cryptography including DES, MD5, and SHA. Do not presume SNMPv3 devices and connections are secure by default.</li> <li>When possible use SNMPv3 with the most secure protocol options mutually supported.</li> <li>The default IP of each node should be considered a management interface and should be protected from network access by an untrusted device by a hardware or software firewall. When possible the IP allocated for SNMP communication should not be shared by the management interface.</li> </ul>"},{"location":"small-environment/","title":"Lightweight installation","text":""},{"location":"small-environment/#lightweight-sc4snmp-installation","title":"Lightweight SC4SNMP installation","text":"<p>SC4SNMP can be successfully installed in small environments with 2 CPUs and 4 GB of memory. One important thing to remember is that Splunk OpenTelemetry Collector for Kubernetes cannot be installed in such a small environment along with SC4SNMP. The other difference from normal installation is that the <code>resources</code> limits must be set for Kubernetes pods. See the example of <code>values.yaml</code> with the appropriate resources here.</p> <p>The rest of the installation is the same as in online, or the offline installation.</p> <p>Keep in mind that a lightweight instance of SC4SNMP won\u2019t be able to poll from many devices and may experience delays  if there is frequent polling.</p>"},{"location":"upgrade/","title":"Upgrade SC4SNMP","text":""},{"location":"upgrade/#upgrading-sc4snmp","title":"Upgrading SC4SNMP","text":""},{"location":"upgrade/#upgrade-to-the-latest-version","title":"Upgrade to the latest version","text":"<p>To upgrade SC4SNMP to the latest version, simply run the following command:</p> <pre><code>microk8s helm3 repo update\n</code></pre> <p>Afterwards, run:</p> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <p>SC4SNMP will be upgraded to the newest version. You can see the latest version after hitting the command:</p> <pre><code>microk8s helm3 search repo snmp\n</code></pre> <p>The output looks like that:</p> <pre><code>NAME                                            CHART VERSION   APP VERSION DESCRIPTION                           splunk-connect-for-snmp/splunk-connect-for-snmp 1.6.2           1.6.2       A Helm chart for SNMP Connect for SNMP\n</code></pre> <p>So in this case, the latest version is <code>1.6.2</code> and it will be installed after <code>helm3 upgrade</code> command.</p>"},{"location":"upgrade/#upgrade-to-a-specific-version","title":"Upgrade to a specific version","text":"<p>Alternatively, you can install one of the previous versions, or a development one. You can list all the previous versions with:</p> <pre><code>microk8s helm3 search repo snmp --versions\n</code></pre> <p>And all the development versions:</p> <pre><code>microk8s helm3 search repo snmp --devel\n</code></pre> <p>To upgrade your SC4SNMP instance to any of the listed versions, run <code>helm3 upgrade</code> with the <code>--version</code> flag:</p> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace --version &lt;VERSION&gt;\n</code></pre> <p>For example:</p> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace --version 1.6.3-beta.13\n</code></pre>"},{"location":"configuration/configuring-groups/","title":"Configuring Groups","text":""},{"location":"configuration/configuring-groups/#configuring-groups","title":"Configuring Groups","text":"<p>It is common to configure whole groups of devices instead of just single ones.  SC4SNMP allows both types of configuration. Group consists of many hosts. Each of them is configured in the <code>values.yaml</code>  file, in the <code>scheduler</code> section. After configuring a group, its name can be used in the <code>address</code> field in the inventory record. All settings specified in the inventory record will be assigned to hosts from the given group,  unless specific host configuration overrides it.</p> <ul> <li>Group configuration example and documentation can be found in the Scheduler Configuration page.</li> <li>Use of groups in the inventory can be found in  the Poller Configuration page.</li> </ul> <p>If the host is configured in the group and both the group and the single host are included in the inventory (like in the example below), the configuration for the single host will be ignored in favour of group configuration:</p> <pre><code>scheduler:\ngroups: |\nexample_group_1:\n- address: 10.202.4.202\nport: 161\n- address: 63.2.40.0\nport: 161\n</code></pre> <pre><code>poller:\ninventory: |\naddress,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\nexample_group_1,,2c,public,,,2000,my_profile2,,\n10.202.4.202,,2c,public,,,2000,my_profile1,,\n</code></pre> <p>If the specific host from the group has to be configured separately, first it must be deleted from the group configuration, and then it can be inserted as a new record in the inventory (like in the example below):</p> <pre><code>scheduler:\ngroups: |\nexample_group_1:\n- address: 63.2.40.0\nport: 161\n</code></pre> <pre><code>poller:\ninventory: |\naddress,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\nexample_group_1,,2c,public,,,2000,my_profile2,,\n10.202.4.202,,2c,public,,,2000,my_profile1,,\n</code></pre>"},{"location":"configuration/configuring-profiles/","title":"Configuring Profiles","text":""},{"location":"configuration/configuring-profiles/#configuring-profiles","title":"Configuring profiles","text":"<p>Profiles are the units where you can configure what you want to poll, and then assign them to the device. The definition of profile can be found in the <code>values.yaml</code> file under the <code>scheduler</code> section.</p> <p>Here are the instructions on how to use profiles: Update Inventory and Profile. </p> <p>There are two types of profiles in general:</p> <ol> <li>Static profile - polling starts when the profile is added to the <code>profiles</code> field in the <code>inventory</code> of the device.</li> <li> <p>Smart profile - polling starts when configured conditions are fulfilled, and the device to poll from has <code>smart_profiles</code> enabled in inventory. Smart profiles are useful when we have many devices of a certain kind, and we don\u2019t want to configure each of them individually with static profiles.</p> <p>In order to configure smart profile, do the following:</p> <ol> <li>Choose one of the fields polled from the device, most commonly sysDescr. </li> <li>Set the filter to match all the devices of this kind.</li> <li>Setup polling of the profile by enabling smart profiles for devices you want to be polled.</li> </ol> </li> </ol> <p>The template of the profile looks like the following:</p> <pre><code>scheduler:\nprofiles: |\n#Name of profile\nbasev1:\n# Define frequency for profile\nfrequency: 100\n#Define condition\ncondition:\n# Define type of condition. Allowed value field, base and walk\ntype: field\nfield: \"SNMPv2-MIB.sysDescr\"\n# Define paterns\npatterns:\n- '.*STRING_TO_BE_MATCHED.*'\n#Define varbinds to query\nvarBinds:\n# Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]]\n- ['SNMPv2-MIB']\n- ['SNMPv2-MIB', 'sysName']\n- ['SNMPv2-MIB', 'sysUpTime',0]\n</code></pre> <p>For example, we have configured two profiles. One is smart, and the other one is static:</p> <pre><code>scheduler:\nprofiles: |\nsmart_profile:\nfrequency: 100\ncondition:\ntype: field\nfield: \"SNMPv2-MIB.sysDescr\"\npatterns:\n- '.*linux.*'\nvarBinds:\n- ['SNMPv2-MIB']\n- ['SNMPv2-MIB', 'sysName']\n- ['SNMPv2-MIB', 'sysUpTime',0]\nstatic_profile:\nfrequency: 300\nvarBinds:\n- ['IP-MIB']\n</code></pre> <p>If we want to enable only <code>static_profile</code> polling for the host <code>10.202.4.202</code>, we will configure similar inventory:</p> <pre><code>poller:\ninventory: |\naddress,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n10.202.4.202,,2c,public,,,2000,static_profile,f,\n</code></pre> <p>If we want to enable checking the <code>10.202.4.202</code> device against smart profiles, we need to set <code>smart_profiles</code> to <code>t</code>:</p> <pre><code>poller:\ninventory: |\naddress,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n10.202.4.202,,2c,public,,,2000,,t,\n</code></pre> <p>Then, if the device <code>sysDescr</code> matches the <code>'.*linux.*'</code> filter, the <code>smart_profile</code> profile will be polled.</p>"},{"location":"configuration/configuring-profiles/#varbinds-configuration","title":"varBinds configuration","text":"<p><code>varBinds</code> is short for \u201cvariable binding\u201d in the SNMP. It is the combination of an Object Identifier (OID) and a value.  <code>varBinds</code> are used for defining what OIDs should be requested from SNMP Agents. <code>varBinds</code> is a required  subsection of each profile. The syntax configuration of <code>varBinds</code> looks like the following:</p> <p>[ \u201cMIB-Component\u201d, \u201cMIB object\u201d[Optional], \u201cMIB index number\u201d[Optional]]</p> <ul> <li><code>MIB-Component</code> - The SNMP MIB itself consists of distinct component MIBs, each of which refers to a specific   defined collection of management information that is part of the overall SNMP MIB, eg., <code>SNMPv2-MIB</code>.   If only the <code>MIB-Component</code> is set, then the SC4SNMP will get the whole subtree.</li> <li><code>MIB object</code> -  The SNMP MIB stores only simple data types: scalars and two-dimensional arrays of scalars,   called tables. The keywords SYNTAX, ACCESS, and DESCRIPTION as well as other keywords such as STATUS and   INDEX are used to define the SNMP MIB managed objects. </li> <li><code>MIB index number</code> - Define index number for given MIB Object eg. <code>0</code>.</li> </ul> <p>Example:</p> <pre><code>  varBinds:\n# Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]]\n- ['SNMPv2-MIB']\n- ['SNMPv2-MIB', 'sysName']\n- ['SNMPv2-MIB', 'sysUpTime',0]\n</code></pre>"},{"location":"configuration/configuring-profiles/#static-profile-configuration","title":"Static Profile configuration","text":"<p>Static Profile is used when they are defined on a list of profiles in the inventory configuration in the <code>poller</code>  service Inventory configuration. Static Profiles are executed  even if the SmartProfile flag in inventory is set to false.  To configure Static Profile value needs to be set in the <code>profiles</code> section:</p> <ul> <li><code>ProfileName</code> - define as subsection key in <code>profiles</code>. <ul> <li><code>frequency</code> - define interval between executing SNMP gets in second.  </li> <li><code>varBinds</code> - define var binds to query. </li> </ul> </li> </ul> <p>Example:</p> <pre><code>scheduler:\nprofiles: |\nstatic_profile_example:\nfrequency: 20\nvarBinds:\n- ['SNMPv2-MIB']\n- ['SNMPv2-MIB', 'sysName']\n- ['SNMPv2-MIB', 'sysUpTime',0]\n</code></pre>"},{"location":"configuration/configuring-profiles/#particular-kinds-of-static-profiles","title":"Particular kinds of static profiles","text":"<p>Sometimes static profiles have additional functionalities to be used in specific scenarios. </p>"},{"location":"configuration/configuring-profiles/#walk-profile","title":"WALK profile","text":"<p>If you would like to limit the scope of the walk, you should set one of the profiles in the inventory to point to the profile definition of type <code>walk</code>:</p> <pre><code>scheduler:\nprofiles: |\nsmall_walk:\ncondition: \ntype: \"walk\"\nvarBinds:\n- ['UDP-MIB']\n</code></pre> <p>This profile should be placed in the profiles section of the inventory definition. It will be executed with the frequency defined in <code>walk_interval</code>. If multiple profiles of type <code>walk</code> is placed in profiles, the last one will be used. </p> <p>This is how to use <code>walk</code> profiles:</p> <pre><code>poller:\ninventory: |\naddress,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n10.202.4.202,,2c,public,,,2000,small_walk,,\n</code></pre> <p>NOTE: When small walk is configured, <code>SNMPv2-MIB</code> is enabled by default (we need it to create the state of the device in the database). For example, if you\u2019ve decided to use <code>small_walk</code> from the example above, you\u2019ll be able to poll only <code>UDP-MIB</code>, and <code>SNMPv2-MIB</code> OIDs.</p>"},{"location":"configuration/configuring-profiles/#smartprofile-configuration","title":"SmartProfile configuration","text":"<p>SmartProfile is executed when the SmartProfile flag in inventory is set to true and the condition defined in profile match.  More information about configuring inventory can be found in Inventory configuration.</p> <p>To configure Smart Profile, the following value needs to be set in the <code>profiles</code> section:</p> <ul> <li><code>ProfileName</code> - define as subsection key in <code>profiles</code>. <ul> <li><code>frequency</code> - define an interval between executing SNMP\u2019s gets in second.</li> <li><code>condition</code> - section define conditions to match profile<ul> <li><code>type</code> - key of <code>condition</code> section which defines type of condition. The allowed values are <code>base</code> and <code>field</code> (<code>walk</code> type is also allowed here, but it\u2019s not part of smart profiles).<ul> <li><code>base</code> type of condition will be executed when <code>SmartProfile</code> in inventory is set to true.</li> <li><code>field</code> type of condition will be executed if it matches <code>pattern</code> for defined <code>field</code>. Supported fields are:<ul> <li>\u201cSNMPv2-MIB.sysDescr\u201d</li> <li>\u201cSNMPv2-MIB.sysObjectID\u201d</li> </ul> </li> </ul> </li> <li><code>field</code> Define field name for condition type field. </li> <li><code>pattern</code> Define list of regular expression patterns for MIB object field defined in <code>field</code> section. For example:         - \u201c.linux.\u201c</li> </ul> </li> <li><code>varBinds</code> - define var binds to query. </li> </ul> </li> </ul> <p>Example of <code>base</code> type profile:</p> <pre><code>scheduler:\nprofiles: |\nSmartProfile_base_example:\nfrequency: 100\ncondition: \ntype: \"base\"\nvarBinds:\n- ['SNMPv2-MIB']\n- ['SNMPv2-MIB', 'sysName']\n</code></pre> <p>Example of <code>field</code>  type profile, also called an automatic profile:</p> <pre><code>scheduler:\nprofiles: |\nSmartProfile_field_example:\nfrequency: 100\ncondition: \ntype: \"field\"\nfield: \"SNMPv2-MIB.sysDescr\"\npatterns:\n- '.*STRING_TO_BE_MATCHED.*'\nvarBinds:\n- ['SNMPv2-MIB']\n- ['SNMPv2-MIB', 'sysName']\n</code></pre> <p>NOTE: Be aware that profile changes may not be reflected immediately. It can take up to 1 minute for changes to propagate. In case you changed frequency, or a profile type, the change will be reflected only after the next walk. There is also 5 minute TTL for an inventory pod. Basically, SC4SNMP allows one inventory upgrade and then block updates for the next 5 minutes.</p>"},{"location":"configuration/configuring-profiles/#conditional-profiles","title":"Conditional profiles","text":"<p>There is a way to not explicitly give what SNMP objects we want to poll - only the conditions that must be fulfilled to qualify object for polling.</p> <p>An example of a conditional profile is:</p> <pre><code>IF_conditional_profile:\nfrequency: 30\nconditions:\n- field: IF-MIB.ifAdminStatus\noperation: \"equals\" value: \"up\"\n- field: IF-MIB.ifOperStatus\noperation: \"equals\"\nvalue: \"up\"\nvarBinds:\n- [ 'IF-MIB', 'ifDescr' ]\n- [ 'IF-MIB', 'ifAlias' ]\n- [ 'IF-MIB', 'ifInErrors' ]\n- [ 'IF-MIB', 'ifOutDiscards' ]\n</code></pre> <p>When the such profile is defined and added to a device in an inventory, it will poll all interfaces where <code>ifAdminStatus</code> and <code>ifOperStatus</code> is up. Note that conditional profiles are being evaluated during the walk process (on every <code>walk_interval</code>) and if the status changes in between, the scope of the conditional profile won\u2019t be modified.</p> <p>These are operations possible to use in conditional profiles:</p> <ol> <li><code>equals</code> - value gathered from <code>field</code> is equal to <code>value</code></li> <li><code>gt</code> - value gathered from <code>field</code> is bigger than <code>value</code> (works only for numeric values)</li> <li><code>lt</code> - value gathered from <code>field</code> is smaller than <code>value</code> (works only for numeric values)</li> <li><code>in</code> - value gathered from <code>field</code> is equal to one of the elements provided in <code>value</code>, for ex.:</li> </ol> <pre><code>conditions:\n- field: IF-MIB.ifAdminStatus\noperation: \"in\"\nvalue: - \"down\"\n- 0\n</code></pre> <ol> <li><code>regex</code> - value gathered from <code>field</code> match the pattern provided in <code>value</code>.  You can add options for regular expression after <code>/</code>. Possible options match ones used in mongodb regex operator.</li> </ol> <pre><code>conditions:\n- field: IF-MIB.ifAdminStatus\noperation: \"regex\"\nvalue: \".own/i\"\n</code></pre> <p>To negate operation you can add flag <code>negate_operation: \"true\"</code> to specified <code>field</code>.</p> <pre><code>conditions:\n- field: IF-MIB.ifAdminStatus\noperation: \"equals\" value: \"up\"\nnegate_operation: \"true\"\n</code></pre> <p>It will negate the operator specified in <code>operation</code>. Possible negation:</p> <ol> <li><code>negate_operation + equals</code> - value gathered from <code>field</code> is NOT equal to <code>value</code></li> <li><code>negate_operation + gt</code> - value gathered from <code>field</code> is SMALLER or EQUAL to <code>value</code> (works only for numeric values)</li> <li><code>negate_operation + lt</code> - value gathered from <code>field</code> is BIGGER or EQUAL to <code>value</code> (works only for numeric values)</li> <li><code>negate_operation + in</code> - value gathered from <code>field</code> is NOT equal to any of the elements provided in <code>value</code></li> <li><code>negate_operation + regex</code> - value gathered from <code>field</code> is NOT matching the pattern provided in <code>value</code>. </li> </ol> <p><code>field</code> part of <code>conditions</code> must fulfill the pattern <code>MIB-family.field</code>. Fields must represent textual value (not metric one), you can learn more about it here.</p> <p>You have to explicitly define <code>varBinds</code> (not only the MIB family but also the field to poll), so such config:</p> <pre><code>varBinds:\n- [ 'IF-MIB' ]\n</code></pre> <p>is not correct.</p>"},{"location":"configuration/configuring-profiles/#custom-translations","title":"Custom translations","text":"<p>If the user wants to use custom names/translations of MIB names, it can be configured under the customTranslations section under scheduler config. Translations are grouped by MIB family. In the example below IF-MIB.ifInDiscards will be translated to IF-MIB.myCustomName1:</p> <pre><code>scheduler:\ncustomTranslations:\nIF-MIB:\nifInDiscards: myCustomName1\nifOutErrors: myCustomName2\nSNMPv2-MIB:\nsysDescr: myCustomName3\n</code></pre>"},{"location":"configuration/deployment-configuration/","title":"Deployment","text":""},{"location":"configuration/deployment-configuration/#deployment-configuration","title":"Deployment Configuration","text":"<p><code>values.yaml</code> is the main point of SC4SNMP management. You can check all the default values of Helm dependencies using:</p> <pre><code>microk8s helm3 inspect values splunk-connect-for-snmp/splunk-connect-for-snmp &gt; values.yaml\n</code></pre> <p>The whole file is divided into the following parts:</p> <p>For configuring endpoint for sending SNMP data:</p> <ul> <li><code>splunk</code> - in case you use Splunk Enterprise/Cloud</li> <li><code>sim</code> - in case you use Splunk Observability Cloud. More details: sim configuration</li> </ul> <p>For polling purposes:</p> <ul> <li><code>scheduler</code> - more details: scheduler configuration</li> <li><code>poller</code> - more details: poller configuration</li> </ul> <p>For traps receiving purposes:</p> <ul> <li><code>traps</code> - more details: trap configuration</li> </ul> <p>Shared components:</p> <ul> <li><code>worker</code> - more details: worker configuration</li> <li><code>mongodb</code> - more details: mongo configuration</li> <li><code>redis</code> - more details: redis configuration</li> </ul>"},{"location":"configuration/deployment-configuration/#shared-values","title":"Shared values","text":"<p>All the components have the <code>resources</code> field for adjusting memory resources:</p> <pre><code>  resources:\nlimits:\ncpu: 1000m\nmemory: 2Gi\nrequests:\ncpu: 1000m\nmemory: 2Gi\n</code></pre> <p>More information about the concept of <code>resources</code> can be found in the kuberentes documentation.</p> <p>There is an option to create common annotations across all the services. It can be set by:</p> <pre><code>commonAnnotations:\nannotation_key: annotation_value\n</code></pre>"},{"location":"configuration/mongo-configuration/","title":"Mongo DB","text":""},{"location":"configuration/mongo-configuration/#mongo-db-configuration","title":"Mongo DB Configuration","text":"<p>Mongo DB is used as the database for keeping schedules.</p>"},{"location":"configuration/mongo-configuration/#mongo-db-configuration-file","title":"Mongo DB configuration file","text":"<p>Mongo DB configuration is kept in the <code>values.yaml</code> file in the <code>mongodb</code> section. <code>values.yaml</code> is used during the installation process for configuring kubernetes values.</p> <p>Example:</p> <pre><code>mongodb:\n#Architecture, Architecture for Mongo deployments is immutable to move from standalone to replicaset will require a uninstall.\n# \"replicaset\" for HA or multi node deployments\n# \"standalone\" for single node non HA\n#architecture: \"standalone\"\npdb:\ncreate: true\n#The following requests and limits are appropriate starting points\n#For productions deployments\nresources: limits:\ncpu: 2\nmemory: 2Gi\nrequests:\ncpu: 750m\nmemory: 512Mi    persistence:\nstorageClass: \"microk8s-hostpath\"\nvolumePermissions:\nenabled: true\n</code></pre> <p>It is recommended not to change this setting. If it is necessary to change it, see: MongoDB on Kubernetes </p>"},{"location":"configuration/poller-configuration/","title":"Poller","text":""},{"location":"configuration/poller-configuration/#poller-configuration","title":"Poller Configuration","text":"<p>Poller is a service which is responsible for querying  SNMP devices using the SNMP GET, and the SNMP WALK functionality. Poller executes two main types of tasks:</p> <ul> <li> <p>Walk task - executes SNMP walk. SNMP walk is an SNMP application that uses SNMP GETNEXT requests to  collect SNMP data from the network and infrastructure SNMP-enabled devices, such as switches and routers. It is a time-consuming task, which may overload the SNMP device when executed too often. It is used by the SC4SNMP to collect and push all OID values, which the provided ACL has access to. </p> </li> <li> <p>Get task - it is a lightweight task whose goal is to query a subset of OIDs defined by the customer. The task serves frequent monitoring OIDs, like memory or CPU utilization.  </p> </li> </ul> <p>Poller has an <code>inventory</code>, which defines what and how often SC4SNMP has to poll.</p>"},{"location":"configuration/poller-configuration/#poller-configuration-file","title":"Poller configuration file","text":"<p>The poller configuration is kept in a <code>values.yaml</code> file in the <code>poller</code> section. <code>values.yaml</code> is used during the installation process for configuring Kubernetes values.</p> <p>Poller example configuration:</p> <pre><code>poller:\nusernameSecrets:\n- sc4snmp-hlab-sha-aes\n- sc4snmp-hlab-sha-des\nlogLevel: \"WARN\"\ninventory: |\naddress,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n10.202.4.202,,2c,public,,,2000,,,\n</code></pre> <p>NOTE: header\u2019s line (<code>address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete</code>) is necessary for the correct execution of SC4SNMP. Do not remove it.</p>"},{"location":"configuration/poller-configuration/#define-log-level","title":"Define log level","text":"<p>The log level for poller can be set by changing the value for the key <code>logLevel</code>. The allowed values are: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>.  The default value is <code>WARNING</code>.</p>"},{"location":"configuration/poller-configuration/#define-usernamesecrets","title":"Define usernameSecrets","text":"<p>Secrets are required to run SNMPv3 polling. To add v3 authentication details, create the k8s Secret object: SNMPv3 Configuration, and put its name in <code>poller.usernameSecrets</code>.</p>"},{"location":"configuration/poller-configuration/#append-oid-index-part-to-the-metrics","title":"Append OID index part to the metrics","text":"<p>Not every SNMP metric object is structured the way it has its index as a one of the field value. We can append the index part of OID with:</p> <pre><code>poller:\nmetricsIndexingEnabled: true\n</code></pre> <p>So the following change will make this metric object (derived from the OID <code>1.3.6.1.2.1.6.20.1.4.0.0.443</code>)</p> <pre><code>{\n   frequency: 5\n   metric_name:sc4snmp.TCP-MIB.tcpListenerProcess: 309\n   mibIndex: 0.0.443\n   profiles: generic_switch\n}\n</code></pre> <p>out of this object:</p> <pre><code>{\n   frequency: 5\n   metric_name:sc4snmp.TCP-MIB.tcpListenerProcess: 309\n   profiles: generic_switch\n}\n</code></pre> <p>Not every SNMP metric object is structured the way it has its index as a one of the field value. We can append the index part of OID with:</p> <pre><code>poller:\nmetricsIndexingEnabled: true\n</code></pre>"},{"location":"configuration/poller-configuration/#disable-automatic-polling-of-base-profiles","title":"Disable automatic polling of base profiles","text":"<p>There are two profiles that are being polled by default - so that even without any configuration you can see the data in Splunk. You can disable it with <code>pollBaseProfiles</code> parameter.</p> <pre><code>poller:\npollBaseProfiles: false\n</code></pre>"},{"location":"configuration/poller-configuration/#configure-inventory","title":"Configure inventory","text":"<p>To update inventory, see: Update Inventory and Profile.</p> <p><code>inventory</code> section in <code>poller</code> has the following fields to configure:</p> <ul> <li><code>address</code> [REQUIRED] - IP address which SC4SNMP should connect to collect data from or name of the group of hosts. General information about groups can be found on Configuring Groups page.</li> <li><code>port</code> [OPTIONAL] - SNMP listening port. Default value <code>161</code>.</li> <li><code>version</code> [REQUIRED] - SNMP version, allowed values: <code>1</code>, <code>2c</code> or <code>3</code></li> <li><code>community</code> [OPTIONAL] - SNMP community string, filed is required when <code>version</code> is <code>1</code> or <code>2c</code></li> <li><code>secret</code> [OPTIONAL] - reference to the secret from <code>poller.usernameSecrets</code> that should be used to poll from the device</li> <li><code>security_engine</code> [OPTIONAL] - security engine ID required by SNMPv3. If not provided for version <code>3</code> it is autogenerated.</li> <li><code>walk_interval</code> [OPTIONAL] - interval in seconds for SNMP walk, default value <code>42000</code>. This value needs to be between <code>1800</code> and <code>604800</code></li> <li><code>profiles</code> [OPTIONAL] - list of SNMP profiles used for the device. More than one profile can be added by semicolon  separation eg. <code>profile1;profile2</code>. More about profiles in Profile Configuration</li> <li><code>smart_profiles</code> [OPTIONAL] - enabled smart profiles, by default it\u2019s <code>true</code>. Allowed value: <code>true</code>, <code>false</code>.</li> <li><code>delete</code> [OPTIONAL] - flags which define if inventory should be deleted from scheduled tasks for WALKs and GETs.  Allowed value: <code>true</code>, <code>false</code>. Default value is <code>false</code>.</li> </ul> <p>Example:</p> <pre><code>poller:\ninventory: |\naddress,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n10.202.4.202,,2c,public,,,2000,my_profile1,,\nexample_group_1,,2c,public,,,2000,my_profile2;my_profile3,,\n</code></pre>"},{"location":"configuration/poller-configuration/#update-inventory","title":"Update Inventory","text":"<p>Adding new devices for <code>values.yaml</code> is quite expensive from the Splunk Connect for SNMP perspective.  As it interacts with real, networking devices, it requires several checks before applying changes. SC4SNMP was designed to prevent changes in inventory task more often than every 5 min. </p> <p>To apply inventory changes in <code>values.yaml</code>, the following steps need to be executed:</p> <ol> <li>Edit <code>values.yaml</code> </li> <li>Check if inventory pod is still running by the execute command:</li> </ol> <pre><code>microk8s kubectl -n sc4snmp get pods | grep inventory\n</code></pre> <p>If the command does not return any pods, follow the next step. In another case, wait and execute the command again until the moment  when inventory job finishes. </p> <p>If you really need to apply changes immediately, you can get around the limitation by deleting the inventory job with:</p> <pre><code>microk8s kubectl delete job/snmp-splunk-connect-for-snmp-inventory -n sc4snmp\n</code></pre> <p>After running this command, you can proceed with upgrading without a need to wait.</p> <ol> <li>Run upgrade command :</li> </ol> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <p>NOTE: If you decide to change the frequency of the profile without changing the inventory data, the change will be reflected after  next the walk process for the host. The walk happens every <code>walk_interval</code>, or on any change in inventory.</p>"},{"location":"configuration/poller-configuration/#upgrade-with-the-csv-file","title":"Upgrade with the csv file","text":"<p>There is a possibility to update inventory by making changes outside the <code>values.yaml</code>. It can be put to separate csv file and upgraded passing <code>--set-file poller.inventory=&lt;path_to_file&gt;</code>.</p> <p>Example of the CSV file configuration:</p> <pre><code>address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n10.202.4.202,,2c,public,,,3000,my_profile,,\n</code></pre> <p>Example of upgrade command with the csv file:</p> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml --set-file poller.inventory=inventory.csv splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre>"},{"location":"configuration/redis-configuration/","title":"Redis","text":""},{"location":"configuration/redis-configuration/#redis-configuration","title":"Redis configuration","text":"<p>Recently, RabbitMQ was replaced with Redis as a queue service and periodic task database. The reason for this is to increase SC4SNMP performance and protect against bottlenecks.</p> <p>Redis both manages periodic tasks and queues the SC4SNMP service. It queues tasks like SNMP Walk and Poll.  </p>"},{"location":"configuration/redis-configuration/#redis-configuration-file","title":"Redis configuration file","text":"<p>Redis configuration is kept in the <code>values.yaml</code> file in the <code>redis</code> section. <code>values.yaml</code> is being used during the installation process for configuring Kubernetes values.</p> <p>To edit the configuration, see: Redis on Kubernetes </p>"},{"location":"configuration/scheduler-configuration/","title":"Scheduler","text":""},{"location":"configuration/scheduler-configuration/#scheduler-configuration","title":"Scheduler configuration","text":"<p>The scheduler is a service with is responsible for managing schedules for SNMP walks and GETs. Schedules definition  are stored in Mongo DB. </p>"},{"location":"configuration/scheduler-configuration/#scheduler-configuration-file","title":"Scheduler configuration file","text":"<p>Scheduler configuration is kept in <code>values.yaml</code> file in section <code>scheduler</code>. <code>values.yaml</code> is being used during the installation process for configuring Kubernetes values.</p> <p>Example:</p> <pre><code>scheduler:\nlogLevel: \"WARN\"\nprofiles: |\ntest_profile:\nfrequency: 5 \ncondition: \ntype: \"field\" \nfield: \"SNMPv2-MIB.sysDescr\" \npatterns: \n- \"^.*\"\nvarBinds:\n# Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]]\n- [\"SNMPv2-MIB\", \"sysDescr\",0]\n</code></pre>"},{"location":"configuration/scheduler-configuration/#define-log-level","title":"Define log level","text":"<p>Log level for scheduler can be set by changing the value for key <code>logLevel</code>. Allowed values are: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>.  The default value is <code>WARNING</code></p>"},{"location":"configuration/scheduler-configuration/#define-resource-requests-and-limits","title":"Define resource requests and limits","text":"<pre><code>scheduler:\n#The following resource specification is appropriate for most deployments to scale the\n#Larger inventories may require more memory but should not require additional cpu\nresources:\nlimits:\ncpu: 1\nmemory: 1Gi\nrequests:\ncpu: 200m\nmemory: 128Mi\n</code></pre>"},{"location":"configuration/scheduler-configuration/#define-groups-of-hosts","title":"Define groups of hosts","text":"<p>To get the general idea when groups are useful see Configuring Groups.</p> <p>Example group configuration:</p> <pre><code>scheduler:\ngroups: |\nexample_group_1:\n- address: 123.0.0.1\nport: 161\n- address: 178.8.8.1\nport: 999\n- address: 12.22.23\nport: 161\ncommunity: 'private'\nexample_group_2:\n- address: 103.0.0.1\nport: 1161\nversion: '3'\nsecret: 'my_secret'\n- address: 178.80.8.1\nport: 999\n</code></pre> <p>The one obligatory field for the host configuration is <code>address</code>. If <code>port</code> isn\u2019t configured its default value is <code>161</code>.  Other fields that can be modified here are: <code>community</code>, <code>secret</code>, <code>version</code>, <code>security_engine</code>. However, if they remain unspecified in the host configuration, they will be derived from the inventory record regarding this specific group.</p>"},{"location":"configuration/scheduler-configuration/#define-tasks-expiry-time","title":"Define tasks expiry time","text":"<p>Define time in second after which polling or walk tasks, that haven\u2019t been picked up by the worker, will be revoked. Check the celery documentation for more details.</p> <pre><code>scheduler:\ntasksExpiryTime: 300\n</code></pre>"},{"location":"configuration/sim-configuration/","title":"Splunk Infrastructure Monitoring","text":""},{"location":"configuration/sim-configuration/#otel-and-splunk-observability-cloud-configuration","title":"OTEL and Splunk Observability Cloud configuration","text":"<p>Splunk OpenTelemetry Collector is a component that provides an option to send metrics to Splunk Observability Cloud. In order to use it, you must set <code>enabled</code> flag in <code>values.yaml</code> to <code>true</code>:</p> <pre><code>sim:\n# sim must be enabled if you want to use SignalFx\nenabled: true\n</code></pre>"},{"location":"configuration/sim-configuration/#token-and-realm","title":"Token and realm","text":"<p>You need to specify Splunk Observability Cloud token and realm. There are two ways of configuring them:</p> <ol> <li>Pass those in a plain text via <code>values.yaml</code> so at the end sim element looks like this:</li> </ol> <pre><code>sim:\nenabled: true\nsignalfxToken: BCwaJ_Ands4Xh7Nrg\nsignalfxRealm: us0\n</code></pre> <ol> <li>Alternatively, create microk8s secret by yourself and pass its name in <code>values.yaml</code> file. Create secret:</li> </ol> <pre><code>microk8s kubectl create -n &lt;namespace&gt; secret generic &lt;secretname&gt; \\\n  --from-literal=signalfxToken=&lt;signalfxToken&gt; \\\n  --from-literal=signalfxRealm=&lt;signalfxRealm&gt;\n</code></pre> <p>Modify <code>sim.secret</code> section of <code>values.yaml</code>. Disable creation of the secret with <code>sim.secret.create</code> and provide the <code>&lt;secretname&gt;</code> matching the one from the previous step. Pass it via <code>sim.secret.name</code>. For example, for <code>&lt;secretname&gt;</code>=<code>signalfx</code> the <code>sim</code> section would look like:</p> <pre><code>sim:\nsecret:\ncreate: false\nname: signalfx\n</code></pre> <p>Note: After the initial installation, if you change <code>sim.signalfxToken</code> and/or <code>sim.signalfxRealm</code> and no <code>sim.secret.name</code> is given,  the <code>sim</code> pod will sense the update by itself (after <code>helm3 upgrade</code> command) and trigger the recreation. But, when you edit secret created outside of <code>values.yaml</code> (given by <code>sim.secret.name</code>), you need to roll out the deployment by yourself or delete the pod to update the data.</p>"},{"location":"configuration/sim-configuration/#define-annotations","title":"Define annotations","text":"<p>In case you need to append some annotations to the <code>sim</code> service, you can do it by setting <code>sim.service.annotations</code>, for ex.:</p> <pre><code>sim:\nservice:\nannotations:\nannotation_key: annotation_value\n</code></pre>"},{"location":"configuration/sim-configuration/#verify-the-deployment","title":"Verify the deployment","text":"<p>After executing <code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace</code>, the sim pod should be up and running:</p> <pre><code>splunker@ip-10-202-13-233:~$ microk8s kubectl get pods -n sc4snmp\nNAME                                                      READY   STATUS    RESTARTS   AGE\nsnmp-splunk-connect-for-snmp-scheduler-7ddbc8d75-bljsj        1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-poller-57cd8f4665-9z9vx   1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-sender-5c44cbb9c5-ppmb5   1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-trap-549766d4-28qzh       1/1     Running   0          133m\nsnmp-mibserver-7f879c5b7c-hz9tz                               1/1     Running   0          133m\nsnmp-mongodb-869cc8586f-vvr9f                                 2/2     Running   0          133m\nsnmp-redis-master-0                                           1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-trap-78759bfc8b-79m6d            1/1     Running   0          99m\nsnmp-splunk-connect-for-snmp-sim-59b89747f-kn6tf              1/1     Running   0          32s\n</code></pre>"},{"location":"configuration/snmp-data-format/","title":"SNMP data format","text":""},{"location":"configuration/snmp-data-format/#snmp-data-format","title":"SNMP Data Format","text":"<p>SC4SNMP classifies SNMP data elements as metrics or textual fields. We assume that the metric types are the indicators worth monitoring,  that changes dynamically, and textual fields are the context helpful to understand what an SNMP object really means.</p> <p>SC4SNMP classify the data element as a metric when its type is one of:</p> <ul> <li><code>Unsigned</code></li> <li><code>Counter</code></li> <li><code>TimeTicks</code></li> <li><code>Gauge</code></li> <li><code>Integer</code></li> </ul> <p>Every other type is interpreted as a field value.</p> <p>Sometimes, the MIB file indicates a field as an <code>INTEGER</code>, but there is also some mapping defined, like for example in case of <code>IF-MIB.ifOperStatus</code>:</p> <pre><code>ifOperStatus OBJECT-TYPE\nSYNTAX  INTEGER {\n                up(1),        -- ready to pass packets\ndown(2),\n                testing(3),   -- in some test mode\nunknown(4),   -- status can not be determined\n-- for some reason.\n                dormant(5),\n                notPresent(6),    -- some component is missing\nlowerLayerDown(7) -- down due to state of\n-- lower-layer interface(s)\n}\n</code></pre> <p>source</p> <p>Here we expect some numeric value, but actually what SNMP Agents gets from the device is a <code>string</code> value, like <code>up</code>. To avoid setting textual value as a metrics, SC4SNMP does an additional check and tries to cast the numeric value to float. If the check fails, the values is classified as a textual field.</p> <p>Let\u2019s go through a simple example. We\u2019ve just added a device and didn\u2019t configure anything special. The data from a walk in Splunk\u2019s metrics index is:</p> <pre><code>{\n   ifAdminStatus: up\n   ifDescr: GigabitEthernet1\n   ifIndex: 1\n   ifOperStatus: up\n   ifPhysAddress: 0a:aa:ef:53:67:15\n   ifType: ethernetCsmacd\n   metric_name:sc4snmp.IF-MIB.ifInDiscards: 0\n   metric_name:sc4snmp.IF-MIB.ifInErrors: 0\n   metric_name:sc4snmp.IF-MIB.ifInOctets: 3873878708\n   metric_name:sc4snmp.IF-MIB.ifInUcastPkts: 47512921\n   metric_name:sc4snmp.IF-MIB.ifInUnknownProtos: 0\n   metric_name:sc4snmp.IF-MIB.ifLastChange: 454107556\n   metric_name:sc4snmp.IF-MIB.ifMtu: 1500\n   metric_name:sc4snmp.IF-MIB.ifOutDiscards: 0\n   metric_name:sc4snmp.IF-MIB.ifOutErrors: 0\n   metric_name:sc4snmp.IF-MIB.ifOutOctets: 1738565177\n   metric_name:sc4snmp.IF-MIB.ifOutUcastPkts: 44295751\n   metric_name:sc4snmp.IF-MIB.ifSpeed: 1000000000\n}\n</code></pre> <p>Clearly we can see the textual part:</p> <pre><code>   ifAdminStatus: up\n   ifDescr: GigabitEthernet1\n   ifIndex: 1\n   ifOperStatus: up\n   ifPhysAddress: 0a:aa:ef:53:67:15\n   ifType: ethernetCsmacd\n</code></pre> <p>And a metric one:</p> <pre><code>   metric_name:sc4snmp.IF-MIB.ifInDiscards: 0\n   metric_name:sc4snmp.IF-MIB.ifInErrors: 0\n   metric_name:sc4snmp.IF-MIB.ifInOctets: 3873878708\n   metric_name:sc4snmp.IF-MIB.ifInUcastPkts: 47512921\n   metric_name:sc4snmp.IF-MIB.ifInUnknownProtos: 0\n   metric_name:sc4snmp.IF-MIB.ifLastChange: 454107556\n   metric_name:sc4snmp.IF-MIB.ifMtu: 1500\n   metric_name:sc4snmp.IF-MIB.ifOutDiscards: 0\n   metric_name:sc4snmp.IF-MIB.ifOutErrors: 0\n   metric_name:sc4snmp.IF-MIB.ifOutOctets: 1738565177\n   metric_name:sc4snmp.IF-MIB.ifOutUcastPkts: 44295751\n   metric_name:sc4snmp.IF-MIB.ifSpeed: 1000000000\n</code></pre>"},{"location":"configuration/snmp-data-format/#to-which-splunk-index-will-my-data-go","title":"To which Splunk index will my data go?","text":""},{"location":"configuration/snmp-data-format/#metric-index","title":"Metric index","text":"<p>The rule is, if we poll a profile with AT LEAST one metric value, it will go to the metric index and will be enriched with all the textual fields we have for this object. For example, when polling:</p> <pre><code>profile_with_one_metric:\nfrequency: 100\nvarBinds:\n- ['IF-MIB', 'ifOutUcastPkts']\n- ['IF-MIB', 'ifInUcastPkts']\n</code></pre> <p>The record that we\u2019ll see in Splunk <code>| mpreview index=net*</code> for the same case as above would be:</p> <pre><code>   ifAdminStatus: up\n   ifDescr: GigabitEthernet1\n   ifIndex: 1\n   ifOperStatus: up\n   ifPhysAddress: 0a:aa:ef:53:67:15\n   ifType: ethernetCsmacd\n   metric_name:sc4snmp.IF-MIB.ifOutUcastPkts: 44295751\n   metric_name:sc4snmp.IF-MIB.ifInUcastPkts: 47512921\n</code></pre> <p>Note, that only fields specified in <code>varBinds</code> are actively polled form the device. In case of <code>profile_with_one_metric</code> shown above, the textual fields <code>ifAdminStatus</code>, <code>ifDescr</code>, <code>ifIndex</code>, <code>ifOperStatus</code> and <code>ifPhysAddress</code> are taken from  the database cache, which is updated on every <code>walk</code> process. This is fine for the most of the cases, as things like MAC address, interface type or interface status shouldn\u2019t change frequently if ever.</p> <p>If you want to keep <code>ifOperStatus</code> and <code>ifAdminStatus</code> up to date all the time, define profile like:</p> <pre><code>profile_with_one_metric:\nfrequency: 100\nvarBinds:\n- ['IF-MIB', 'ifOutUcastPkts']\n- ['IF-MIB', 'ifInUcastPkts']\n- ['IF-MIB', 'ifOperStatus']\n- ['IF-MIB', 'ifAdminStatus']\n</code></pre> <p>The result in Splunk will look the same, but <code>ifOperStatus</code> and <code>ifAdminStatus</code> will be actively polled.</p>"},{"location":"configuration/snmp-data-format/#event-index","title":"Event index","text":"<p>It is possible to create an event without a single metric value, in such scenario it will go to an event index. An example of such profile would be:</p> <pre><code>profile_with_only_textual_fields:\nfrequency: 100\nvarBinds:\n- ['IF-MIB', 'ifDescr']\n- ['IF-MIB', 'ifName']\n- ['IF-MIB', 'ifOperStatus']\n</code></pre> <p>In this case no additional enrichment will be done. The events in event index <code>index=netops</code> of Splunk will look like:</p> <pre><code>{ [-]\n   IF-MIB.ifDescr: { [-]\n     name: IF-MIB.ifDescr\n     oid: 1.3.6.1.2.1.2.2.1.2.5\n     time: 1676302789.9729967\n     type: f\n     value: VirtualPortGroup0\n   }\n   IF-MIB.ifName: { [-]\n     name: IF-MIB.ifName\n     oid: 1.3.6.1.2.1.31.1.1.1.1.5\n     time: 1676302789.6655216\n     type: f\n     value: Vi0\n   }\n   IF-MIB.ifOperStatus: { [-]\n     name: IF-MIB.ifOperStatus\n     oid: 1.3.6.1.2.1.2.2.1.8.5\n     time: 1676302789.6655502\n     type: g\n     value: up\n   }\n}\n</code></pre>"},{"location":"configuration/snmpv3-configuration/","title":"SNMPv3 configuration","text":""},{"location":"configuration/snmpv3-configuration/#create-snmp-v3-users","title":"Create SNMP v3 users","text":"<p>Configuration of SNMP v3, when supported by the monitored devices, is the most secure choice available for authentication and data privacy. Each set of credentials will be stored as \u201cSecret\u201d objects in k8s, and will be referenced in values.yaml. This allows the secret to be created once, including automation by third-party password managers, then consumed without storing sensitive data in plain text.</p> <pre><code># &lt;secretname&gt;=Arbitrary name of the secret often the same as the username or prefixed with \"sc4snmp-\"\n# &lt;namespace&gt;=Namespace used to install sc4snmp\n# &lt;username&gt;=the SNMPv3 Username\n# &lt;key&gt;=key note must be at least 8 char long subject to target limitations\n# &lt;authProtocol&gt;=One of SHA (SHA1) or MD5 \n# &lt;privProtocol&gt;=One of AES or DES \n# Note MD5 and DES are considered insecure but must be supported for standards compliance\nmicrok8s kubectl create -n &lt;namespace&gt; secret generic &lt;secretname&gt; \\\n--from-literal=userName=&lt;username&gt; \\\n--from-literal=authKey=&lt;key&gt; \\\n--from-literal=privKey=&lt;key&gt; \\\n--from-literal=authProtocol=&lt;authProtocol&gt; \\\n--from-literal=privProtocol=&lt;privProtocol&gt; </code></pre> <p>Configured credentials can be used in poller and trap services.  In service configuration, <code>secretname</code> needs to be provided. </p>"},{"location":"configuration/step-by-step-poll/","title":"Step by Step polling example","text":""},{"location":"configuration/step-by-step-poll/#an-example-of-a-polling-scenario","title":"An example of a polling scenario","text":"<p>We have 4 hosts we want to poll from:</p> <ol> <li><code>10.202.4.201:161</code></li> <li><code>10.202.4.202:161</code></li> <li><code>10.202.4.203:161</code></li> <li><code>10.202.4.204:163</code></li> </ol> <p>To retrieve data from the device efficiently, first determine the specific data needed. Instead of walking through  the entire <code>1.3.6.1</code>, limit the walk to poll only the necessary data. Configure the <code>IF-MIB</code> family for interfaces and  the <code>UCD-SNMP-MIB</code> for CPU-related statistics. In the <code>scheduler</code> section of <code>values.yaml</code>, define the target group and  establish the polling parameters, known as the profile, to gather the desired data precisely:</p> <pre><code>scheduler:\nlogLevel: \"INFO\"\nprofiles: |\nsmall_walk:\ncondition:\ntype: \"walk\"\nvarBinds:\n- [\"IF-MIB\"]\n- [\"UCD-SNMP-MIB\"]\nswitch_profile:\nfrequency: 60\nvarBinds:\n- [\"IF-MIB\", \"ifDescr\"]\n- [\"IF-MIB\", \"ifAdminStatus\"]\n- [\"IF-MIB\", \"ifOperStatus\"]\n- [\"IF-MIB\", \"ifName\"]\n- [\"IF-MIB\", \"ifAlias\"]\n- [\"IF-MIB\", \"ifIndex\"]\n- [\"IF-MIB\", \"ifInDiscards\"]\n- [\"IF-MIB\", \"ifInErrors\"]\n- [\"IF-MIB\", \"ifInOctets\"]\n- [\"IF-MIB\", \"ifOutDiscards\"]\n- [\"IF-MIB\", \"ifOutErrors\"]\n- [\"IF-MIB\", \"ifOutOctets\"]\n- [\"IF-MIB\", \"ifOutQLen\"]\n- [\"UCD-SNMP-MIB\"]\ngroups: |\nswitch_group:\n- address: 10.202.4.201\n- address: 10.202.4.202\n- address: 10.202.4.203\n- address: 10.202.4.204\nport: 163\n</code></pre> <p>Then it is required to pass the proper instruction of what to do for SC4SNMP instance. This can be done by appending a new row to <code>poller.inventory</code>:</p> <pre><code>poller:\nlogLevel: \"WARN\"\ninventory: |\naddress,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\nswitch_group,,2c,public,,,2000,small_walk;switch_profile,,\n</code></pre> <p>The provided configuration will make:</p> <ol> <li>Walk devices from <code>switch_group</code> with <code>IF-MIB</code> and <code>UCD-SNMP-MIB</code> every 2000 seconds</li> <li>Poll specific <code>IF-MIB</code> fields and the whole <code>UCD-SNMP-MIB</code> every 60 seconds</li> </ol> <p>Note: you could as well limit walk profile even more if you want to enhance the performance.</p> <p>It makes sense to put in the walk the textual values that don\u2019t required to be constantly monitored, and monitor only the metrics you\u2019re interested in:</p> <pre><code>small_walk:\ncondition:\ntype: \"walk\"\nvarBinds:\n- [\"IF-MIB\", \"ifDescr\"]\n- [\"IF-MIB\", \"ifAdminStatus\"]\n- [\"IF-MIB\", \"ifOperStatus\"]\n- [\"IF-MIB\", \"ifName\"]\n- [\"IF-MIB\", \"ifAlias\"]\n- [\"IF-MIB\", \"ifIndex\"]\nswitch_profile:\nfrequency: 60\nvarBinds:\n- [\"IF-MIB\", \"ifInDiscards\"]\n- [\"IF-MIB\", \"ifInErrors\"]\n- [\"IF-MIB\", \"ifInOctets\"]\n- [\"IF-MIB\", \"ifOutDiscards\"]\n- [\"IF-MIB\", \"ifOutErrors\"]\n- [\"IF-MIB\", \"ifOutOctets\"]\n- [\"IF-MIB\", \"ifOutQLen\"]\n</code></pre> <p>Then every metric object will be enriched with the textual values gathered from a walk process. Learn more about SNMP format here.</p> <p>Now we\u2019re ready to reload SC4SNMP. We run the <code>helm3 upgrade</code> command:</p> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <p>We should see the new pod with <code>Running</code> -&gt; <code>Completed</code> state:</p> <pre><code>microk8s kubectl get pods -n sc4snmp -w\n</code></pre> <p>Example output:</p> <pre><code>NAME                                                          READY   STATUS    RESTARTS   AGE\nsnmp-splunk-connect-for-snmp-worker-sender-5bc5cf864b-cwmfw   1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-worker-poller-76dcfb5896-d55pd   1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-worker-trap-68fb6476db-zl9rb     1/1     Running   0          5h52m\nsnmp-mibserver-58b558f5b4-zqf85                               1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-scheduler-57c5878444-k4qv4       1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-worker-poller-76dcfb5896-bzgrm   1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-trap-6cb76fcb49-l62f9            1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-trap-6cb76fcb49-d7c88            1/1     Running   0          5h52m\nsnmp-mongodb-869cc8586f-kw67q                                 2/2     Running   0          5h52m\nsnmp-redis-master-0                                           1/1     Running   0          5h52m\nsnmp-splunk-connect-for-snmp-inventory-g4bs7                  1/1     Running   0          3s\nsnmp-splunk-connect-for-snmp-inventory-g4bs7                  0/1     Completed   0          5s\nsnmp-splunk-connect-for-snmp-inventory-g4bs7                  0/1     Completed   0          6s\nsnmp-splunk-connect-for-snmp-inventory-g4bs7                  0/1     Completed   0          7s\n</code></pre> <p>We can check the pod\u2019s logs to make sure everything was reloaded right, with:</p> <pre><code>microk8s kubectl logs -f snmp-splunk-connect-for-snmp-inventory-g4bs7  -n sc4snmp\n</code></pre> <p>Example output:</p> <pre><code>Successfully connected to redis://snmp-redis-headless:6379/0\nSuccessfully connected to redis://snmp-redis-headless:6379/1\nSuccessfully connected to mongodb://snmp-mongodb:27017\nSuccessfully connected to http://snmp-mibserver/index.csv\n{\"message\": \"Loading inventory from /app/inventory/inventory.csv\", \"time\": \"2022-09-05T14:30:30.605420\", \"level\": \"INFO\"}\n{\"message\": \"New Record address='10.202.4.201' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\", \"time\": \"2022-09-05T14:30:30.607641\", \"level\": \"INFO\"}\n{\"message\": \"New Record address='10.202.4.202' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\", \"time\": \"2022-09-05T14:30:30.607641\", \"level\": \"INFO\"}\n{\"message\": \"New Record address='10.202.4.203' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\", \"time\": \"2022-09-05T14:30:30.607641\", \"level\": \"INFO\"}\n{\"message\": \"New Record address='10.202.4.204' port=163 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\", \"time\": \"2022-09-05T14:30:30.607641\", \"level\": \"INFO\"}\n</code></pre> <p>In some time (depending on how long the walk takes), we\u2019ll see events under:</p> <pre><code>| mpreview index=netmetrics | search profiles=switch_profile\n</code></pre> <p>query in Splunk. When groups are used, we can also use querying by the group name:</p> <pre><code>| mpreview index=netmetrics | search group=switch_group\n</code></pre> <p>Keep in mind, that querying by profiles/group in Splunk is only possible in the metrics index. Every piece of data being sent by SC4SNMP is formed based on the MIB file\u2019s definition of the SNMP object\u2019s index. The object is forwarded to an event index only if it doesn\u2019t have any metric value inside.</p> <p>The <code>raw</code> metrics in Splunk example is:</p> <pre><code>{\n\"frequency\":\"60\",\n\"group\":\"switch_group\",\n\"ifAdminStatus\":\"up\",\n\"ifAlias\":\"1\",\n\"ifDescr\":\"lo\",\n\"ifIndex\":\"1\",\n\"ifName\":\"lo\",\n\"ifOperStatus\":\"up\",\n\"ifPhysAddress\":\"1\",\n\"ifType\":\"softwareLoopback\",\n\"profiles\":\"switch_profile\",\n\"metric_name:sc4snmp.IF-MIB.ifInDiscards\":21877,\n\"metric_name:sc4snmp.IF-MIB.ifInErrors\":21840,\n\"metric_name:sc4snmp.IF-MIB.ifInNUcastPkts\":14152789,\n\"metric_name:sc4snmp.IF-MIB.ifInOctets\":1977814270,\n\"metric_name:sc4snmp.IF-MIB.ifInUcastPkts\":220098191,\n\"metric_name:sc4snmp.IF-MIB.ifInUnknownProtos\":1488029,\n\"metric_name:sc4snmp.IF-MIB.ifLastChange\":124000001,\n\"metric_name:sc4snmp.IF-MIB.ifMtu\":16436,\n\"metric_name:sc4snmp.IF-MIB.ifOutDiscards\":21862,\n\"metric_name:sc4snmp.IF-MIB.ifOutErrors\":21836,\n\"metric_name:sc4snmp.IF-MIB.ifOutNUcastPkts\":14774727,\n\"metric_name:sc4snmp.IF-MIB.ifOutOctets\":1346799625,\n\"metric_name:sc4snmp.IF-MIB.ifOutQLen\":4294967295,\n\"metric_name:sc4snmp.IF-MIB.ifOutUcastPkts\":74003841,\n\"metric_name:sc4snmp.IF-MIB.ifSpeed\":10000000\n}\n</code></pre> <p>or</p> <pre><code>{\n\"frequency\":\"60\",\n\"group\":\"switch_group\",\n\"laNames\":\"Load-1\",\n\"profiles\":\"switch_profile\",\n\"metric_name:sc4snmp.UCD-SNMP-MIB.laIndex\":1\n}\n</code></pre>"},{"location":"configuration/trap-configuration/","title":"Traps","text":""},{"location":"configuration/trap-configuration/#trap-configuration","title":"Trap Configuration","text":"<p>A trap service is a simple server that can handle SNMP traps sent by SNMP devices like routers or switches.   </p>"},{"location":"configuration/trap-configuration/#trap-configuration-file","title":"Trap configuration file","text":"<p>The trap configuration is kept in the <code>values.yaml</code> file in section traps. <code>values.yaml</code> is used during the installation process for configuring Kubernetes values.</p> <p>Trap example configuration:</p> <pre><code>traps:\ncommunities:\n1:\n- public 2c:\n- public\n- homelab\nusernameSecrets:\n- secretv3\n- sc4snmp-homesecure-sha-des\n\n# Overrides the image tag whose default is the chart appVersion.\nlogLevel: \"WARN\"\n# replicas: Number of replicas for trap container should be 2x number of nodes\nreplicas: 2\n#loadBalancerIP: The IP address in the metallb pool\nloadBalancerIP: 10.202.4.202\nresources: limits:\ncpu: 500m\nmemory: 512Mi\nrequests:\ncpu: 200m\nmemory: 256Mi  </code></pre>"},{"location":"configuration/trap-configuration/#define-communities","title":"Define communities","text":"<p><code>communities</code> define a version of SNMP protocol and SNMP community string, which should be used.  <code>communities</code> key is split by protocol version, supported values are <code>1</code> and <code>2c</code>. Under the <code>version</code> section, SNMP community string can be defined. </p> <p>Example: </p> <pre><code>traps:\ncommunities:\n1:\n- public 2c:\n- public\n- homelab\n</code></pre>"},{"location":"configuration/trap-configuration/#configure-user-secrets-for-snmpv3","title":"Configure user secrets for SNMPv3","text":"<p>The <code>usernameSecrets</code> key in the <code>traps</code> section define SNMPv3 secrets for trap messages sent by SNMP device. <code>usernameSecrets</code> define which secrets  in \u201cSecret\u201d objects in k8s should be used, as a value it needs the name of \u201cSecret\u201d objects.  More information on how to define the \u201cSecret\u201d object for SNMPv3 can be found in SNMPv3 Configuration.</p> <p>Example:</p> <pre><code>traps:\nusernameSecrets:\n- sc4snmp-homesecure-sha-aes\n- sc4snmp-homesecure-sha-des\n</code></pre>"},{"location":"configuration/trap-configuration/#define-security-engines-id-for-snmpv3","title":"Define security engines ID for SNMPv3","text":"<p>SNMPv3 TRAPs require the configuration SNMP Engine ID of the TRAP sending application for the USM users table of the TRAP receiving  application for each USM user. The SNMP Engine ID is usually unique for the device, and the SC4SNMP as a trap receiver has to be aware of  which security engine IDs to accept. Define all of them under <code>traps.securityEngineId</code> in <code>values.yaml</code>.</p> <p>By default, it is set to one-element list: <code>[80003a8c04]</code>. </p> <p>Example:</p> <pre><code>traps:\nsecurityEngineId: - \"80003a8c04\"\n</code></pre> <p>Security engine ID is a substitute of the <code>-e</code> variable in <code>snmptrap</code>. An example of SNMPv3 trap is:</p> <pre><code>snmptrap -v3 -e 80003a8c04 -l authPriv -u snmp-poller -a SHA -A PASSWORD1 -x AES -X PASSWORD1 10.202.13.233 '' 1.3.6.1.2.1.2.2.1.1.1\n</code></pre>"},{"location":"configuration/trap-configuration/#define-external-gateway-for-traps","title":"Define external gateway for traps","text":"<p>If you use SC4SNMP standalone, configure <code>loadBalancerIP</code>. <code>loadBalancerIP</code> is the IP address in the metallb pool.  Example:</p> <pre><code>traps:\nloadBalancerIP: 10.202.4.202\n</code></pre> <p>If you want to use SC4SNMP trap receiver in K8S cluster, configure <code>NodePort</code> instead. The snippet of config is:</p> <pre><code>traps:\nservice: type: NodePort\nexternalTrafficPolicy: Cluster\nnodePort: 30000\n</code></pre> <p>Using this method, SNMP trap will always be forwarded to one of the trap receiver pods listening on port 30000 (as in the example above, remember - you can configure any other port). So doesn\u2019t matter IP address of which node you use, adding nodePort will make it end up in a correct place everytime. </p> <p>Here, good practice is to create IP floating address/Anycast pointing to the healthy nodes, so the traffic is forwarded in case of the failover. The best way is to create external LoadBalancer which balance the traffic between nodes.</p>"},{"location":"configuration/trap-configuration/#define-number-of-traps-server-replica","title":"Define number of traps server replica","text":"<p><code>replicaCount</code> defines that the number of replicas for trap container should be 2x number of nodes. The default value is <code>2</code>.  Example:</p> <pre><code>traps:\n#For production deployments the value should be at least 2x the number of nodes\n# Minimum 2 for a single node\n# Minimum 6 for multi-node HA\nreplicaCount: 2\n</code></pre>"},{"location":"configuration/trap-configuration/#define-log-level","title":"Define log level","text":"<p>The log level for trap can be set by changing the value for the <code>logLevel</code> key. The allowed values are: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>.  The default value is <code>WARNING</code>.</p>"},{"location":"configuration/trap-configuration/#define-annotations","title":"Define annotations","text":"<p>In case you need to append some annotations to the <code>trap</code> service, you can do so by setting <code>traps.service.annotations</code>, for ex.:</p> <pre><code>traps:\nservice:\nannotations:\nannotation_key: annotation_value\n</code></pre>"},{"location":"configuration/trap-configuration/#aggregate-traps","title":"Aggregate traps","text":"<p>In case you want to see traps events collected as one event inside splunk you can enable it by setting <code>traps.aggregateTrapsEvents</code>. Example:</p> <pre><code>traps:\naggregateTrapsEvents: \"true\"\n</code></pre>"},{"location":"configuration/worker-configuration/","title":"Worker","text":""},{"location":"configuration/worker-configuration/#worker-configuration","title":"Worker Configuration","text":"<p>The <code>worker</code> is a kubernetes pod which is responsible for the actual execution of polling, processing trap messages, and sending  data to Splunk.</p>"},{"location":"configuration/worker-configuration/#worker-types","title":"Worker types","text":"<p>SC4SNMP has two base functionalities: monitoring traps and polling. These operations are handled by 3 types of workers:</p> <ol> <li> <p>The <code>trap</code> worker consumes all the trap related tasks produced by the trap pod. </p> </li> <li> <p>The <code>poller</code> worker consumes all the tasks related to polling.</p> </li> <li> <p>The <code>sender</code> worker handles sending data to Splunk. You need to always have at least one sender pod running.</p> </li> </ol>"},{"location":"configuration/worker-configuration/#worker-configuration-file","title":"Worker configuration file","text":"<p>Worker configuration is kept in the <code>values.yaml</code> file in the <code>worker</code> section. <code>worker</code> has 3 subsections: <code>poller</code>, <code>sender</code>, or <code>trap</code>, that refer to the workers\u2019 types. <code>values.yaml</code> is used during the installation process for configuring Kubernetes values. The <code>worker</code> default configuration is:</p> <pre><code>worker:\n# There are 3 types of workers \ntrap:\n# replicaCount: number of trap-worker pods which consumes trap tasks\nreplicaCount: 2\n#autoscaling: use it instead of replicaCount in order to make pods scalable by itself\n#autoscaling:\n#  enabled: true\n#  minReplicas: 2\n#  maxReplicas: 10\n#  targetCPUUtilizationPercentage: 80\npoller:\n# replicaCount: number of poller-worker pods which consumes polling tasks\nreplicaCount: 2\n#autoscaling: use it instead of replicaCount in order to make pods scalable by itself\n#autoscaling:\n#  enabled: true\n#  minReplicas: 2\n#  maxReplicas: 10\n#  targetCPUUtilizationPercentage: 80\nsender:\n# replicaCount: number of sender-worker pods which consumes sending tasks\nreplicaCount: 1\n# autoscaling: use it instead of replicaCount in order to make pods scalable by itself\n#autoscaling:\n#  enabled: true\n#  minReplicas: 2\n#  maxReplicas: 10\n#  targetCPUUtilizationPercentage: 80\n# udpConnectionTimeout: timeout in seconds for SNMP operations\n#udpConnectionTimeout: 5\nlogLevel: \"INFO\"\n</code></pre> <p>All parameters are described in the Worker parameters section.</p>"},{"location":"configuration/worker-configuration/#worker-scaling","title":"Worker scaling","text":"<p>You can adjust worker pods in two ways: set fixed value in <code>replicaCount</code>, or enable <code>autoscaling</code>, which scales pods automatically. </p>"},{"location":"configuration/worker-configuration/#real-life-scenario-i-use-sc4snmp-for-only-trap-monitoring-i-want-to-use-my-resources-effectively","title":"Real life scenario: I use SC4SNMP for only trap monitoring, I want to use my resources effectively.","text":"<p>If you don\u2019t use polling at all, set <code>worker.poller.replicaCount</code> to <code>0</code>. If you\u2019ll want to use polling in the future, you need to increase <code>replicaCount</code>. To monitor traps, adjust <code>worker.trap.replicaCount</code> depending on your needs and <code>worker.sender.replicaCount</code> to send traps to Splunk. Usually you need much less sender pods than trap ones.</p> <p>This is the example of <code>values.yaml</code> without using autoscaling:</p> <pre><code>worker:\ntrap:\nreplicaCount: 4\nsender:\nreplicaCount: 1\npoller:\nreplicaCount: 0\nlogLevel: \"WARNING\"\n</code></pre> <p>This is the example of <code>values.yaml</code> with autoscaling:</p> <pre><code>worker:\ntrap:\nautoscaling:\nenabled: true\nminReplicas: 4\nmaxReplicas: 10\ntargetCPUUtilizationPercentage: 80\nsender:\nautoscaling:\nenabled: true\nminReplicas: 2\nmaxReplicas: 5\ntargetCPUUtilizationPercentage: 80\npoller:\nreplicaCount: 0\nlogLevel: \"WARNING\"\n</code></pre> <p>In the example above both trap and sender pods are autoscaled. During an upgrade process, the number of pods is created through <code>minReplicas</code>, and then new ones are created only if the CPU threshold exceeds the <code>targetCPUUtilizationPercentage</code>, which by default is 80%. This solution helps you to keep  resources usage adjusted to what you actually need. </p> <p>After helm upgrade process, you will see <code>horizontalpodautoscaler</code> in <code>microk8s kubectl get all -n sc4snmp</code>:</p> <pre><code>NAME                                                                             REFERENCE                                               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nhorizontalpodautoscaler.autoscaling/snmp-mibserver                               Deployment/snmp-mibserver                               1%/80%    1         3         1          97m\nhorizontalpodautoscaler.autoscaling/snmp-splunk-connect-for-snmp-worker-sender   Deployment/snmp-splunk-connect-for-snmp-worker-sender   1%/80%    2         5         2          28m\nhorizontalpodautoscaler.autoscaling/snmp-splunk-connect-for-snmp-worker-trap     Deployment/snmp-splunk-connect-for-snmp-worker-trap     1%/80%    4         10        4          28m\n</code></pre> <p>If you see <code>&lt;unknown&gt;/80%</code> in <code>TARGETS</code> section instead of the CPU percentage, you probably don\u2019t have the <code>metrics-server</code> add-on enabled. Enable it using <code>microk8s enable metrics-server</code>.</p>"},{"location":"configuration/worker-configuration/#real-life-scenario-i-have-a-significant-delay-in-polling","title":"Real life scenario: I have a significant delay in polling","text":"<p>Sometimes when polling is configured to be run frequently and on many devices, workers get overloaded  and there is a delay in delivering data to Splunk. To avoid such situations, we can scale poller and sender pods. Because of the walk cycles (walk is a costly operation ran once in a while), poller workers require more resources  for a short time. For this reason, enabling autoscaling is recommended. </p> <p>This is the example of <code>values.yaml</code> with autoscaling:</p> <pre><code>worker:\ntrap:\nautoscaling:\nenabled: true\nminReplicas: 4\nmaxReplicas: 10\ntargetCPUUtilizationPercentage: 80\nsender:\nautoscaling:\nenabled: true\nminReplicas: 2\nmaxReplicas: 5\ntargetCPUUtilizationPercentage: 80\npoller:\nautoscaling:\nenabled: true\nminReplicas: 2\nmaxReplicas: 20\ntargetCPUUtilizationPercentage: 80\nlogLevel: \"WARNING\"\n</code></pre> <p>Remember, that the system won\u2019t scale itself infinitely, there is a finite amount of resources that you can allocate. By default, every worker has configured the following resources:</p> <pre><code>    resources:\nlimits:\ncpu: 500m\nrequests:\ncpu: 250m\n</code></pre>"},{"location":"configuration/worker-configuration/#i-have-autoscaling-enabled-and-experience-problems-with-mongo-and-redis-pod","title":"I have autoscaling enabled and experience problems with Mongo and Redis pod","text":"<p>If MongoDB and Redis pods are crushing, and some of the pods are in infinite <code>Pending</code> state, that means  you\u2019re over your resources and SC4SNMP cannot scale more. You should decrease the number of <code>maxReplicas</code> in  workers, so that it\u2019s not going beyond the available CPU.</p>"},{"location":"configuration/worker-configuration/#i-dont-know-how-to-set-autoscaling-parameters-and-how-many-replicas-i-need","title":"I don\u2019t know how to set autoscaling parameters and how many replicas I need","text":"<p>The best way to see if pods are overloaded is to run:</p> <pre><code>microk8s kubectl top pods -n sc4snmp\n</code></pre> <pre><code>NAME                                                          CPU(cores)   MEMORY(bytes)   snmp-mibserver-7f879c5b7c-nnlfj                               1m           3Mi             snmp-mongodb-869cc8586f-q8lkm                                 18m          225Mi           snmp-redis-master-0                                           10m          2Mi             snmp-splunk-connect-for-snmp-scheduler-558dccfb54-nb97j       2m           136Mi           snmp-splunk-connect-for-snmp-trap-5878f89bbf-24wrz            2m           129Mi           snmp-splunk-connect-for-snmp-trap-5878f89bbf-z9gd5            2m           129Mi           snmp-splunk-connect-for-snmp-worker-poller-599c7fdbfb-cfqjm   260m         354Mi           snmp-splunk-connect-for-snmp-worker-poller-599c7fdbfb-ztf7l   312m         553Mi           snmp-splunk-connect-for-snmp-worker-sender-579f796bbd-vmw88   14m           257Mi           snmp-splunk-connect-for-snmp-worker-trap-5474db6fc6-46zhf     3m           259Mi           snmp-splunk-connect-for-snmp-worker-trap-5474db6fc6-mjtpv     4m           259Mi   </code></pre> <p>Here you can see how much CPU and Memory is being used by the pods. If the CPU is close to 500m (which is the limit for one pod by default), you should enable autoscaling/increase maxReplicas or increase replicaCount with autoscaling off.</p> <p>Here you can read about Horizontal Autoscaling and how to adjust maximum replica value to the resources you have: Horizontal Autoscaling.</p>"},{"location":"configuration/worker-configuration/#worker-parameters","title":"Worker parameters","text":"variable description default worker.taskTimeout task timeout in seconds (usually necessary when walk process takes a long time) 2400 worker.walkRetryMaxInterval maximum time interval between walk attempts 600 worker.poller.replicaCount number of poller worker replicas 2 worker.poller.autoscaling.enabled enabling autoscaling for poller worker pods false worker.poller.autoscaling.minReplicas minimum number of running poller worker pods when autoscaling is enabled 2 worker.poller.autoscaling.maxReplicas maximum number of running poller worker pods when autoscaling is enabled 40 worker.poller.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on poller worker pods to spawn another replica 80 worker.poller.resources.limits the resources limits for poller worker container {} worker.poller.resources.requests the requested resources for poller worker container {} worker.trap.replicaCount number of trap worker replicas 2 worker.trap.autoscaling.enabled enabling autoscaling for trap worker pods false worker.trap.autoscaling.minReplicas minimum number of running trap worker pods when autoscaling is enabled 2 worker.trap.autoscaling.maxReplicas maximum number of running trap worker pods when autoscaling is enabled 40 worker.trap.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on trap worker pods to spawn another replica 80 worker.trap.resources.limits the resources limits for poller worker container {} worker.trap.resources.requests the requested resources for poller worker container {} worker.sender.replicaCount number of sender worker replicas 2 worker.sender.autoscaling.enabled enabling autoscaling for sender worker pods false worker.sender.autoscaling.minReplicas minimum number of running sender worker pods when autoscaling is enabled 2 worker.sender.autoscaling.maxReplicas maximum number of running sender worker pods when autoscaling is enabled 40 worker.sender.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on sender worker pods to spawn another replica 80 worker.sender.resources.limits the resources limits for poller worker container {} worker.sender.resources.requests the requested resources for poller worker container {}"},{"location":"gettingstarted/sc4snmp-installation/","title":"Install SC4SNMP","text":""},{"location":"gettingstarted/sc4snmp-installation/#sc4snmp-helm-installation","title":"SC4SNMP Helm installation","text":"<p>The basic installation and configuration process discussed in this section is typical  for single node non-HA deployments. It does not have resource requests and limits. See the mongo, redis, scheduler, worker, and traps configuration sections for guidance on production configuration.</p>"},{"location":"gettingstarted/sc4snmp-installation/#installation-process","title":"Installation process","text":""},{"location":"gettingstarted/sc4snmp-installation/#offline-installation","title":"Offline installation","text":"<p>For offline installation instructions see this page.</p>"},{"location":"gettingstarted/sc4snmp-installation/#online-installation","title":"Online installation","text":""},{"location":"gettingstarted/sc4snmp-installation/#add-sc4snmp-repository","title":"Add SC4SNMP repository","text":"<pre><code>microk8s helm3 repo add splunk-connect-for-snmp https://splunk.github.io/splunk-connect-for-snmp\nmicrok8s helm3 repo update\n</code></pre> <p>Now the package should be visible in <code>helm3</code> search command result:</p> <pre><code>microk8s helm3 search repo snmp\n</code></pre> <p>Example output:</p> <pre><code>NAME                                               CHART VERSION  APP VERSION    DESCRIPTION                           splunk-connect-for-snmp/splunk-connect-for-snmp        1.0.0        1.0.0       A Helm chart for SNMP Connect for SNMP\n</code></pre>"},{"location":"gettingstarted/sc4snmp-installation/#download-and-modify-valuesyaml","title":"Download and modify values.yaml","text":"<p>The installation of SC4SNMP requires the creation of a <code>values.yaml</code> file, which serves as the configuration file. To configure this file, follow these steps:</p> <ol> <li>Start with checking out the basic configuration template</li> <li>Review the examples to determine which areas require configuration.</li> <li>For more advanced configuration options, refer to the complete default values.yaml or download it directly from Helm using the command <code>microk8s helm3 show values splunk-connect-for-snmp/splunk-connect-for-snmp</code> </li> <li>In order to learn more about each of the config parts, check configuration section.</li> </ol> <p>It is recommended to start by completing the base template and gradually add additional configurations as needed.</p>"},{"location":"gettingstarted/sc4snmp-installation/#install-sc4snmp","title":"Install SC4SNMP","text":"<p>After the <code>values.yaml</code> creation, you can proceed with the SC4SNMP installation:</p> <pre><code>microk8s helm3 install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <p>From now on, when editing SC4SNMP configuration, the configuration change must be inserted in the corresponding section of <code>values.yaml</code>. For more details check configuration section.</p> <p>Use the following command to propagate configuration changes:</p> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre>"},{"location":"gettingstarted/sc4snmp-installation/#verification-of-the-deployment","title":"Verification of the deployment","text":"<p>In a few minutes, all pods should be up and running. It can be verified with:</p> <pre><code>microk8s kubectl get pods -n sc4snmp\n</code></pre> <p>Example output:</p> <pre><code>NAME                                                      READY   STATUS             RESTARTS      AGE\nsnmp-splunk-connect-for-snmp-scheduler-7ddbc8d75-bljsj        1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-poller-57cd8f4665-9z9vx   1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-sender-5c44cbb9c5-ppmb5   1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-worker-trap-549766d4-28qzh       1/1     Running   0          133m\nsnmp-mibserver-7f879c5b7c-hz9tz                               1/1     Running   0          133m\nsnmp-mongodb-869cc8586f-vvr9f                                 2/2     Running   0          133m\nsnmp-redis-master-0                                           1/1     Running   0          133m\nsnmp-splunk-connect-for-snmp-trap-78759bfc8b-79m6d            1/1     Running   0          99m\nsnmp-splunk-connect-for-snmp-inventory-mjccw                  0/1     Completed 0          6s\n</code></pre> <p>The output may vary depending on the configuration. In the above example, both polling and traps are configured,  and the data is being sent to Splunk.</p> <p>If you have <code>traps</code> configured, you should see <code>EXTERNAL-IP</code> in <code>snmp-splunk-connect-for-snmp-trap</code> service. Check it using the command:</p> <pre><code>microk8s kubectl get svc -n sc4snmp </code></pre> <p>Here is an example of the correct setup:</p> <pre><code>NAME                                TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE\nsnmp-redis-headless                 ClusterIP      None             &lt;none&gt;        6379/TCP        33h\nsnmp-mongodb                        ClusterIP      10.152.183.147   &lt;none&gt;        27017/TCP       33h\nsnmp-mibserver                      ClusterIP      10.152.183.253   &lt;none&gt;        80/TCP          33h\nsnmp-redis-master                   ClusterIP      10.152.183.135   &lt;none&gt;        6379/TCP        33h\nsnmp-mongodb-metrics                ClusterIP      10.152.183.217   &lt;none&gt;        9216/TCP        33h\nsnmp-splunk-connect-for-snmp-trap   LoadBalancer   10.152.183.33    10.202.9.21   162:30161/UDP   33h\n</code></pre> <p>If there\u2019s <code>&lt;pending&gt;</code> communicate instead of the IP address, that means you either provided the wrong IP address in <code>traps.loadBalancerIP</code> or there\u2019s something wrong with the <code>metallb</code> microk8s addon.</p> <p>For the sake of the example, let\u2019s assume we haven\u2019t changed the default indexes names and the metric data goes to <code>netmetrics</code> and the events goes to <code>netops</code>.</p>"},{"location":"gettingstarted/sc4snmp-installation/#test-snmp-traps","title":"Test SNMP Traps","text":"<ol> <li>Simulate the event. On a Linux system, you can download <code>snmpd</code> package for its purpose and run:</li> </ol> <pre><code>apt update\napt-get install snmpd\nsnmptrap -v2c -c public EXTERNAL-IP 123 1.3.6.1.2.1.1.4 1.3.6.1.2.1.1.4 s test\n</code></pre> <p>Remember to replace <code>EXTERNAL-IP</code> with the ip address of the <code>snmp-splunk-connect-for-snmp-trap</code> service from the above.</p> <ol> <li>Search Splunk: You should see one event per trap command with the host value of the test machine <code>EXTERNAL-IP</code> IP address.</li> </ol> <pre><code>index=\"netops\" sourcetype=\"sc4snmp:traps\"\n</code></pre>"},{"location":"gettingstarted/sc4snmp-installation/#test-snmp-poller","title":"Test SNMP Poller","text":"<ol> <li>To test SNMP poller, you can either use the device you already have, or configure snmpd on your Linux system.  Snmpd needs to be configured to listen on the external IP. To enable listening snmpd to external IP, go to the <code>/etc/snmp/snmpd.conf</code> configuration file, and replace the IP address <code>10.0.101.22</code> with the server IP address where snmpd is configured: <code>agentaddress  10.0.101.22,127.0.0.1,[::1]</code>. Restart snmpd through the execute command:</li> </ol> <pre><code>service snmpd stop\nservice snmpd start\n</code></pre> <ol> <li>Configure SC4SNMP Poller to test and add the IP address which you want to poll. Add the configuration entry into the <code>values.yaml</code> file by  replacing the IP address <code>10.0.101.22</code> with the server IP address where the snmpd was configured.</li> </ol> <pre><code>poller:\n  inventory: |\naddress,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete\n    10.0.101.22,,2c,public,,,42000,,,\n</code></pre> <ol> <li>Load <code>values.yaml</code> file into SC4SNMP</li> </ol> <pre><code>microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre> <ol> <li>Verify if the records appeared in Splunk:</li> </ol> <pre><code>index=\"netops\" sourcetype=\"sc4snmp:event\"\n</code></pre> <pre><code>| mpreview index=\"netmetrics\" | search sourcetype=\"sc4snmp:metric\"\n</code></pre> <p>NOTE: Before polling starts, SC4SNMP must perform SNMP WALK process on the device. It is run first time after configuring the new device, and then the run time in every <code>walk_interval</code>.  Its purpose is to gather all the data and provide meaningful context for the polling records. For example, it might report that your device is so large that the walk takes too long, so the scope of walking needs to be limited. In such cases, enable the small walk. See: walk takes too much time. When the walk finishes, events appear in Splunk.</p>"},{"location":"gettingstarted/sc4snmp-installation/#next-steps","title":"Next Steps","text":"<p>A good way to start with SC4SNMP polling is to follow the Step by Step guide for polling. Advanced configuration of polling is available in Poller configuration section. SNMP data format is explained in SNMP data format section.</p> <p>For advanced trap configuration, check the Traps configuration section.</p>"},{"location":"gettingstarted/sc4snmp-installation/#uninstall-splunk-connect-for-snmp","title":"Uninstall Splunk Connect for SNMP","text":"<p>To uninstall SC4SNMP run the following commands:</p> <pre><code> microk8s helm3 uninstall snmp -n sc4snmp\n microk8s kubectl delete pvc --all -n sc4snmp\n</code></pre>"},{"location":"gettingstarted/sck-installation/","title":"Install Splunk OpenTelemetry Collector for Kubernetes","text":""},{"location":"gettingstarted/sck-installation/#splunk-opentelemetry-collector-for-kubernetes-installation","title":"Splunk OpenTelemetry Collector for Kubernetes installation","text":"<p>Splunk OpenTelemetry Collector for Kubernetes is not required for SC4SNMP installation. This is the tool that sends logs and metrics from a k8s cluster to a Splunk instance, which makes SC4SNMP easier to debug.  You can do the same using the <code>microk8s kubectl logs</code> command on instances you\u2019re interested in, but if you\u2019re not proficient in Kubernetes, Splunk OpenTelemetry Collector for Kubernetes is strongly advised.</p> <p>The below steps are sufficient for a Splunk OpenTelemetry Collector installation for the SC4SNMP project with Splunk Enterprise/Enterprise Cloud. In order to learn more about Splunk OpenTelemetry Collector, visit Splunk OpenTelemetry Collector.</p>"},{"location":"gettingstarted/sck-installation/#offline-installation","title":"Offline installation","text":"<p>For offline installation instructions see this page.</p>"},{"location":"gettingstarted/sck-installation/#add-splunk-opentelemetry-collector-repository-to-helm","title":"Add Splunk OpenTelemetry Collector repository to HELM","text":"<pre><code>microk8s helm3 repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart\n</code></pre>"},{"location":"gettingstarted/sck-installation/#install-splunk-opentelemetry-collector-with-helm-for-a-splunk-platform","title":"Install Splunk OpenTelemetry Collector with HELM for a Splunk Platform","text":"<p>In order to run Splunk OpenTelemetry Collector on your environment, replace <code>&lt;&gt;</code> variables according to the description presented below</p> <pre><code>microk8s helm3 upgrade --install sck \\\n--set=\"clusterName=&lt;cluster_name&gt;\" \\\n--set=\"splunkPlatform.endpoint=&lt;splunk_endpoint&gt;\" \\\n--set=\"splunkPlatform.insecureSkipVerify=&lt;insecure_skip_verify&gt;\" \\\n--set=\"splunkPlatform.token=&lt;splunk_token&gt;\" \\\n--set=\"logsEngine=otel\" \\\n--set=\"splunkPlatform.metricsEnabled=true\" \\\n--set=\"splunkPlatform.metricsIndex=em_metrics\" \\\n--set=\"splunkPlatform.index=em_logs\" \\\nsplunk-otel-collector-chart/splunk-otel-collector\n</code></pre>"},{"location":"gettingstarted/sck-installation/#variables-description","title":"Variables description","text":"Placeholder Description Example splunk_endpoint host address of splunk instance https://endpoint.example.com:8088/services/collector insecure_skip_verify is insecure ssl allowed false splunk_token Splunk HTTP Event Collector token 450a69af-16a9-4f87-9628-c26f04ad3785 cluster_name name of the cluster my-cluster <p>An example of filled up command is:</p> <pre><code>microk8s helm3 upgrade --install sck \\\n--set=\"clusterName=my-cluster\" \\\n--set=\"splunkPlatform.endpoint=https://endpoint.example.com/services/collector\" \\\n--set=\"splunkPlatform.insecureSkipVerify=false\" \\\n--set=\"splunkPlatform.token=4d22911c-18d9-4706-ae7b-dd1b976ca6f7\" \\\n--set=\"splunkPlatform.metricsEnabled=true\" \\\n--set=\"splunkPlatform.metricsIndex=em_metrics\" \\\n--set=\"splunkPlatform.index=em_logs\" \\\nsplunk-otel-collector-chart/splunk-otel-collector\n</code></pre>"},{"location":"gettingstarted/sck-installation/#install-splunk-opentelemetry-collector-with-helm-for-splunk-observability-for-kubernetes","title":"Install Splunk OpenTelemetry Collector with HELM for Splunk Observability for Kubernetes","text":"<p>To run Splunk OpenTelemetry Collector on your environment, replace the <code>&lt;&gt;</code> variables according to the description presented below:</p> <pre><code>microk8s helm3 upgrade --install sck\n--set=\"clusterName=&lt;cluster_name&gt;\"\n--set=\"splunkObservability.realm=&lt;realm&gt;\"\n--set=\"splunkObservability.accessToken=&lt;token&gt;\"\n--set=\"splunkObservability.ingestUrl=&lt;ingest_url&gt;\"\n--set=\"splunkObservability.apiUrl=&lt;api_url&gt;\"\n--set=\"splunkObservability.metricsEnabled=true\"\n--set=\"splunkObservability.tracesEnabled=false\"\n--set=\"splunkObservability.logsEnabled=false\"\nsplunk-otel-collector-chart/splunk-otel-collector\n</code></pre>"},{"location":"gettingstarted/sck-installation/#variables-description_1","title":"Variables description","text":"Placeholder Description Example cluster_name name of the cluster my_cluster realm Realm obtained from the Splunk Observability Cloud environment us0 token Token obtained from the Splunk Observability Cloud environment BCwaJ_Ands4Xh7Nrg ingest_url Ingest URL from the Splunk Observability Cloud environment https://ingest..signalfx.com api_url API URL from the Splunk Observability Cloud environment https://api..signalfx.com <p>An example of a filled up command is:</p> <pre><code>microk8s helm3 upgrade --install sck\n--set=\"clusterName=my_cluster\"\n--set=\"splunkObservability.realm=us0\"\n--set=\"splunkObservability.accessToken=BCwaJ_Ands4Xh7Nrg\"\n--set=\"splunkObservability.ingestUrl=https://ingest..signalfx.com\"\n--set=\"splunkObservability.apiUrl=https://api..signalfx.com\"\n--set=\"splunkObservability.metricsEnabled=true\"\n--set=\"splunkObservability.tracesEnabled=false\"\n--set=\"splunkObservability.logsEnabled=false\"\nsplunk-otel-collector-chart/splunk-otel-collector\n</code></pre>"},{"location":"gettingstarted/splunk-requirements/","title":"Splunk Requirements","text":""},{"location":"gettingstarted/splunk-requirements/#splunk-requirements","title":"Splunk requirements","text":""},{"location":"gettingstarted/splunk-requirements/#prepare-splunk","title":"Prepare Splunk","text":"<p>See the following prerequisites for the Splunk Connect for SNMP. </p>"},{"location":"gettingstarted/splunk-requirements/#requirements-splunk-enterpriseenterprise-cloud","title":"Requirements (Splunk Enterprise/Enterprise Cloud)","text":"<ol> <li>Manually create the following indexes in Splunk:</li> </ol> <ul> <li>Indexes for logs and metrics from SC4SNMP Connector:<ul> <li>em_metrics (metrics type)</li> <li>em_logs (event type)</li> </ul> </li> <li>Indexes where SNMP Data will be forwarded:<ul> <li>netmetrics (metrics type)</li> <li>netops (event type)</li> </ul> </li> </ul> <p>Note: <code>netmetrics</code> and <code>netops</code> are the default names of SC4SNMP indexes. You can use the index names of your choice and reference it in the <code>values.yaml</code> file later on. See parameters and instructions for details: SC4SNMP Parameters.</p> <ol> <li>Create or obtain a new Splunk HTTP Event Collector token and the correct HTTPS endpoint.</li> <li> <p>Verify the token using curl. Note: The endpoint must use a publicly trusted certificate authority.</p> </li> <li> <p>The SHARED IP address to be used for SNMP Traps. Note Simple and POC deployments will use the same IP as the host server. If HA deployment will be used, the IP must be in addition to the management interface of each cluster member.</p> </li> <li>Obtain the IP address of an internal DNS server that can resolve the Splunk Endpoint.</li> </ol>"},{"location":"gettingstarted/splunk-requirements/#requirements-splunk-infrastructure-monitoring","title":"Requirements (Splunk Infrastructure Monitoring)","text":"<p>Obtain the following from your Splunk Observability Cloud environment:</p> <ol> <li>Realm</li> <li>Token</li> </ol>"},{"location":"gettingstarted/mk8s/k8s-microk8s/","title":"Platform Microk8s","text":""},{"location":"gettingstarted/mk8s/k8s-microk8s/#splunk-connect-for-snmp-using-microk8s","title":"Splunk Connect for SNMP using MicroK8s","text":"<p>See the following requirements to use any Linux deployment of Microk8s to support SC4SMP. The minimum requirements below are suitable for proof of value and small installations, and actual requirements will differ.</p> <p>Single node minimum: </p> <ul> <li>4 cores</li> <li>8 GB of memory per node</li> <li>50 GB mounted as /</li> </ul> <p>Three node minimum per node:</p> <ul> <li>4 cores</li> <li>8 GB of memory per node</li> <li>50 GB mounted /</li> </ul>"},{"location":"gettingstarted/mk8s/k8s-microk8s/#microk8s-installation-on-ubuntu","title":"MicroK8s installation on Ubuntu","text":"<p>The following quick start guidance is based on Ubuntu 20.04LTS with MicroK8s with internet access. Other deployment options may be found in the MicroK8s documentation including offline and with proxy. </p>"},{"location":"gettingstarted/mk8s/k8s-microk8s/#install-microk8s-using-snap","title":"Install MicroK8s using Snap","text":"<pre><code>sudo snap install microk8s --classic --channel=1.25/stable\n</code></pre> <p>Add a user to the microk8s group so the <code>sudo</code> command is no longer necessary:</p> <pre><code>sudo usermod -a -G microk8s $USER\nsudo chown -f -R $USER ~/.kube\nsu - $USER\n</code></pre> <p>Wait for Installation of Mk8S to complete:</p> <pre><code>microk8s status --wait-ready\n</code></pre>"},{"location":"gettingstarted/mk8s/k8s-microk8s/#add-additional-nodes-optional","title":"Add additional nodes (optional)","text":"<ul> <li>Repeat the steps above for each additional node (minimum total 3)</li> <li>On the first node issue the following to return the instructions to join: </li> </ul> <pre><code>microk8s add-node\n</code></pre> <ul> <li>On each additional node, use the output from the command above</li> </ul>"},{"location":"gettingstarted/mk8s/k8s-microk8s/#install-basic-services-required-for-sc4snmp","title":"Install basic services required for sc4snmp","text":"<p>The following commands can be issued from any one node in a cluster:</p> <pre><code>sudo systemctl enable iscsid\nmicrok8s enable helm3\nmicrok8s enable hostpath-storage\nmicrok8s enable rbac\nmicrok8s enable metrics-server\nmicrok8s status --wait-ready\n</code></pre> <p>Install the DNS server for mk8s and configure the forwarding DNS servers. Replace the IP addressed below (opendns) with allowed values for your network: </p> <pre><code>microk8s enable dns:208.67.222.222,208.67.220.220\nmicrok8s status --wait-ready\n</code></pre>"},{"location":"gettingstarted/mk8s/k8s-microk8s/#install-metallb","title":"Install Metallb","text":"<p>Note: when installing Metallb you will be prompted for one or more IPs to use as entry points into the cluster. If your plan to enable clustering, this IP should not be assigned to the host (floats). If you do not plan to cluster, then this IP should be the IP of your host.</p> <p>Note2: a single IP in cidr format is x.x.x.x/32. Use CIDR or range syntax for single server installations. This can be the same as the primary IP.</p> <pre><code>microk8s enable metallb\nmicrok8s status --wait-ready\n</code></pre>"},{"location":"offlineinstallation/offline-microk8s/","title":"Install Microk8s","text":""},{"location":"offlineinstallation/offline-microk8s/#offline-microk8s-installation-issues","title":"Offline Microk8s installation issues","text":"<p>Offline installation of Microk8s is described here, but there are additional steps to install microk8s offline. </p>"},{"location":"offlineinstallation/offline-microk8s/#importing-images","title":"Importing images","text":"<p>After running:</p> <pre><code>snap ack microk8s_{microk8s_version}.assert\nsnap install microk8s_{microk8s_version}.snap --classic\n</code></pre> <p>You should check if the microk8s instance is healthy. Do it with:</p> <pre><code>microk8s kubectl get pods -A\n</code></pre> <p>The output will probably look like:</p> <pre><code>NAMESPACE      NAME                                       READY   STATUS     RESTARTS   AGE\nkube-system    calico-kube-controllers-7c9c8dd885-fg8f2   0/1     Pending    0          14m\nkube-system    calico-node-zg4c4                          0/1     Init:0/3   0          23s\n</code></pre> <p>The pods are in the <code>Pending</code>/<code>Init</code> state because they\u2019re trying to download images, which is impossible to do offline. In order to make them work you need to download all the images on a different server with an internet connection, pack it up, and import it to a microk8s image registry on your offline server. </p>"},{"location":"offlineinstallation/offline-microk8s/#packing-up-images-for-offline-environment","title":"Packing up images for offline environment","text":"<p>You need to monitor</p> <pre><code>microk8s kubectl get events -A\n</code></pre> <p>to see if <code>microk8s</code> fails to pull images, and then import anything it needs. An example of such information is:</p> <pre><code>kube-system    0s          Warning   Failed              pod/calico-node-sc784                           Failed to pull image \"docker.io/calico/cni:v3.21.4\": rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/calico/cni:v3.21.4\": failed to resolve reference \"docker.io/calico/cni:v3.21.4\": failed to do request: Head \"https://registry-1.docker.io/v2/calico/cni/manifests/v3.21.4\": dial tcp 54.83.42.45:443: i/o timeout\nkube-system    0s          Warning   Failed              pod/calico-node-sc784                           Error: ErrImagePull\n</code></pre> <p>This shows you that you lack a <code>docker.io/calico/cni:v3.21.4</code> image, and need to import it in order to fix the issue.</p> <p>The process of such action is always:</p> <pre><code>docker pull &lt;needed_image&gt;\ndocker save &lt;needed_image&gt; &gt; image.tar\n</code></pre> <p>Transfer package to the offline lab and execute:</p> <pre><code>microk8s ctr image import image.tar\n</code></pre>"},{"location":"offlineinstallation/offline-microk8s/#example-of-the-offline-installation","title":"Example of the offline installation","text":"<p>For example, <code>microk8s</code> version <code>3597</code> requires these images to work correctly:</p> <pre><code>docker pull docker.io/calico/kube-controllers:v3.21.4 docker pull docker.io/calico/node:v3.21.4\ndocker pull docker.io/calico/pod2daemon-flexvol:v3.21.4\ndocker pull docker.io/calico/cni:v3.21.4  docker pull k8s.gcr.io/pause:3.1 docker pull k8s.gcr.io/metrics-server/metrics-server:v0.5.2 </code></pre> <p>You should issue the above commands on your instance connected to the internet, then save it to <code>tar</code> packages:</p> <pre><code>docker save docker.io/calico/kube-controllers:v3.21.4 &gt; kube-controllers.tar\ndocker save docker.io/calico/node:v3.21.4 &gt; node.tar\ndocker save docker.io/calico/pod2daemon-flexvol:v3.21.4 &gt; pod2daemon-flexvol.tar\ndocker save docker.io/calico/cni:v3.21.4 &gt; cni.tar\ndocker save k8s.gcr.io/pause:3.1  &gt; pause.tar\ndocker save cdkbot/hostpath-provisioner:1.2.0 &gt; cdkbot.tar docker save k8s.gcr.io/metrics-server/metrics-server:v0.5.2 &gt; metrics.tar\n</code></pre> <p>After that, <code>scp</code> those packages to your offline server and import it to its <code>microk8s</code> image registry:</p> <pre><code>microk8s ctr image import kube-controllers.tar\nmicrok8s ctr image import node.tar\nmicrok8s ctr image import pod2daemon-flexvol.tar\nmicrok8s ctr image import cni.tar\nmicrok8s ctr image import pause.tar\nmicrok8s ctr image import metrics.tar\n</code></pre> <p>NOTE: for other versions of <code>microk8s</code>, tags of images may differ. </p> <p>The healthy instance of microk8s, after running:</p> <pre><code>microk8s enable hostpath-storage\nmicrok8s enable rbac\nmicrok8s enable metrics-server\n</code></pre> <p>should look like this:</p> <pre><code>NAMESPACE      NAME                                       READY   STATUS                  RESTARTS   AGE\nkube-system    calico-kube-controllers-7c9c8dd885-wxms9   1/1     Running                 0          3h21m\nkube-system    calico-node-8cxsq                          1/1     Running                 0          3h21m\nkube-system    hostpath-provisioner-f57964d5f-zs4sj       1/1     Running                 0          5m41s\nkube-system    metrics-server-5f8f64cb86-x7k29            1/1     Running                 0          2m15s\n</code></pre>"},{"location":"offlineinstallation/offline-microk8s/#enabling-dns-and-metallb","title":"Enabling DNS and Metallb","text":"<p>The <code>dns</code> and <code>metallb</code> don\u2019t require importing any images, so you can enable them simply by:</p> <pre><code>microk8s enable dns\nmicrok8s enable metallb\n</code></pre> <p>More on <code>metallb</code> here.</p>"},{"location":"offlineinstallation/offline-microk8s/#installing-helm3","title":"Installing helm3","text":"<p>The additional problem is the installation of <code>helm3</code> add-on. You need to do a few things to make it work.</p> <ol> <li>Check your server\u2019s platform with:</li> </ol> <pre><code>dpkg --print-architecture\n</code></pre> <p>The output would be for ex.: <code>amd64</code>. You need the platform to download the correct version of helm.</p> <ol> <li> <p>Download the helm package from <code>https://get.helm.sh/helm-v3.8.0-linux-{{arch}}.tar.gz</code>, where <code>{{arch}}</code> should be  replaced with the result from the previous command. Example: <code>https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz</code></p> </li> <li> <p>Rename package to <code>helm.tar.gz</code> and send it to an offline lab.</p> </li> <li>Create <code>tmp</code> directory in <code>/var/snap/microk8s/current</code> and copy the package there:</li> </ol> <pre><code>sudo mkdir -p /var/snap/microk8s/current/tmp/helm3\nsudo cp helm.tar.gz /var/snap/microk8s/current/tmp/helm3\n</code></pre> <ol> <li>Go to the directory containing <code>enable</code> script for <code>helm3</code>:</li> </ol> <pre><code>cd /var/snap/microk8s/common/addons/core/addons/helm3\n</code></pre> <p>Open <code>enable</code> file with vi, nano, or some other editor. Comment this line:</p> <pre><code>#fetch_as $SOURCE_URI/helm-$HELM_VERSION-linux-${SNAP_ARCH}.tar.gz \"$SNAP_DATA/tmp/helm3/helm.tar.gz\"\n</code></pre> <p>Save file.</p> <ol> <li>Run <code>microk8s enable helm3</code></li> </ol>"},{"location":"offlineinstallation/offline-microk8s/#verify-your-instance","title":"Verify your instance","text":"<p>Check if all the add-ons were installed successfully with command: <code>microk8s status --wait-ready</code>. An example of a correct output is:</p> <pre><code>microk8s is running\nhigh-availability: no\ndatastore master nodes: 127.0.0.1:19001\ndatastore standby nodes: none\naddons:\nenabled:\ndns                  # (core) CoreDNS\nha-cluster           # (core) Configure high availability on the current node\nhelm3                # (core) Helm 3 - Kubernetes package manager\nhostpath-storage     # (core) Storage class; allocates storage from host directory\nmetallb              # (core) Loadbalancer for your Kubernetes cluster\nmetrics-server       # (core) K8s Metrics Server for API access to service metrics\nrbac                 # (core) Role-Based Access Control for authorisation\nstorage              # (core) Alias to hostpath-storage add-on, deprecated\ndisabled:\ncommunity            # (core) The community addons repository\ndashboard            # (core) The Kubernetes dashboard\ngpu                  # (core) Automatic enablement of Nvidia CUDA\nhelm                 # (core) Helm 2 - the package manager for Kubernetes\nhost-access          # (core) Allow Pods connecting to Host services smoothly\ningress              # (core) Ingress controller for external access\nmayastor             # (core) OpenEBS MayaStor\nprometheus           # (core) Prometheus operator for monitoring and logging\nregistry             # (core) Private image registry exposed on localhost:32000\n</code></pre>"},{"location":"offlineinstallation/offline-sc4snmp/","title":"Install SC4SNMP","text":""},{"location":"offlineinstallation/offline-sc4snmp/#offline-sc4snmp-installation","title":"Offline SC4SNMP installation","text":""},{"location":"offlineinstallation/offline-sc4snmp/#local-machine-with-internet-access","title":"Local machine with internet access","text":"<p>To install the SC4SNMP offline, first, some packages must be downloaded from the Github release and then moved to the SC4SNMP installation server. Those packages are:</p> <ul> <li><code>dependencies-images.tar</code></li> <li><code>splunk-connect-for-snmp-chart.tar</code></li> </ul> <p>Additionally, you\u2019ll need </p> <ul> <li><code>pull_mibserver.sh</code> script</li> </ul> <p>to easily pull and export mibserver image.</p> <p>Moreover, SC4SNMP Docker image must be pulled, saved as a <code>.tar</code> package, and then moved to the server as well.  This process requires Docker to be installed locally.</p> <p>Images can be pulled from the following repository: <code>ghcr.io/splunk/splunk-connect-for-snmp/container:&lt;tag&gt;</code>.  The latest tag can be found here under the Releases section with the label <code>latest</code>.</p> <p>Example of docker pull command:</p> <pre><code>docker pull ghcr.io/splunk/splunk-connect-for-snmp/container:&lt;tag&gt;\n</code></pre> <p>Then save the image. Directory where this image will be saved can be specified after the <code>&gt;</code> sign:</p> <pre><code>docker save ghcr.io/splunk/splunk-connect-for-snmp/container:&lt;tag&gt; &gt; snmp_image.tar\n</code></pre> <p>Another package you have to pull is the mibserver image. You can do it by executing <code>pull_mibserver.sh</code> script from the Release section, or copy-pasting its content.</p> <pre><code>chmod a+x pull_mibserver.sh # you'll probably need to make file executable\n./pull_mibserver.sh\n</code></pre> <p>This script should produce <code>mibserver.tar</code> with the image of the mibserver inside.</p> <p>All four packages, <code>mibserver.tar</code>, <code>snmp_image.tar</code>, <code>dependencies-images.tar</code>, and <code>splunk-connect-for-snmp-chart.tar</code>, must be moved to the SC4SNMP installation server.</p>"},{"location":"offlineinstallation/offline-sc4snmp/#installation-on-the-server","title":"Installation on the server","text":"<p>On the server, all the images must be imported to the microk8s cluster. This can be done with the following command:</p> <pre><code>microk8s ctr image import &lt;name_of_tar_image&gt;\n</code></pre> <p>In case of this installation the following commands must be run:</p> <pre><code>microk8s ctr image import dependencies-images.tar\nmicrok8s ctr image import snmp_image.tar\nmicrok8s ctr image import mibserver.tar\n</code></pre> <p>Then create <code>values.yaml</code>. It\u2019s a little different from <code>values.yaml</code> used in an online installation.  The difference between the two files is the following, which is used for automatic image pulling:</p> <pre><code>image:\npullPolicy: \"Never\"\n</code></pre> <p>Example <code>values.yaml</code> file can be found here.</p> <p>The next step is to unpack the chart package <code>splunk-connect-for-snmp-chart.tar</code>. It will result in creating the <code>splunk-connect-for-snmp</code> directory:</p> <pre><code>tar -xvf splunk-connect-for-snmp-chart.tar --exclude='._*'\n</code></pre> <p>Finally, run the helm install command in the directory where both the <code>values.yaml</code> and <code>splunk-connect-for-snmp</code> directories are located:</p> <pre><code>microk8s helm3 install snmp -f values.yaml splunk-connect-for-snmp --namespace=sc4snmp --create-namespace\n</code></pre>"},{"location":"offlineinstallation/offline-sck/","title":"Install Splunk OpenTelemetry Collector for Kubernetes","text":""},{"location":"offlineinstallation/offline-sck/#splunk-opentelemetry-collector-for-kubernetes-offline-installation","title":"Splunk OpenTelemetry Collector for Kubernetes offline installation","text":""},{"location":"offlineinstallation/offline-sck/#local-machine-with-internet-access","title":"Local machine with internet access","text":"<p>To install Splunk OpenTelemetry Collector offline first one must download packed chart <code>splunk-otel-collector-&lt;tag&gt;.tgz</code> and the otel image <code>otel_image.tar</code> from github release where <code>&lt;tag&gt;</code> is the current OpenTelemetry release tag. Both packages must be later moved to the installation server.</p>"},{"location":"offlineinstallation/offline-sck/#installation-on-the-server","title":"Installation on the server","text":"<p>Otel image has to be imported to the <code>microk8s</code> registry with:</p> <pre><code>microk8s ctr image import otel_image.tar </code></pre> <p>Imported package must be unpacked with the following command :</p> <pre><code>tar -xvf splunk-otel-collector-&lt;tag&gt;.tgz --exclude='._*'\n</code></pre> <p>In order to run Splunk OpenTelemetry Collector on your environment, replace <code>&lt;&gt;</code> variables according to the description presented below</p> <pre><code>microk8s helm3 install sck \\\n--set=\"clusterName=&lt;cluster_name&gt;\" \\\n--set=\"splunkPlatform.endpoint=&lt;splunk_endpoint&gt;\" \\\n--set=\"splunkPlatform.insecureSkipVerify=&lt;insecure_skip_verify&gt;\" \\\n--set=\"splunkPlatform.token=&lt;splunk_token&gt;\" \\\n--set=\"logsEngine=otel\" \\\n--set=\"splunkPlatform.metricsEnabled=true\" \\\n--set=\"splunkPlatform.metricsIndex=em_metrics\" \\\n--set=\"splunkPlatform.index=em_logs\" \\\nsplunk-otel-collector\n</code></pre>"},{"location":"offlineinstallation/offline-sck/#variables-description","title":"Variables description","text":"Placeholder Description Example splunk_endpoint host address of splunk instance https://endpoint.example.com:8088/services/collector insecure_skip_verify is insecure ssl allowed false splunk_token Splunk HTTP Event Collector token 450a69af-16a9-4f87-9628-c26f04ad3785 cluster_name name of the cluster my-cluster <p>An example of filled up command is:</p> <pre><code>microk8s helm3 install sck \\\n--set=\"clusterName=my-cluster\" \\\n--set=\"splunkPlatform.endpoint=https://endpoint.example.com/services/collector\" \\\n--set=\"splunkPlatform.insecureSkipVerify=false\" \\\n--set=\"splunkPlatform.token=4d22911c-18d9-4706-ae7b-dd1b976ca6f7\" \\\n--set=\"splunkPlatform.metricsEnabled=true\" \\\n--set=\"splunkPlatform.metricsIndex=em_metrics\" \\\n--set=\"splunkPlatform.index=em_logs\" \\\nsplunk-otel-collector\n</code></pre>"},{"location":"offlineinstallation/offline-sck/#install-splunk-opentelemetry-collector-with-helm-for-splunk-observability-for-kubernetes","title":"Install Splunk OpenTelemetry Collector with HELM for Splunk Observability for Kubernetes","text":"<p>To run Splunk OpenTelemetry Collector on your environment, replace <code>&lt;&gt;</code> variables according to the description presented below</p> <pre><code>microk8s helm3 install sck\n--set=\"clusterName=&lt;cluster_name&gt;\"\n--set=\"splunkObservability.realm=&lt;realm&gt;\"\n--set=\"splunkObservability.accessToken=&lt;token&gt;\"\n--set=\"splunkObservability.ingestUrl=&lt;ingest_url&gt;\"\n--set=\"splunkObservability.apiUrl=&lt;api_url&gt;\"\n--set=\"splunkObservability.metricsEnabled=true\"\n--set=\"splunkObservability.tracesEnabled=false\"\n--set=\"splunkObservability.logsEnabled=false\"\nsplunk-otel-collector\n</code></pre>"},{"location":"offlineinstallation/offline-sck/#variables-description_1","title":"Variables description","text":"Placeholder Description Example cluster_name name of the cluster my_cluster realm Realm obtained from the Splunk Observability Cloud environment us0 token Token obtained from the Splunk Observability Cloud environment BCwaJ_Ands4Xh7Nrg ingest_url Ingest URL from the Splunk Observability Cloud environment https://ingest..signalfx.com api_url API URL from the Splunk Observability Cloud environment https://api..signalfx.com <p>An example of filled up command is:</p> <pre><code>microk8s helm3 install sck \n--set=\"clusterName=my_cluster\"\n--set=\"splunkObservability.realm=us0\"\n--set=\"splunkObservability.accessToken=BCwaJ_Ands4Xh7Nrg\"\n--set=\"splunkObservability.ingestUrl=https://ingest..signalfx.com\"\n--set=\"splunkObservability.apiUrl=https://api..signalfx.com\"\n--set=\"splunkObservability.metricsEnabled=true\"\n--set=\"splunkObservability.tracesEnabled=false\"\n--set=\"splunkObservability.logsEnabled=false\"\nsplunk-otel-collector\n</code></pre>"}]}