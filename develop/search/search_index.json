{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Splunk Connect for SNMP \u00b6 Splunk welcomes your experimentation and feedback. Let your account team know you are testing Splunk Connect for SNMP. Splunk Connect for SNMP is an edge-deployed, containerized, and highly available solution for collecting SNMP data for Splunk Enterprise, Splunk Enterprise Cloud and Splunk Infrastructure Monitoring. SC4SNMP provides context-full information. It not only forwards SNMP data to Splunk, but also integrates the data into meaningful objects. For example, you don\u2019t need to write queries in order to gather information about interfaces of the device, because SC4SNMP does that automatically: What makes it easy to visualize the data in Analytics of Splunk: Here is a short presentation of how to browse SNMP data in Splunk: SC4SNMP can also easily monitor trap events sent by different SNMP devices. Trap events are JSON formatted, and are being stored under netops index.","title":"Home"},{"location":"#splunk-connect-for-snmp","text":"Splunk welcomes your experimentation and feedback. Let your account team know you are testing Splunk Connect for SNMP. Splunk Connect for SNMP is an edge-deployed, containerized, and highly available solution for collecting SNMP data for Splunk Enterprise, Splunk Enterprise Cloud and Splunk Infrastructure Monitoring. SC4SNMP provides context-full information. It not only forwards SNMP data to Splunk, but also integrates the data into meaningful objects. For example, you don\u2019t need to write queries in order to gather information about interfaces of the device, because SC4SNMP does that automatically: What makes it easy to visualize the data in Analytics of Splunk: Here is a short presentation of how to browse SNMP data in Splunk: SC4SNMP can also easily monitor trap events sent by different SNMP devices. Trap events are JSON formatted, and are being stored under netops index.","title":"Splunk Connect for SNMP"},{"location":"bestpractices/","text":"Debug Splunk Connect for SNMP \u00b6 Pieces of Advice \u00b6 Check when SNMP WALK was executed last time for the device \u00b6 Configure Splunk OpenTelemetry Collector for Kubernetes Go to your Splunk and execute search: index=\"em_logs\" \"Sending due task\" \"sc4snmp;<IP_ADDRESS>;walk\" and replace with the pertinent IP Address. Uninstall Splunk Connect for SNMP \u00b6 To uninstall SC4SNMP run the following commands: microk8s helm3 uninstall snmp -n sc4snmp microk8s kubectl delete pvc --all -n sc4snmp Installing Splunk Connect for SNMP on Linux RedHat \u00b6 Installation of RedHat may be blocking ports required by microk8s. Installing microk8s on RedHat requires checking to see if the firewall is not blocking any of required microk8s ports . Issues \u00b6 \u201cEmpty SNMP response message\u201d problem \u00b6 In case you see the following line in the worker\u2019s logs: [ 2022 - 01 - 04 11 : 44 : 22 , 553 : INFO / ForkPoolWorker - 1 ] Task splunk_connect_for_snmp . snmp . tasks . walk [ 8 e62fc62 - 569 c - 473 f - a765 - ff92577774e5 ] retry : Retry in 3489 s : SnmpActionError ( 'An error of SNMP isWalk=True for a host 192.168.10.20 occurred: Empty SNMP response message' ) that causes infinite retry of walk operation, add worker.ignoreEmptyVarbinds parameter to values.yaml and set it to true. An example configuration for a worker in values.yaml is: worker : ignoreEmptyVarbinds : true \u201cOID not increasing\u201d problem \u00b6 In case you see the following line in worker\u2019s logs: [ 2022 - 01 - 04 11 : 44 : 22 , 553 : INFO / ForkPoolWorker - 1 ] Task splunk_connect_for_snmp . snmp . tasks . walk [ 8 e62fc62 - 569 c - 473 f - a765 - ff92577774e5 ] retry : Retry in 3489 s : SnmpActionError ( 'An error of SNMP isWalk=True for a host 192.168.10.20 occurred: OID not increasing' ) that causes infinite retry of walk operation, add worker.ignoreNotIncreasingOid array to values.yaml and fill with the addresses of hosts where the problem appears. An example configuration for a worker in values.yaml is: worker : ignoreNotIncreasingOid : - \"127.0.0.1:164\" - \"127.0.0.6\" If you put only IP address (ex. 127.0.0.1 ), then errors will be ignored for all of its devices (like 127.0.0.1:161 , 127.0.0.1:163 \u2026). If you put IP address and host structured as {host}:{port} , that means the error will be ignored only for this device. Walking a device takes too much time \u00b6 Enable small walk functionality with the following instruction: Configure small walk profile . An error of SNMP isWalk=True blocks traffic on SC4SNMP instance \u00b6 If you see many An error of SNMP isWalk=True errors in logs, that means that there is a connection problem with the hosts you\u2019re polling from. Walk will try to retry multiple times, which will eventually cause a worker to be blocked for the retries time. In this case, you might want to limit the maximum retries time. You can do this by setting the variable worker.walkRetryMaxInterval , for example: worker : walkRetryMaxInterval : 60 With the configuration from the above, \u2018walk\u2019 will retry exponentially until it reaches 60 seconds. SNMP Rollover \u00b6 The Rollover problem is due to the integer value stored (especially when the value is 32-bit) being finite. When it reaches its maximum, it gets rolled down to 0 again. This causes a strange drop in Analytics data. The most common case of this issue is interface speed on high speed ports. As a solution to this problem, SNMPv2 SMI defined a new object type, counter64, for 64-bit counters ( read more about it ). Not all the devices support it, but if they do, poll the counter64 type OID instead of the counter32 one. For example, use ifHCInOctets instead of ifInOctets . If 64-bit counter are not supported on your device, you can write your own Splunk queries that calculate the shift based on maximum integer value + current state. The same works for values big enough that they\u2019re not fitting a 64-bit value. An example for a SPLUNK query like that (interface counter), would be: | streamstats current = f last ( ifInOctets ) as p_ifInOctets last ( ifOutOctets ) as p_ifOutOctets by ifAlias | eval in_delta = ( ifInOctets - p_ifInOctets ) | eval out_delta = ( ifOutOctets - p_ifOutOctets ) | eval max = pow ( 2 , 64 ) | eval out = if ( out_delta < 0 , (( max + out_delta ) * 8 / ( 5 * 60 * 1000 * 1000 * 1000 )) , ( out_delta ) * 8 / ( 5 * 60 * 1000 * 1000 * 1000 )) | timechart span = 5 m avg ( in ) AS in , avg ( out ) AS out by ifAlias","title":"Troubleshooting"},{"location":"bestpractices/#debug-splunk-connect-for-snmp","text":"","title":"Debug Splunk Connect for SNMP"},{"location":"bestpractices/#pieces-of-advice","text":"","title":"Pieces of Advice"},{"location":"bestpractices/#check-when-snmp-walk-was-executed-last-time-for-the-device","text":"Configure Splunk OpenTelemetry Collector for Kubernetes Go to your Splunk and execute search: index=\"em_logs\" \"Sending due task\" \"sc4snmp;<IP_ADDRESS>;walk\" and replace with the pertinent IP Address.","title":"Check when SNMP WALK was executed last time for the device"},{"location":"bestpractices/#uninstall-splunk-connect-for-snmp","text":"To uninstall SC4SNMP run the following commands: microk8s helm3 uninstall snmp -n sc4snmp microk8s kubectl delete pvc --all -n sc4snmp","title":"Uninstall Splunk Connect for SNMP"},{"location":"bestpractices/#installing-splunk-connect-for-snmp-on-linux-redhat","text":"Installation of RedHat may be blocking ports required by microk8s. Installing microk8s on RedHat requires checking to see if the firewall is not blocking any of required microk8s ports .","title":"Installing Splunk Connect for SNMP on Linux RedHat"},{"location":"bestpractices/#issues","text":"","title":"Issues"},{"location":"bestpractices/#empty-snmp-response-message-problem","text":"In case you see the following line in the worker\u2019s logs: [ 2022 - 01 - 04 11 : 44 : 22 , 553 : INFO / ForkPoolWorker - 1 ] Task splunk_connect_for_snmp . snmp . tasks . walk [ 8 e62fc62 - 569 c - 473 f - a765 - ff92577774e5 ] retry : Retry in 3489 s : SnmpActionError ( 'An error of SNMP isWalk=True for a host 192.168.10.20 occurred: Empty SNMP response message' ) that causes infinite retry of walk operation, add worker.ignoreEmptyVarbinds parameter to values.yaml and set it to true. An example configuration for a worker in values.yaml is: worker : ignoreEmptyVarbinds : true","title":"\"Empty SNMP response message\" problem"},{"location":"bestpractices/#oid-not-increasing-problem","text":"In case you see the following line in worker\u2019s logs: [ 2022 - 01 - 04 11 : 44 : 22 , 553 : INFO / ForkPoolWorker - 1 ] Task splunk_connect_for_snmp . snmp . tasks . walk [ 8 e62fc62 - 569 c - 473 f - a765 - ff92577774e5 ] retry : Retry in 3489 s : SnmpActionError ( 'An error of SNMP isWalk=True for a host 192.168.10.20 occurred: OID not increasing' ) that causes infinite retry of walk operation, add worker.ignoreNotIncreasingOid array to values.yaml and fill with the addresses of hosts where the problem appears. An example configuration for a worker in values.yaml is: worker : ignoreNotIncreasingOid : - \"127.0.0.1:164\" - \"127.0.0.6\" If you put only IP address (ex. 127.0.0.1 ), then errors will be ignored for all of its devices (like 127.0.0.1:161 , 127.0.0.1:163 \u2026). If you put IP address and host structured as {host}:{port} , that means the error will be ignored only for this device.","title":"\"OID not increasing\" problem"},{"location":"bestpractices/#walking-a-device-takes-too-much-time","text":"Enable small walk functionality with the following instruction: Configure small walk profile .","title":"Walking a device takes too much time"},{"location":"bestpractices/#an-error-of-snmp-iswalktrue-blocks-traffic-on-sc4snmp-instance","text":"If you see many An error of SNMP isWalk=True errors in logs, that means that there is a connection problem with the hosts you\u2019re polling from. Walk will try to retry multiple times, which will eventually cause a worker to be blocked for the retries time. In this case, you might want to limit the maximum retries time. You can do this by setting the variable worker.walkRetryMaxInterval , for example: worker : walkRetryMaxInterval : 60 With the configuration from the above, \u2018walk\u2019 will retry exponentially until it reaches 60 seconds.","title":"An error of SNMP isWalk=True blocks traffic on SC4SNMP instance"},{"location":"bestpractices/#snmp-rollover","text":"The Rollover problem is due to the integer value stored (especially when the value is 32-bit) being finite. When it reaches its maximum, it gets rolled down to 0 again. This causes a strange drop in Analytics data. The most common case of this issue is interface speed on high speed ports. As a solution to this problem, SNMPv2 SMI defined a new object type, counter64, for 64-bit counters ( read more about it ). Not all the devices support it, but if they do, poll the counter64 type OID instead of the counter32 one. For example, use ifHCInOctets instead of ifInOctets . If 64-bit counter are not supported on your device, you can write your own Splunk queries that calculate the shift based on maximum integer value + current state. The same works for values big enough that they\u2019re not fitting a 64-bit value. An example for a SPLUNK query like that (interface counter), would be: | streamstats current = f last ( ifInOctets ) as p_ifInOctets last ( ifOutOctets ) as p_ifOutOctets by ifAlias | eval in_delta = ( ifInOctets - p_ifInOctets ) | eval out_delta = ( ifOutOctets - p_ifOutOctets ) | eval max = pow ( 2 , 64 ) | eval out = if ( out_delta < 0 , (( max + out_delta ) * 8 / ( 5 * 60 * 1000 * 1000 * 1000 )) , ( out_delta ) * 8 / ( 5 * 60 * 1000 * 1000 * 1000 )) | timechart span = 5 m avg ( in ) AS in , avg ( out ) AS out by ifAlias","title":"SNMP Rollover"},{"location":"ha/","text":"High Availability Considerations \u00b6 The SNMP protocol uses UDP as the transport protocol. Network reliability is a constraint. Consider network architecture when designing for high availability: When using a single node collector, ensure automatic recovery from virtual infrastructure (i.e. VMware, Openstack, etc). When using a multi-node cluster, ensure nodes are not located such that a simple majority of nodes can be lost. For example, consider row, rack, network, power, and storage. When determining the placement of clusters, the closest location by the number of network hops should be utilized. For \u201cdata center\u201d applications, collection should be local to the data center. Consider IP Anycast.","title":"High Availability"},{"location":"ha/#high-availability-considerations","text":"The SNMP protocol uses UDP as the transport protocol. Network reliability is a constraint. Consider network architecture when designing for high availability: When using a single node collector, ensure automatic recovery from virtual infrastructure (i.e. VMware, Openstack, etc). When using a multi-node cluster, ensure nodes are not located such that a simple majority of nodes can be lost. For example, consider row, rack, network, power, and storage. When determining the placement of clusters, the closest location by the number of network hops should be utilized. For \u201cdata center\u201d applications, collection should be local to the data center. Consider IP Anycast.","title":"High Availability Considerations"},{"location":"mib-request/","text":"MIB submission process \u00b6 To achieve human-readable OIDs, the corresponding MIB files are necessary. They are being stored in one of the components of SC4SNMP - the MIB server. The list of currently available MIBs is here: https://pysnmp.github.io/mibs/index.csv An alternative way to check if the MIB you\u2019re interested in is being served is to check the link: https://pysnmp.github.io/mibs/asn1/@mib@ where @mib@ is the name of MIB (for example IF-MIB ). If the file is downloading, that means the MIB file exists in the mib server. Submit a new MIB file \u00b6 In case you want to add a new MIB file to the MIB server, follow the steps: Create a fork of the https://github.com/pysnmp/mibs repository Put MIB file/s under src/vendor/@vendor_name@ where @vendor_name@ is the name of the MIB file\u2019s vendor (in case there is no directory of vendors you need, create it by yourself) Create a pull request to a main branch Name the pull request the following way: feat: add @vendor_name@ MIB files An alternative way of adding MIBs to the MIB server is to create an issue on https://github.com/pysnmp/mibs repository, attaching the files and information about the vendor. Update your instance of SC4SNMP with the newest MIB server \u00b6 Usually SC4SNMP is released with the newest version of MIB server every time the new MIB files were added. But, if you want to use the newest MIB server right after its released, you can do it manually via the values.yaml file. Append mibserver config to the values.yaml, with the mibserver.image.tag of a value of the newest mibserver , for ex.: mibserver : image : tag : \"1.14.5\" Check all the MIB server releases here . Run microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace Restart worker-trap and worker-poller deployments: microk8s kubectl rollout restart deployment snmp - splunk - connect - for - snmp - worker - trap - n sc4snmp microk8s kubectl rollout restart deployment snmp - splunk - connect - for - snmp - worker - poller - n sc4snmp Beta: use MIB server with local MIBs \u00b6 From the 1.15.0 version of the MIB server, there is a way to use local MIB files. This may be useful when your MIB files are proprietary, or you use SC4SNMP offline - this way you can update necessary MIBs by yourself, without a need of going through the MIB request procedure. In order to add your MIB files to the MIB server in standalone SC4SNMP installation: Create/Choose a directory on the machine where SC4SNMP is installed. For example: /home/user/local_mibs . Create vendor directories inside. For example, if you have MIB files from VENDOR1 and VENDOR2 , create /home/user/local_mibs/VENDOR1 and /home/user/local_mibs/VENDOR2 and put files inside accordingly. Putting wrong vendor names won\u2019t make compilation fail, this is more for the logging purposes. Segregating your files will make troubleshooting easier. MIB files should be named the same as the contained MIB module. The MIB module name is specified at the beginning of the MIB file before ::= BEGIN keyword. Add following config to the values.yaml : mibserver : localMibs : pathToMibs : \"/home/user/local_mibs\" To verify if the process of compilation was completed successfully, check the mibserver logs with: microk8s kubectl logs -f deployments/snmp-mibserver -n sc4snmp It creates a Kubernetes pvc with MIB files inside and maps it to MIB server pod. Also, you can change the storageClass and size of persistence according to the mibserver schema: check here . The default persistence size is 1 Gibibyte, so consider reducing or expanding it to the amount you actually need. Whenever you add new MIB files, rollout restart MIB server pods to compile them again: microk8s kubectl rollout restart deployment snmp-mibserver -n sc4snmp NOTE: In case of multi-node Kubernetes installation, create pvc beforehand, copy files onto it and add to the MIB server using persistence.existingClaim . If you go with localMibs.pathToMibs solution in case of multi-node installation (with nodeSelector set up to schedule MIB server pods on the same node where the MIB files are), it will work - but when the Node with hostPath mapped fails, you\u2019ll use access to the MIB files on another node.","title":"Request MIB"},{"location":"mib-request/#mib-submission-process","text":"To achieve human-readable OIDs, the corresponding MIB files are necessary. They are being stored in one of the components of SC4SNMP - the MIB server. The list of currently available MIBs is here: https://pysnmp.github.io/mibs/index.csv An alternative way to check if the MIB you\u2019re interested in is being served is to check the link: https://pysnmp.github.io/mibs/asn1/@mib@ where @mib@ is the name of MIB (for example IF-MIB ). If the file is downloading, that means the MIB file exists in the mib server.","title":"MIB submission process"},{"location":"mib-request/#submit-a-new-mib-file","text":"In case you want to add a new MIB file to the MIB server, follow the steps: Create a fork of the https://github.com/pysnmp/mibs repository Put MIB file/s under src/vendor/@vendor_name@ where @vendor_name@ is the name of the MIB file\u2019s vendor (in case there is no directory of vendors you need, create it by yourself) Create a pull request to a main branch Name the pull request the following way: feat: add @vendor_name@ MIB files An alternative way of adding MIBs to the MIB server is to create an issue on https://github.com/pysnmp/mibs repository, attaching the files and information about the vendor.","title":"Submit a new MIB file"},{"location":"mib-request/#update-your-instance-of-sc4snmp-with-the-newest-mib-server","text":"Usually SC4SNMP is released with the newest version of MIB server every time the new MIB files were added. But, if you want to use the newest MIB server right after its released, you can do it manually via the values.yaml file. Append mibserver config to the values.yaml, with the mibserver.image.tag of a value of the newest mibserver , for ex.: mibserver : image : tag : \"1.14.5\" Check all the MIB server releases here . Run microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace Restart worker-trap and worker-poller deployments: microk8s kubectl rollout restart deployment snmp - splunk - connect - for - snmp - worker - trap - n sc4snmp microk8s kubectl rollout restart deployment snmp - splunk - connect - for - snmp - worker - poller - n sc4snmp","title":"Update your instance of SC4SNMP with the newest MIB server"},{"location":"mib-request/#beta-use-mib-server-with-local-mibs","text":"From the 1.15.0 version of the MIB server, there is a way to use local MIB files. This may be useful when your MIB files are proprietary, or you use SC4SNMP offline - this way you can update necessary MIBs by yourself, without a need of going through the MIB request procedure. In order to add your MIB files to the MIB server in standalone SC4SNMP installation: Create/Choose a directory on the machine where SC4SNMP is installed. For example: /home/user/local_mibs . Create vendor directories inside. For example, if you have MIB files from VENDOR1 and VENDOR2 , create /home/user/local_mibs/VENDOR1 and /home/user/local_mibs/VENDOR2 and put files inside accordingly. Putting wrong vendor names won\u2019t make compilation fail, this is more for the logging purposes. Segregating your files will make troubleshooting easier. MIB files should be named the same as the contained MIB module. The MIB module name is specified at the beginning of the MIB file before ::= BEGIN keyword. Add following config to the values.yaml : mibserver : localMibs : pathToMibs : \"/home/user/local_mibs\" To verify if the process of compilation was completed successfully, check the mibserver logs with: microk8s kubectl logs -f deployments/snmp-mibserver -n sc4snmp It creates a Kubernetes pvc with MIB files inside and maps it to MIB server pod. Also, you can change the storageClass and size of persistence according to the mibserver schema: check here . The default persistence size is 1 Gibibyte, so consider reducing or expanding it to the amount you actually need. Whenever you add new MIB files, rollout restart MIB server pods to compile them again: microk8s kubectl rollout restart deployment snmp-mibserver -n sc4snmp NOTE: In case of multi-node Kubernetes installation, create pvc beforehand, copy files onto it and add to the MIB server using persistence.existingClaim . If you go with localMibs.pathToMibs solution in case of multi-node installation (with nodeSelector set up to schedule MIB server pods on the same node where the MIB files are), it will work - but when the Node with hostPath mapped fails, you\u2019ll use access to the MIB files on another node.","title":"Beta: use MIB server with local MIBs"},{"location":"planning/","text":"Planning \u00b6 Splunk Connect for SNMP (SC4SNMP) is a solution that allows the customer to \"get\" data from network devices and appliances when a more feature-complete solution, such as the Splunk Universal Forwarder, is not available. Architecture \u00b6 SC4SNMP is deployed using a Kubernetes distribution, typically MicroK8s, that\u2019s designed to be a low-touch experience for integration with sensitive edge network devices. It will typically be deployed in the same network management zone as the monitored devices and separated from Splunk by an existing firewall. Requirements \u00b6 A supported deployment of MicroK8s 16 Core/32 threads x64 architecture server or vm (single instance) 12 GB ram HA Requires 3 or more instances (odd numbers) 8 core/16 thread 16 GB ram 50 GB root mount HTTP access (non-proxy) allowed for the HTTP(s) connection from SC4SNMP to the Splunk destination. Splunk Enterprise/Cloud 8.x or newer and/or Splunk Infrastructure Monitoring (SignalFx) Planning Infrastructure \u00b6 A single installation of Splunk Connect for SNMP (SC4SNMP) on a machine with 16 Core/32 threads x64 and 64 GB RAM will be able to handle up to 1500 SNMP TRAPs per second. A single installation of Splunk Connect for SNMP (SC4SNMP) on a machine with 16 Core/32 threads x64 and 64 GB RAM is able to handle up to 2750 SNMP varbinds per second. As for events per second visible in Splunk, please remember that a single SC4SNMP event can contain more than one varbind inside - auto aggregation/grouping feature (varbinds which are describing same thing ie. network interface will be grouped in one event). That is why, depending on configuration, the number of events per second may vary. When planning infrastructure for Splunk Connect for SNMP, (SC4SNMP) note the limitations highlighted above.","title":"Planning"},{"location":"planning/#planning","text":"Splunk Connect for SNMP (SC4SNMP) is a solution that allows the customer to \"get\" data from network devices and appliances when a more feature-complete solution, such as the Splunk Universal Forwarder, is not available.","title":"Planning"},{"location":"planning/#architecture","text":"SC4SNMP is deployed using a Kubernetes distribution, typically MicroK8s, that\u2019s designed to be a low-touch experience for integration with sensitive edge network devices. It will typically be deployed in the same network management zone as the monitored devices and separated from Splunk by an existing firewall.","title":"Architecture"},{"location":"planning/#requirements","text":"A supported deployment of MicroK8s 16 Core/32 threads x64 architecture server or vm (single instance) 12 GB ram HA Requires 3 or more instances (odd numbers) 8 core/16 thread 16 GB ram 50 GB root mount HTTP access (non-proxy) allowed for the HTTP(s) connection from SC4SNMP to the Splunk destination. Splunk Enterprise/Cloud 8.x or newer and/or Splunk Infrastructure Monitoring (SignalFx)","title":"Requirements"},{"location":"planning/#planning-infrastructure","text":"A single installation of Splunk Connect for SNMP (SC4SNMP) on a machine with 16 Core/32 threads x64 and 64 GB RAM will be able to handle up to 1500 SNMP TRAPs per second. A single installation of Splunk Connect for SNMP (SC4SNMP) on a machine with 16 Core/32 threads x64 and 64 GB RAM is able to handle up to 2750 SNMP varbinds per second. As for events per second visible in Splunk, please remember that a single SC4SNMP event can contain more than one varbind inside - auto aggregation/grouping feature (varbinds which are describing same thing ie. network interface will be grouped in one event). That is why, depending on configuration, the number of events per second may vary. When planning infrastructure for Splunk Connect for SNMP, (SC4SNMP) note the limitations highlighted above.","title":"Planning Infrastructure"},{"location":"releases/","text":"Base Information \u00b6 Known Issues \u00b6 List of open known issues is available under Known issue link Open issues to the product \u00b6 To open issue for Splunk Connect for SNMP go to github SC4SNMP project and open issue. Releases \u00b6 To check Splunk Connect for SNMP releases please visit: SC4SNMP Releases","title":"Releases"},{"location":"releases/#base-information","text":"","title":"Base Information"},{"location":"releases/#known-issues","text":"List of open known issues is available under Known issue link","title":"Known Issues"},{"location":"releases/#open-issues-to-the-product","text":"To open issue for Splunk Connect for SNMP go to github SC4SNMP project and open issue.","title":"Open issues to the product"},{"location":"releases/#releases","text":"To check Splunk Connect for SNMP releases please visit: SC4SNMP Releases","title":"Releases"},{"location":"security/","text":"Security Considerations \u00b6 The SC4SNMP solution implements SNMP in a compatible mode for current and legacy network device gear. SNMP is a protocol widely considered to be risky and requires threat mitigation at the network level. Do not expose SNMP endpoints to untrusted connections such as the internet or general LAN network of a typical enterprise. Do not allow SNMPv1 or SNMPv2 connections to cross a network zone where a man in the middle interception is possible. Be aware many SNMPv3 devices rely on insecure cryptography including DES, MD5, and SHA. Do not presume SNMPv3 devices and connections are secure by default. When possible use SNMPv3 with the most secure protocol options mutually supported. The default IP of each node should be considered a management interface and should be protected from network access by an untrusted device by a hardware or software firewall. When possible the IP allocated for SNMP communication should not be shared by the management interface.","title":"Security"},{"location":"security/#security-considerations","text":"The SC4SNMP solution implements SNMP in a compatible mode for current and legacy network device gear. SNMP is a protocol widely considered to be risky and requires threat mitigation at the network level. Do not expose SNMP endpoints to untrusted connections such as the internet or general LAN network of a typical enterprise. Do not allow SNMPv1 or SNMPv2 connections to cross a network zone where a man in the middle interception is possible. Be aware many SNMPv3 devices rely on insecure cryptography including DES, MD5, and SHA. Do not presume SNMPv3 devices and connections are secure by default. When possible use SNMPv3 with the most secure protocol options mutually supported. The default IP of each node should be considered a management interface and should be protected from network access by an untrusted device by a hardware or software firewall. When possible the IP allocated for SNMP communication should not be shared by the management interface.","title":"Security Considerations"},{"location":"small-environment/","text":"Lightweight SC4SNMP installation \u00b6 SC4SNMP can be successfully installed in small environments with 2 CPUs and 4 GB of memory. One important thing to remember is that Splunk OpenTelemetry Collector for Kubernetes cannot be installed in such a small environment along with SC4SNMP. The other difference from normal installation is that the resources limits must be set for Kubernetes pods. See the example of values.yaml with the appropriate resources here . The rest of the installation is the same as in online , or the offline installation. Keep in mind that a lightweight instance of SC4SNMP won\u2019t be able to poll from many devices and may experience delays if there is frequent polling.","title":"Lightweight installation"},{"location":"small-environment/#lightweight-sc4snmp-installation","text":"SC4SNMP can be successfully installed in small environments with 2 CPUs and 4 GB of memory. One important thing to remember is that Splunk OpenTelemetry Collector for Kubernetes cannot be installed in such a small environment along with SC4SNMP. The other difference from normal installation is that the resources limits must be set for Kubernetes pods. See the example of values.yaml with the appropriate resources here . The rest of the installation is the same as in online , or the offline installation. Keep in mind that a lightweight instance of SC4SNMP won\u2019t be able to poll from many devices and may experience delays if there is frequent polling.","title":"Lightweight SC4SNMP installation"},{"location":"upgrade/","text":"Upgrading SC4SNMP \u00b6 Upgrade to the latest version \u00b6 To upgrade SC4SNMP to the latest version, simply run the following command: microk8s helm3 repo update Afterwards, run: microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace SC4SNMP will be upgraded to the newest version. You can see the latest version after hitting the command: microk8s helm3 search repo snmp The output looks like that: NAME CHART VERSION APP VERSION DESCRIPTION splunk - connect - for - snmp / splunk - connect - for - snmp 1 . 6 . 2 1 . 6 . 2 A Helm chart for SNMP Connect for SNMP So in this case, the latest version is 1.6.2 and it will be installed after helm3 upgrade command. Upgrade to a specific version \u00b6 Alternatively, you can install one of the previous versions, or a development one. You can list all the previous versions with: microk8s helm3 search repo snmp --versions And all the development versions: microk8s helm3 search repo snmp --devel To upgrade your SC4SNMP instance to any of the listed versions, run helm3 upgrade with the --version flag: microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace --version <VERSION> For example: microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace --version 1.6.3-beta.13","title":"Upgrade SC4SNMP"},{"location":"upgrade/#upgrading-sc4snmp","text":"","title":"Upgrading SC4SNMP"},{"location":"upgrade/#upgrade-to-the-latest-version","text":"To upgrade SC4SNMP to the latest version, simply run the following command: microk8s helm3 repo update Afterwards, run: microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace SC4SNMP will be upgraded to the newest version. You can see the latest version after hitting the command: microk8s helm3 search repo snmp The output looks like that: NAME CHART VERSION APP VERSION DESCRIPTION splunk - connect - for - snmp / splunk - connect - for - snmp 1 . 6 . 2 1 . 6 . 2 A Helm chart for SNMP Connect for SNMP So in this case, the latest version is 1.6.2 and it will be installed after helm3 upgrade command.","title":"Upgrade to the latest version"},{"location":"upgrade/#upgrade-to-a-specific-version","text":"Alternatively, you can install one of the previous versions, or a development one. You can list all the previous versions with: microk8s helm3 search repo snmp --versions And all the development versions: microk8s helm3 search repo snmp --devel To upgrade your SC4SNMP instance to any of the listed versions, run helm3 upgrade with the --version flag: microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace --version <VERSION> For example: microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace --version 1.6.3-beta.13","title":"Upgrade to a specific version"},{"location":"configuration/configuring-groups/","text":"Configuring Groups \u00b6 It is common to configure whole groups of devices instead of just single ones. SC4SNMP allows both types of configuration. Group consists of many hosts. Each of them is configured in the values.yaml file, in the scheduler section. After configuring a group, its name can be used in the address field in the inventory record. All settings specified in the inventory record will be assigned to hosts from the given group, unless specific host configuration overrides it. Group configuration example and documentation can be found in the Scheduler Configuration page. Use of groups in the inventory can be found in the Poller Configuration page. If the host is configured in the group and both the group and the single host are included in the inventory (like in the example below), the configuration for the single host will be ignored in favour of group configuration: scheduler : groups : | example_group_1: - address: 10.202.4.202 port: 161 - address: 63.2.40.0 port: 161 poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete example_group_1,,2c,public,,,2000,my_profile2,, 10.202.4.202,,2c,public,,,2000,my_profile1,, If the specific host from the group has to be configured separately, first it must be deleted from the group configuration, and then it can be inserted as a new record in the inventory (like in the example below): scheduler : groups : | example_group_1: - address: 63.2.40.0 port: 161 poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete example_group_1,,2c,public,,,2000,my_profile2,, 10.202.4.202,,2c,public,,,2000,my_profile1,,","title":"Configuring Groups"},{"location":"configuration/configuring-groups/#configuring-groups","text":"It is common to configure whole groups of devices instead of just single ones. SC4SNMP allows both types of configuration. Group consists of many hosts. Each of them is configured in the values.yaml file, in the scheduler section. After configuring a group, its name can be used in the address field in the inventory record. All settings specified in the inventory record will be assigned to hosts from the given group, unless specific host configuration overrides it. Group configuration example and documentation can be found in the Scheduler Configuration page. Use of groups in the inventory can be found in the Poller Configuration page. If the host is configured in the group and both the group and the single host are included in the inventory (like in the example below), the configuration for the single host will be ignored in favour of group configuration: scheduler : groups : | example_group_1: - address: 10.202.4.202 port: 161 - address: 63.2.40.0 port: 161 poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete example_group_1,,2c,public,,,2000,my_profile2,, 10.202.4.202,,2c,public,,,2000,my_profile1,, If the specific host from the group has to be configured separately, first it must be deleted from the group configuration, and then it can be inserted as a new record in the inventory (like in the example below): scheduler : groups : | example_group_1: - address: 63.2.40.0 port: 161 poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete example_group_1,,2c,public,,,2000,my_profile2,, 10.202.4.202,,2c,public,,,2000,my_profile1,,","title":"Configuring Groups"},{"location":"configuration/configuring-profiles/","text":"Configuring profiles \u00b6 Profiles are the units where you can configure what you want to poll, and then assign them to the device. The definition of profile can be found in the values.yaml file under the scheduler section. Here are the instructions on how to use profiles: Update Inventory and Profile . There are two types of profiles in general: Static profile - polling starts when the profile is added to the profiles field in the inventory of the device. Smart profile - polling starts when configured conditions are fulfilled, and the device to poll from has smart_profiles enabled in inventory. Smart profiles are useful when we have many devices of a certain kind, and we don\u2019t want to configure each of them individually with static profiles. In order to configure smart profile, do the following: Choose one of the fields polled from the device, most commonly sysDescr. Set the filter to match all the devices of this kind. Setup polling of the profile by enabling smart profiles for devices you want to be polled. The template of the profile looks like the following: scheduler : profiles : | #Name of profile basev1: # Define frequency for profile frequency: 100 #Define condition condition: # Define type of condition. Allowed value field, base and walk type: field field: \"SNMPv2-MIB.sysDescr\" # Define paterns patterns: - '.*STRING_TO_BE_MATCHED.*' #Define varbinds to query varBinds: # Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]] - ['SNMPv2-MIB'] - ['SNMPv2-MIB', 'sysName'] - ['SNMPv2-MIB', 'sysUpTime',0] For example, we have configured two profiles. One is smart, and the other one is static: scheduler : profiles : | smart_profile: frequency: 100 condition: type: field field: \"SNMPv2-MIB.sysDescr\" patterns: - '.*linux.*' varBinds: - ['SNMPv2-MIB'] - ['SNMPv2-MIB', 'sysName'] - ['SNMPv2-MIB', 'sysUpTime',0] static_profile: frequency: 300 varBinds: - ['IP-MIB'] If we want to enable only static_profile polling for the host 10.202.4.202 , we will configure similar inventory: poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10.202.4.202,,2c,public,,,2000,static_profile,f, If we want to enable checking the 10.202.4.202 device against smart profiles, we need to set smart_profiles to t : poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10.202.4.202,,2c,public,,,2000,,t, Then, if the device sysDescr matches the '.*linux.*' filter, the smart_profile profile will be polled. varBinds configuration \u00b6 varBinds is short for \u201cvariable binding\u201d in the SNMP. It is the combination of an Object Identifier (OID) and a value. varBinds are used for defining what OIDs should be requested from SNMP Agents. varBinds is a required subsection of each profile. The syntax configuration of varBinds looks like the following: [ \u201cMIB-Component\u201d, \u201cMIB object\u201d[Optional], \u201cMIB index number\u201d[Optional]] MIB-Component - The SNMP MIB itself consists of distinct component MIBs, each of which refers to a specific defined collection of management information that is part of the overall SNMP MIB, eg., SNMPv2-MIB . If only the MIB-Component is set, then the SC4SNMP will get the whole subtree. MIB object - The SNMP MIB stores only simple data types: scalars and two-dimensional arrays of scalars, called tables. The keywords SYNTAX, ACCESS, and DESCRIPTION as well as other keywords such as STATUS and INDEX are used to define the SNMP MIB managed objects. MIB index number - Define index number for given MIB Object eg. 0 . Example: varBinds : # Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]] - [ 'SNMPv2-MIB' ] - [ 'SNMPv2-MIB' , 'sysName' ] - [ 'SNMPv2-MIB' , 'sysUpTime' , 0 ] Static Profile configuration \u00b6 Static Profile is used when they are defined on a list of profiles in the inventory configuration in the poller service Inventory configuration . Static Profiles are executed even if the SmartProfile flag in inventory is set to false. To configure Static Profile value needs to be set in the profiles section: ProfileName - define as subsection key in profiles . frequency - define interval between executing SNMP gets in second. varBinds - define var binds to query. Example: scheduler : profiles : | static_profile_example: frequency: 20 varBinds: - ['SNMPv2-MIB'] - ['SNMPv2-MIB', 'sysName'] - ['SNMPv2-MIB', 'sysUpTime',0] Particular kinds of static profiles \u00b6 Sometimes static profiles have additional functionalities to be used in specific scenarios. WALK profile \u00b6 If you would like to limit the scope of the walk, you should set one of the profiles in the inventory to point to the profile definition of type walk : scheduler : profiles : | small_walk: condition: type: \"walk\" varBinds: - ['UDP-MIB'] This profile should be placed in the profiles section of the inventory definition. It will be executed with the frequency defined in walk_interval . If multiple profiles of type walk is placed in profiles, the last one will be used. This is how to use walk profiles: poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10.202.4.202,,2c,public,,,2000,small_walk,, NOTE: When small walk is configured, SNMPv2-MIB is enabled by default (we need it to create the state of the device in the database). For example, if you\u2019ve decided to use small_walk from the example above, you\u2019ll be able to poll only UDP-MIB , and SNMPv2-MIB OIDs. SmartProfile configuration \u00b6 SmartProfile is executed when the SmartProfile flag in inventory is set to true and the condition defined in profile match. More information about configuring inventory can be found in Inventory configuration . To configure Smart Profile, the following value needs to be set in the profiles section: ProfileName - define as subsection key in profiles . frequency - define an interval between executing SNMP\u2019s gets in second. condition - section define conditions to match profile type - key of condition section which defines type of condition. The allowed values are base and field ( walk type is also allowed here, but it\u2019s not part of smart profiles). base type of condition will be executed when SmartProfile in inventory is set to true. field type of condition will be executed if it matches pattern for defined field . Supported fields are: \u201cSNMPv2-MIB.sysDescr\u201d \u201cSNMPv2-MIB.sysObjectID\u201d field Define field name for condition type field. pattern Define list of regular expression patterns for MIB object field defined in field section. For example: - \u201c. linux. \u201c varBinds - define var binds to query. Example of base type profile: scheduler : profiles : | SmartProfile_base_example: frequency: 100 condition: type: \"base\" varBinds: - ['SNMPv2-MIB'] - ['SNMPv2-MIB', 'sysName'] Example of field type profile, also called an automatic profile: scheduler : profiles : | SmartProfile_field_example: frequency: 100 condition: type: \"field\" field: \"SNMPv2-MIB.sysDescr\" patterns: - '.*STRING_TO_BE_MATCHED.*' varBinds: - ['SNMPv2-MIB'] - ['SNMPv2-MIB', 'sysName'] NOTE: Be aware that profile changes may not be reflected immediately. It can take up to 1 minute for changes to propagate. In case you changed frequency, or a profile type, the change will be reflected only after the next walk. There is also 5 minute TTL for an inventory pod. Basically, SC4SNMP allows one inventory upgrade and then block updates for the next 5 minutes. Conditional profiles \u00b6 There is a way to not explicitly give what SNMP objects we want to poll - only the conditions that must be fulfilled to qualify object for polling. An example of a conditional profile is: IF_conditional_profile : frequency : 30 conditions : - field : IF-MIB.ifAdminStatus operation : \"equals\" value : \"up\" - field : IF-MIB.ifOperStatus operation : \"equals\" value : \"up\" varBinds : - [ 'IF-MIB' , 'ifDescr' ] - [ 'IF-MIB' , 'ifAlias' ] - [ 'IF-MIB' , 'ifInErrors' ] - [ 'IF-MIB' , 'ifOutDiscards' ] When the such profile is defined and added to a device in an inventory, it will poll all interfaces where ifAdminStatus and ifOperStatus is up. Note that conditional profiles are being evaluated during the walk process (on every walk_interval ) and if the status changes in between, the scope of the conditional profile won\u2019t be modified. These are operations possible to use in conditional profiles: equals - value gathered from field is equal to value gt - value gathered from field is bigger than value (works only for numeric values) lt - value gathered from field is smaller than value (works only for numeric values) in - value gathered from field is equal to one of the elements provided in value , for ex.: conditions : - field : IF-MIB.ifAdminStatus operation : \"in\" value : - \"down\" - 0 field part of conditions must fulfill the pattern MIB-family.field . Fields must represent textual value (not metric one), you can learn more about it here . You have to explicitly define varBinds (not only the MIB family but also the field to poll), so such config: varBinds : - [ 'IF-MIB' ] is not correct. Custom translations \u00b6 If the user wants to use custom names/translations of MIB names, it can be configured under the customTranslations section under scheduler config. Translations are grouped by MIB family. In the example below IF-MIB.ifInDiscards will be translated to IF-MIB.myCustomName1: scheduler : customTranslations : IF-MIB : ifInDiscards : myCustomName1 ifOutErrors : myCustomName2 SNMPv2-MIB : sysDescr : myCustomName3","title":"Configuring Profiles"},{"location":"configuration/configuring-profiles/#configuring-profiles","text":"Profiles are the units where you can configure what you want to poll, and then assign them to the device. The definition of profile can be found in the values.yaml file under the scheduler section. Here are the instructions on how to use profiles: Update Inventory and Profile . There are two types of profiles in general: Static profile - polling starts when the profile is added to the profiles field in the inventory of the device. Smart profile - polling starts when configured conditions are fulfilled, and the device to poll from has smart_profiles enabled in inventory. Smart profiles are useful when we have many devices of a certain kind, and we don\u2019t want to configure each of them individually with static profiles. In order to configure smart profile, do the following: Choose one of the fields polled from the device, most commonly sysDescr. Set the filter to match all the devices of this kind. Setup polling of the profile by enabling smart profiles for devices you want to be polled. The template of the profile looks like the following: scheduler : profiles : | #Name of profile basev1: # Define frequency for profile frequency: 100 #Define condition condition: # Define type of condition. Allowed value field, base and walk type: field field: \"SNMPv2-MIB.sysDescr\" # Define paterns patterns: - '.*STRING_TO_BE_MATCHED.*' #Define varbinds to query varBinds: # Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]] - ['SNMPv2-MIB'] - ['SNMPv2-MIB', 'sysName'] - ['SNMPv2-MIB', 'sysUpTime',0] For example, we have configured two profiles. One is smart, and the other one is static: scheduler : profiles : | smart_profile: frequency: 100 condition: type: field field: \"SNMPv2-MIB.sysDescr\" patterns: - '.*linux.*' varBinds: - ['SNMPv2-MIB'] - ['SNMPv2-MIB', 'sysName'] - ['SNMPv2-MIB', 'sysUpTime',0] static_profile: frequency: 300 varBinds: - ['IP-MIB'] If we want to enable only static_profile polling for the host 10.202.4.202 , we will configure similar inventory: poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10.202.4.202,,2c,public,,,2000,static_profile,f, If we want to enable checking the 10.202.4.202 device against smart profiles, we need to set smart_profiles to t : poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10.202.4.202,,2c,public,,,2000,,t, Then, if the device sysDescr matches the '.*linux.*' filter, the smart_profile profile will be polled.","title":"Configuring profiles"},{"location":"configuration/configuring-profiles/#varbinds-configuration","text":"varBinds is short for \u201cvariable binding\u201d in the SNMP. It is the combination of an Object Identifier (OID) and a value. varBinds are used for defining what OIDs should be requested from SNMP Agents. varBinds is a required subsection of each profile. The syntax configuration of varBinds looks like the following: [ \u201cMIB-Component\u201d, \u201cMIB object\u201d[Optional], \u201cMIB index number\u201d[Optional]] MIB-Component - The SNMP MIB itself consists of distinct component MIBs, each of which refers to a specific defined collection of management information that is part of the overall SNMP MIB, eg., SNMPv2-MIB . If only the MIB-Component is set, then the SC4SNMP will get the whole subtree. MIB object - The SNMP MIB stores only simple data types: scalars and two-dimensional arrays of scalars, called tables. The keywords SYNTAX, ACCESS, and DESCRIPTION as well as other keywords such as STATUS and INDEX are used to define the SNMP MIB managed objects. MIB index number - Define index number for given MIB Object eg. 0 . Example: varBinds : # Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]] - [ 'SNMPv2-MIB' ] - [ 'SNMPv2-MIB' , 'sysName' ] - [ 'SNMPv2-MIB' , 'sysUpTime' , 0 ]","title":"varBinds configuration"},{"location":"configuration/configuring-profiles/#static-profile-configuration","text":"Static Profile is used when they are defined on a list of profiles in the inventory configuration in the poller service Inventory configuration . Static Profiles are executed even if the SmartProfile flag in inventory is set to false. To configure Static Profile value needs to be set in the profiles section: ProfileName - define as subsection key in profiles . frequency - define interval between executing SNMP gets in second. varBinds - define var binds to query. Example: scheduler : profiles : | static_profile_example: frequency: 20 varBinds: - ['SNMPv2-MIB'] - ['SNMPv2-MIB', 'sysName'] - ['SNMPv2-MIB', 'sysUpTime',0]","title":"Static Profile configuration"},{"location":"configuration/configuring-profiles/#particular-kinds-of-static-profiles","text":"Sometimes static profiles have additional functionalities to be used in specific scenarios.","title":"Particular kinds of static profiles"},{"location":"configuration/configuring-profiles/#walk-profile","text":"If you would like to limit the scope of the walk, you should set one of the profiles in the inventory to point to the profile definition of type walk : scheduler : profiles : | small_walk: condition: type: \"walk\" varBinds: - ['UDP-MIB'] This profile should be placed in the profiles section of the inventory definition. It will be executed with the frequency defined in walk_interval . If multiple profiles of type walk is placed in profiles, the last one will be used. This is how to use walk profiles: poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10.202.4.202,,2c,public,,,2000,small_walk,, NOTE: When small walk is configured, SNMPv2-MIB is enabled by default (we need it to create the state of the device in the database). For example, if you\u2019ve decided to use small_walk from the example above, you\u2019ll be able to poll only UDP-MIB , and SNMPv2-MIB OIDs.","title":"WALK profile"},{"location":"configuration/configuring-profiles/#smartprofile-configuration","text":"SmartProfile is executed when the SmartProfile flag in inventory is set to true and the condition defined in profile match. More information about configuring inventory can be found in Inventory configuration . To configure Smart Profile, the following value needs to be set in the profiles section: ProfileName - define as subsection key in profiles . frequency - define an interval between executing SNMP\u2019s gets in second. condition - section define conditions to match profile type - key of condition section which defines type of condition. The allowed values are base and field ( walk type is also allowed here, but it\u2019s not part of smart profiles). base type of condition will be executed when SmartProfile in inventory is set to true. field type of condition will be executed if it matches pattern for defined field . Supported fields are: \u201cSNMPv2-MIB.sysDescr\u201d \u201cSNMPv2-MIB.sysObjectID\u201d field Define field name for condition type field. pattern Define list of regular expression patterns for MIB object field defined in field section. For example: - \u201c. linux. \u201c varBinds - define var binds to query. Example of base type profile: scheduler : profiles : | SmartProfile_base_example: frequency: 100 condition: type: \"base\" varBinds: - ['SNMPv2-MIB'] - ['SNMPv2-MIB', 'sysName'] Example of field type profile, also called an automatic profile: scheduler : profiles : | SmartProfile_field_example: frequency: 100 condition: type: \"field\" field: \"SNMPv2-MIB.sysDescr\" patterns: - '.*STRING_TO_BE_MATCHED.*' varBinds: - ['SNMPv2-MIB'] - ['SNMPv2-MIB', 'sysName'] NOTE: Be aware that profile changes may not be reflected immediately. It can take up to 1 minute for changes to propagate. In case you changed frequency, or a profile type, the change will be reflected only after the next walk. There is also 5 minute TTL for an inventory pod. Basically, SC4SNMP allows one inventory upgrade and then block updates for the next 5 minutes.","title":"SmartProfile configuration"},{"location":"configuration/configuring-profiles/#conditional-profiles","text":"There is a way to not explicitly give what SNMP objects we want to poll - only the conditions that must be fulfilled to qualify object for polling. An example of a conditional profile is: IF_conditional_profile : frequency : 30 conditions : - field : IF-MIB.ifAdminStatus operation : \"equals\" value : \"up\" - field : IF-MIB.ifOperStatus operation : \"equals\" value : \"up\" varBinds : - [ 'IF-MIB' , 'ifDescr' ] - [ 'IF-MIB' , 'ifAlias' ] - [ 'IF-MIB' , 'ifInErrors' ] - [ 'IF-MIB' , 'ifOutDiscards' ] When the such profile is defined and added to a device in an inventory, it will poll all interfaces where ifAdminStatus and ifOperStatus is up. Note that conditional profiles are being evaluated during the walk process (on every walk_interval ) and if the status changes in between, the scope of the conditional profile won\u2019t be modified. These are operations possible to use in conditional profiles: equals - value gathered from field is equal to value gt - value gathered from field is bigger than value (works only for numeric values) lt - value gathered from field is smaller than value (works only for numeric values) in - value gathered from field is equal to one of the elements provided in value , for ex.: conditions : - field : IF-MIB.ifAdminStatus operation : \"in\" value : - \"down\" - 0 field part of conditions must fulfill the pattern MIB-family.field . Fields must represent textual value (not metric one), you can learn more about it here . You have to explicitly define varBinds (not only the MIB family but also the field to poll), so such config: varBinds : - [ 'IF-MIB' ] is not correct.","title":"Conditional profiles"},{"location":"configuration/configuring-profiles/#custom-translations","text":"If the user wants to use custom names/translations of MIB names, it can be configured under the customTranslations section under scheduler config. Translations are grouped by MIB family. In the example below IF-MIB.ifInDiscards will be translated to IF-MIB.myCustomName1: scheduler : customTranslations : IF-MIB : ifInDiscards : myCustomName1 ifOutErrors : myCustomName2 SNMPv2-MIB : sysDescr : myCustomName3","title":"Custom translations"},{"location":"configuration/deployment-configuration/","text":"Deployment Configuration \u00b6 values.yaml is the main point of SC4SNMP management. You can check all the default values of Helm dependencies using: microk8s helm3 inspect values splunk - connect - for - snmp / splunk - connect - for - snmp > values . yaml The whole file is divided into the following parts: For configuring endpoint for sending SNMP data: splunk - in case you use Splunk Enterprise/Cloud sim - in case you use Splunk Observability Cloud. More details: sim configuration For polling purposes: scheduler - more details: scheduler configuration poller - more details: poller configuration For traps receiving purposes: traps - more details: trap configuration Shared components: worker - more details: worker configuration mongodb - more details: mongo configuration redis - more details: redis configuration Shared values \u00b6 All the components have the resources field for adjusting memory resources: resources : limits : cpu : 1000m memory : 2Gi requests : cpu : 1000m memory : 2Gi More information about the concept of resources can be found in the kuberentes documentation . There is an option to create common annotations across all the services. It can be set by: commonAnnotations : annotation_key : annotation_value","title":"Deployment"},{"location":"configuration/deployment-configuration/#deployment-configuration","text":"values.yaml is the main point of SC4SNMP management. You can check all the default values of Helm dependencies using: microk8s helm3 inspect values splunk - connect - for - snmp / splunk - connect - for - snmp > values . yaml The whole file is divided into the following parts: For configuring endpoint for sending SNMP data: splunk - in case you use Splunk Enterprise/Cloud sim - in case you use Splunk Observability Cloud. More details: sim configuration For polling purposes: scheduler - more details: scheduler configuration poller - more details: poller configuration For traps receiving purposes: traps - more details: trap configuration Shared components: worker - more details: worker configuration mongodb - more details: mongo configuration redis - more details: redis configuration","title":"Deployment Configuration"},{"location":"configuration/deployment-configuration/#shared-values","text":"All the components have the resources field for adjusting memory resources: resources : limits : cpu : 1000m memory : 2Gi requests : cpu : 1000m memory : 2Gi More information about the concept of resources can be found in the kuberentes documentation . There is an option to create common annotations across all the services. It can be set by: commonAnnotations : annotation_key : annotation_value","title":"Shared values"},{"location":"configuration/mongo-configuration/","text":"Mongo DB Configuration \u00b6 Mongo DB is used as the database for keeping schedules. Mongo DB configuration file \u00b6 Mongo DB configuration is kept in the values.yaml file in the mongodb section. values.yaml is used during the installation process for configuring kubernetes values. Example: mongodb : #Architecture, Architecture for Mongo deployments is immutable to move from standalone to replicaset will require a uninstall. # \"replicaset\" for HA or multi node deployments # \"standalone\" for single node non HA #architecture: \"standalone\" pdb : create : true #The following requests and limits are appropriate starting points #For productions deployments resources : limits : cpu : 2 memory : 2Gi requests : cpu : 750m memory : 512Mi persistence : storageClass : \"microk8s-hostpath\" volumePermissions : enabled : true It is recommended not to change this setting. If it is necessary to change it, see: MongoDB on Kubernetes","title":"Mongo DB"},{"location":"configuration/mongo-configuration/#mongo-db-configuration","text":"Mongo DB is used as the database for keeping schedules.","title":"Mongo DB Configuration"},{"location":"configuration/mongo-configuration/#mongo-db-configuration-file","text":"Mongo DB configuration is kept in the values.yaml file in the mongodb section. values.yaml is used during the installation process for configuring kubernetes values. Example: mongodb : #Architecture, Architecture for Mongo deployments is immutable to move from standalone to replicaset will require a uninstall. # \"replicaset\" for HA or multi node deployments # \"standalone\" for single node non HA #architecture: \"standalone\" pdb : create : true #The following requests and limits are appropriate starting points #For productions deployments resources : limits : cpu : 2 memory : 2Gi requests : cpu : 750m memory : 512Mi persistence : storageClass : \"microk8s-hostpath\" volumePermissions : enabled : true It is recommended not to change this setting. If it is necessary to change it, see: MongoDB on Kubernetes","title":"Mongo DB configuration file"},{"location":"configuration/poller-configuration/","text":"Poller Configuration \u00b6 Poller is a service which is responsible for querying SNMP devices using the SNMP GET, and the SNMP WALK functionality. Poller executes two main types of tasks: Walk task - executes SNMP walk. SNMP walk is an SNMP application that uses SNMP GETNEXT requests to collect SNMP data from the network and infrastructure SNMP-enabled devices, such as switches and routers. It is a time-consuming task, which may overload the SNMP device when executed too often. It is used by the SC4SNMP to collect and push all OID values, which the provided ACL has access to. Get task - it is a lightweight task whose goal is to query a subset of OIDs defined by the customer. The task serves frequent monitoring OIDs, like memory or CPU utilization. Poller has an inventory , which defines what and how often SC4SNMP has to poll. Poller configuration file \u00b6 The poller configuration is kept in a values.yaml file in the poller section. values.yaml is used during the installation process for configuring Kubernetes values. Poller example configuration: poller : usernameSecrets : - sc4snmp-hlab-sha-aes - sc4snmp-hlab-sha-des logLevel : \"WARN\" inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10.202.4.202,,2c,public,,,2000,,, NOTE: header\u2019s line ( address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete ) is necessary for the correct execution of SC4SNMP. Do not remove it. Define log level \u00b6 The log level for poller can be set by changing the value for the key logLevel . The allowed values are: DEBUG , INFO , WARNING , ERROR . The default value is WARNING . Define usernameSecrets \u00b6 Secrets are required to run SNMPv3 polling. To add v3 authentication details, create the k8s Secret object: SNMPv3 Configuration , and put its name in poller.usernameSecrets . Append OID index part to the metrics \u00b6 Not every SNMP metric object is structured the way it has its index as a one of the field value. We can append the index part of OID with: poller : metricsIndexingEnabled : true So the following change will make this metric object (derived from the OID 1.3.6.1.2.1.6.20.1.4.0.0.443 ) { frequency: 5 metric_name:sc4snmp.TCP-MIB.tcpListenerProcess: 309 mibIndex: 0.0.443 profiles: generic_switch } out of this object: { frequency: 5 metric_name:sc4snmp.TCP-MIB.tcpListenerProcess: 309 profiles: generic_switch } Not every SNMP metric object is structured the way it has its index as a one of the field value. We can append the index part of OID with: poller : metricsIndexingEnabled : true Disable automatic polling of base profiles \u00b6 There are two profiles that are being polled by default - so that even without any configuration you can see the data in Splunk. You can disable it with pollBaseProfiles parameter. poller : pollBaseProfiles : false Configure inventory \u00b6 To update inventory, see: Update Inventory and Profile . inventory section in poller has the following fields to configure: address [REQUIRED] - IP address which SC4SNMP should connect to collect data from or name of the group of hosts. General information about groups can be found on Configuring Groups page. port [OPTIONAL] - SNMP listening port. Default value 161 . version [REQUIRED] - SNMP version, allowed values: 1 , 2c or 3 community [OPTIONAL] - SNMP community string, filed is required when version is 1 or 2c secret [OPTIONAL] - reference to the secret from poller.usernameSecrets that should be used to poll from the device security_engine [OPTIONAL] - security engine ID required by SNMPv3. If not provided for version 3 it is autogenerated. walk_interval [OPTIONAL] - interval in seconds for SNMP walk, default value 42000 . This value needs to be between 1800 and 42000 profiles [OPTIONAL] - list of SNMP profiles used for the device. More than one profile can be added by semicolon separation eg. profile1;profile2 . More about profiles in Profile Configuration smart_profiles [OPTIONAL] - enabled smart profiles, by default it\u2019s true . Allowed value: true , false . delete [OPTIONAL] - flags which define if inventory should be deleted from scheduled tasks for WALKs and GETs. Allowed value: true , false . Default value is false . Example: poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10.202.4.202,,2c,public,,,2000,my_profile1,, example_group_1,,2c,public,,,2000,my_profile2;my_profile3,, Update Inventory \u00b6 Adding new devices for values.yaml is quite expensive from the Splunk Connect for SNMP perspective. As it interacts with real, networking devices, it requires several checks before applying changes. SC4SNMP was designed to prevent changes in inventory task more often than every 5 min. To apply inventory changes in values.yaml , the following steps need to be executed: Edit values.yaml Check if inventory pod is still running by the execute command: microk8s kubectl -n sc4snmp get pods | grep inventory If the command does not return any pods, follow the next step. In another case, wait and execute the command again until the moment when inventory job finishes. If you really need to apply changes immediately, you can get around the limitation by deleting the inventory job with: microk8s kubectl delete job/snmp-splunk-connect-for-snmp-inventory -n sc4snmp After running this command, you can proceed with upgrading without a need to wait. Run upgrade command : microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace = sc4snmp --create-namespace NOTE: If you decide to change the frequency of the profile without changing the inventory data, the change will be reflected after next the walk process for the host. The walk happens every walk_interval , or on any change in inventory.","title":"Poller"},{"location":"configuration/poller-configuration/#poller-configuration","text":"Poller is a service which is responsible for querying SNMP devices using the SNMP GET, and the SNMP WALK functionality. Poller executes two main types of tasks: Walk task - executes SNMP walk. SNMP walk is an SNMP application that uses SNMP GETNEXT requests to collect SNMP data from the network and infrastructure SNMP-enabled devices, such as switches and routers. It is a time-consuming task, which may overload the SNMP device when executed too often. It is used by the SC4SNMP to collect and push all OID values, which the provided ACL has access to. Get task - it is a lightweight task whose goal is to query a subset of OIDs defined by the customer. The task serves frequent monitoring OIDs, like memory or CPU utilization. Poller has an inventory , which defines what and how often SC4SNMP has to poll.","title":"Poller Configuration"},{"location":"configuration/poller-configuration/#poller-configuration-file","text":"The poller configuration is kept in a values.yaml file in the poller section. values.yaml is used during the installation process for configuring Kubernetes values. Poller example configuration: poller : usernameSecrets : - sc4snmp-hlab-sha-aes - sc4snmp-hlab-sha-des logLevel : \"WARN\" inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10.202.4.202,,2c,public,,,2000,,, NOTE: header\u2019s line ( address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete ) is necessary for the correct execution of SC4SNMP. Do not remove it.","title":"Poller configuration file"},{"location":"configuration/poller-configuration/#define-log-level","text":"The log level for poller can be set by changing the value for the key logLevel . The allowed values are: DEBUG , INFO , WARNING , ERROR . The default value is WARNING .","title":"Define log level"},{"location":"configuration/poller-configuration/#define-usernamesecrets","text":"Secrets are required to run SNMPv3 polling. To add v3 authentication details, create the k8s Secret object: SNMPv3 Configuration , and put its name in poller.usernameSecrets .","title":"Define usernameSecrets"},{"location":"configuration/poller-configuration/#append-oid-index-part-to-the-metrics","text":"Not every SNMP metric object is structured the way it has its index as a one of the field value. We can append the index part of OID with: poller : metricsIndexingEnabled : true So the following change will make this metric object (derived from the OID 1.3.6.1.2.1.6.20.1.4.0.0.443 ) { frequency: 5 metric_name:sc4snmp.TCP-MIB.tcpListenerProcess: 309 mibIndex: 0.0.443 profiles: generic_switch } out of this object: { frequency: 5 metric_name:sc4snmp.TCP-MIB.tcpListenerProcess: 309 profiles: generic_switch } Not every SNMP metric object is structured the way it has its index as a one of the field value. We can append the index part of OID with: poller : metricsIndexingEnabled : true","title":"Append OID index part to the metrics"},{"location":"configuration/poller-configuration/#disable-automatic-polling-of-base-profiles","text":"There are two profiles that are being polled by default - so that even without any configuration you can see the data in Splunk. You can disable it with pollBaseProfiles parameter. poller : pollBaseProfiles : false","title":"Disable automatic polling of base profiles"},{"location":"configuration/poller-configuration/#configure-inventory","text":"To update inventory, see: Update Inventory and Profile . inventory section in poller has the following fields to configure: address [REQUIRED] - IP address which SC4SNMP should connect to collect data from or name of the group of hosts. General information about groups can be found on Configuring Groups page. port [OPTIONAL] - SNMP listening port. Default value 161 . version [REQUIRED] - SNMP version, allowed values: 1 , 2c or 3 community [OPTIONAL] - SNMP community string, filed is required when version is 1 or 2c secret [OPTIONAL] - reference to the secret from poller.usernameSecrets that should be used to poll from the device security_engine [OPTIONAL] - security engine ID required by SNMPv3. If not provided for version 3 it is autogenerated. walk_interval [OPTIONAL] - interval in seconds for SNMP walk, default value 42000 . This value needs to be between 1800 and 42000 profiles [OPTIONAL] - list of SNMP profiles used for the device. More than one profile can be added by semicolon separation eg. profile1;profile2 . More about profiles in Profile Configuration smart_profiles [OPTIONAL] - enabled smart profiles, by default it\u2019s true . Allowed value: true , false . delete [OPTIONAL] - flags which define if inventory should be deleted from scheduled tasks for WALKs and GETs. Allowed value: true , false . Default value is false . Example: poller : inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10.202.4.202,,2c,public,,,2000,my_profile1,, example_group_1,,2c,public,,,2000,my_profile2;my_profile3,,","title":"Configure inventory"},{"location":"configuration/poller-configuration/#update-inventory","text":"Adding new devices for values.yaml is quite expensive from the Splunk Connect for SNMP perspective. As it interacts with real, networking devices, it requires several checks before applying changes. SC4SNMP was designed to prevent changes in inventory task more often than every 5 min. To apply inventory changes in values.yaml , the following steps need to be executed: Edit values.yaml Check if inventory pod is still running by the execute command: microk8s kubectl -n sc4snmp get pods | grep inventory If the command does not return any pods, follow the next step. In another case, wait and execute the command again until the moment when inventory job finishes. If you really need to apply changes immediately, you can get around the limitation by deleting the inventory job with: microk8s kubectl delete job/snmp-splunk-connect-for-snmp-inventory -n sc4snmp After running this command, you can proceed with upgrading without a need to wait. Run upgrade command : microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace = sc4snmp --create-namespace NOTE: If you decide to change the frequency of the profile without changing the inventory data, the change will be reflected after next the walk process for the host. The walk happens every walk_interval , or on any change in inventory.","title":"Update Inventory"},{"location":"configuration/redis-configuration/","text":"Redis configuration \u00b6 Recently, RabbitMQ was replaced with Redis as a queue service and periodic task database. The reason for this is to increase SC4SNMP performance and protect against bottlenecks. Redis both manages periodic tasks and queues the SC4SNMP service. It queues tasks like SNMP Walk and Poll. Redis configuration file \u00b6 Redis configuration is kept in the values.yaml file in the redis section. values.yaml is being used during the installation process for configuring Kubernetes values. To edit the configuration, see: Redis on Kubernetes","title":"Redis"},{"location":"configuration/redis-configuration/#redis-configuration","text":"Recently, RabbitMQ was replaced with Redis as a queue service and periodic task database. The reason for this is to increase SC4SNMP performance and protect against bottlenecks. Redis both manages periodic tasks and queues the SC4SNMP service. It queues tasks like SNMP Walk and Poll.","title":"Redis configuration"},{"location":"configuration/redis-configuration/#redis-configuration-file","text":"Redis configuration is kept in the values.yaml file in the redis section. values.yaml is being used during the installation process for configuring Kubernetes values. To edit the configuration, see: Redis on Kubernetes","title":"Redis configuration file"},{"location":"configuration/scheduler-configuration/","text":"Scheduler configuration \u00b6 The scheduler is a service with is responsible for managing schedules for SNMP walks and GETs. Schedules definition are stored in Mongo DB. Scheduler configuration file \u00b6 Scheduler configuration is kept in values.yaml file in section scheduler . values.yaml is being used during the installation process for configuring Kubernetes values. Example: scheduler : logLevel : \"WARN\" profiles : | test_profile: frequency: 5 condition: type: \"field\" field: \"SNMPv2-MIB.sysDescr\" patterns: - \"^.*\" varBinds: # Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]] - [\"SNMPv2-MIB\", \"sysDescr\",0] Define log level \u00b6 Log level for scheduler can be set by changing the value for key logLevel . Allowed values are: DEBUG , INFO , WARNING , ERROR . The default value is WARNING Define resource requests and limits \u00b6 scheduler : #The following resource specification is appropriate for most deployments to scale the #Larger inventories may require more memory but should not require additional cpu resources : limits : cpu : 1 memory : 1Gi requests : cpu : 200m memory : 128Mi Define groups of hosts \u00b6 To get the general idea when groups are useful see Configuring Groups . Example group configuration: scheduler : groups : | example_group_1: - address: 123.0.0.1 port: 161 - address: 178.8.8.1 port: 999 - address: 12.22.23 port: 161 community: 'private' example_group_2: - address: 103.0.0.1 port: 1161 version: '3' secret: 'my_secret' - address: 178.80.8.1 port: 999 The one obligatory field for the host configuration is address . If port isn\u2019t configured its default value is 161 . Other fields that can be modified here are: community , secret , version , security_engine . However, if they remain unspecified in the host configuration, they will be derived from the inventory record regarding this specific group.","title":"Scheduler"},{"location":"configuration/scheduler-configuration/#scheduler-configuration","text":"The scheduler is a service with is responsible for managing schedules for SNMP walks and GETs. Schedules definition are stored in Mongo DB.","title":"Scheduler configuration"},{"location":"configuration/scheduler-configuration/#scheduler-configuration-file","text":"Scheduler configuration is kept in values.yaml file in section scheduler . values.yaml is being used during the installation process for configuring Kubernetes values. Example: scheduler : logLevel : \"WARN\" profiles : | test_profile: frequency: 5 condition: type: \"field\" field: \"SNMPv2-MIB.sysDescr\" patterns: - \"^.*\" varBinds: # Syntax: [ \"MIB-Component\", \"MIB object name\"[Optional], \"MIB index number\"[Optional]] - [\"SNMPv2-MIB\", \"sysDescr\",0]","title":"Scheduler configuration file"},{"location":"configuration/scheduler-configuration/#define-log-level","text":"Log level for scheduler can be set by changing the value for key logLevel . Allowed values are: DEBUG , INFO , WARNING , ERROR . The default value is WARNING","title":"Define log level"},{"location":"configuration/scheduler-configuration/#define-resource-requests-and-limits","text":"scheduler : #The following resource specification is appropriate for most deployments to scale the #Larger inventories may require more memory but should not require additional cpu resources : limits : cpu : 1 memory : 1Gi requests : cpu : 200m memory : 128Mi","title":"Define resource requests and limits"},{"location":"configuration/scheduler-configuration/#define-groups-of-hosts","text":"To get the general idea when groups are useful see Configuring Groups . Example group configuration: scheduler : groups : | example_group_1: - address: 123.0.0.1 port: 161 - address: 178.8.8.1 port: 999 - address: 12.22.23 port: 161 community: 'private' example_group_2: - address: 103.0.0.1 port: 1161 version: '3' secret: 'my_secret' - address: 178.80.8.1 port: 999 The one obligatory field for the host configuration is address . If port isn\u2019t configured its default value is 161 . Other fields that can be modified here are: community , secret , version , security_engine . However, if they remain unspecified in the host configuration, they will be derived from the inventory record regarding this specific group.","title":"Define groups of hosts"},{"location":"configuration/sim-configuration/","text":"OTEL and Splunk Observability Cloud configuration \u00b6 Splunk OpenTelemetry Collector is a component that provides an option to send metrics to Splunk Observability Cloud. In order to use it, you must set enabled flag in values.yaml to true : sim : # sim must be enabled if you want to use SignalFx enabled : true Token and realm \u00b6 You need to specify Splunk Observability Cloud token and realm. There are two ways of configuring them: Pass those in a plain text via values.yaml so at the end sim element looks like this: sim : enabled : true signalfxToken : BCwaJ_Ands4Xh7Nrg signalfxRealm : us0 Alternatively, create microk8s secret by yourself and pass its name in values.yaml file. Create secret: microk8s kubectl create - n < namespace > secret generic < secretname > \\ -- from - literal = signalfxToken =< signalfxToken > \\ -- from - literal = signalfxRealm =< signalfxRealm > Modify sim.secret section of values.yaml . Disable creation of the secret with sim.secret.create and provide the <secretname> matching the one from the previous step. Pass it via sim.secret.name . For example, for <secretname> = signalfx the sim section would look like: sim : secret : create : false name : signalfx Note: After the initial installation, if you change sim.signalfxToken and/or sim.signalfxRealm and no sim.secret.name is given, the sim pod will sense the update by itself (after helm3 upgrade command) and trigger the recreation. But, when you edit secret created outside of values.yaml (given by sim.secret.name ), you need to roll out the deployment by yourself or delete the pod to update the data. Define annotations \u00b6 In case you need to append some annotations to the sim service, you can do it by setting sim.service.annotations , for ex.: sim : service : annotations : annotation_key : annotation_value Verify the deployment \u00b6 After executing microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace , the sim pod should be up and running: splunker@ip-10-202-13-233:~$ microk8s kubectl get pods -n sc4snmp NAME READY STATUS RESTARTS AGE snmp-splunk-connect-for-snmp-scheduler-7ddbc8d75-bljsj 1/1 Running 0 133m snmp-splunk-connect-for-snmp-worker-poller-57cd8f4665-9z9vx 1/1 Running 0 133m snmp-splunk-connect-for-snmp-worker-sender-5c44cbb9c5-ppmb5 1/1 Running 0 133m snmp-splunk-connect-for-snmp-worker-trap-549766d4-28qzh 1/1 Running 0 133m snmp-mibserver-7f879c5b7c-hz9tz 1/1 Running 0 133m snmp-mongodb-869cc8586f-vvr9f 2/2 Running 0 133m snmp-redis-master-0 1/1 Running 0 133m snmp-splunk-connect-for-snmp-trap-78759bfc8b-79m6d 1/1 Running 0 99m snmp-splunk-connect-for-snmp-sim-59b89747f-kn6tf 1/1 Running 0 32s","title":"Splunk Infrastructure Monitoring"},{"location":"configuration/sim-configuration/#otel-and-splunk-observability-cloud-configuration","text":"Splunk OpenTelemetry Collector is a component that provides an option to send metrics to Splunk Observability Cloud. In order to use it, you must set enabled flag in values.yaml to true : sim : # sim must be enabled if you want to use SignalFx enabled : true","title":"OTEL and Splunk Observability Cloud configuration"},{"location":"configuration/sim-configuration/#token-and-realm","text":"You need to specify Splunk Observability Cloud token and realm. There are two ways of configuring them: Pass those in a plain text via values.yaml so at the end sim element looks like this: sim : enabled : true signalfxToken : BCwaJ_Ands4Xh7Nrg signalfxRealm : us0 Alternatively, create microk8s secret by yourself and pass its name in values.yaml file. Create secret: microk8s kubectl create - n < namespace > secret generic < secretname > \\ -- from - literal = signalfxToken =< signalfxToken > \\ -- from - literal = signalfxRealm =< signalfxRealm > Modify sim.secret section of values.yaml . Disable creation of the secret with sim.secret.create and provide the <secretname> matching the one from the previous step. Pass it via sim.secret.name . For example, for <secretname> = signalfx the sim section would look like: sim : secret : create : false name : signalfx Note: After the initial installation, if you change sim.signalfxToken and/or sim.signalfxRealm and no sim.secret.name is given, the sim pod will sense the update by itself (after helm3 upgrade command) and trigger the recreation. But, when you edit secret created outside of values.yaml (given by sim.secret.name ), you need to roll out the deployment by yourself or delete the pod to update the data.","title":"Token and realm"},{"location":"configuration/sim-configuration/#define-annotations","text":"In case you need to append some annotations to the sim service, you can do it by setting sim.service.annotations , for ex.: sim : service : annotations : annotation_key : annotation_value","title":"Define annotations"},{"location":"configuration/sim-configuration/#verify-the-deployment","text":"After executing microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace , the sim pod should be up and running: splunker@ip-10-202-13-233:~$ microk8s kubectl get pods -n sc4snmp NAME READY STATUS RESTARTS AGE snmp-splunk-connect-for-snmp-scheduler-7ddbc8d75-bljsj 1/1 Running 0 133m snmp-splunk-connect-for-snmp-worker-poller-57cd8f4665-9z9vx 1/1 Running 0 133m snmp-splunk-connect-for-snmp-worker-sender-5c44cbb9c5-ppmb5 1/1 Running 0 133m snmp-splunk-connect-for-snmp-worker-trap-549766d4-28qzh 1/1 Running 0 133m snmp-mibserver-7f879c5b7c-hz9tz 1/1 Running 0 133m snmp-mongodb-869cc8586f-vvr9f 2/2 Running 0 133m snmp-redis-master-0 1/1 Running 0 133m snmp-splunk-connect-for-snmp-trap-78759bfc8b-79m6d 1/1 Running 0 99m snmp-splunk-connect-for-snmp-sim-59b89747f-kn6tf 1/1 Running 0 32s","title":"Verify the deployment"},{"location":"configuration/snmp-data-format/","text":"SNMP Data Format \u00b6 SC4SNMP classifies SNMP data elements as metrics or textual fields. We assume that the metric types are the indicators worth monitoring, that changes dynamically, and textual fields are the context helpful to understand what an SNMP object really means. SC4SNMP classify the data element as a metric when its type is one of: Unsigned Counter TimeTicks Gauge Integer Every other type is interpreted as a field value. Sometimes, the MIB file indicates a field as an INTEGER , but there is also some mapping defined, like for example in case of IF-MIB.ifOperStatus : ifOperStatus OBJECT - TYPE SYNTAX INTEGER { up ( 1 ) , -- ready to pass packets down ( 2 ) , testing ( 3 ) , -- in some test mode unknown ( 4 ) , -- status can not be determined -- for some reason . dormant ( 5 ) , notPresent ( 6 ) , -- some component is missing lowerLayerDown ( 7 ) -- down due to state of -- lower - layer interface ( s ) } source Here we expect some numeric value, but actually what SNMP Agents gets from the device is a string value, like up . To avoid setting textual value as a metrics, SC4SNMP does an additional check and tries to cast the numeric value to float. If the check fails, the values is classified as a textual field. Let\u2019s go through a simple example. We\u2019ve just added a device and didn\u2019t configure anything special. The data from a walk in Splunk\u2019s metrics index is: { ifAdminStatus : up ifDescr : GigabitEthernet1 ifIndex : 1 ifOperStatus : up ifPhysAddress : 0 a : aa : ef : 53 : 67 : 15 ifType : ethernetCsmacd metric_name : sc4snmp . IF - MIB . ifInDiscards : 0 metric_name : sc4snmp . IF - MIB . ifInErrors : 0 metric_name : sc4snmp . IF - MIB . ifInOctets : 3873878708 metric_name : sc4snmp . IF - MIB . ifInUcastPkts : 47512921 metric_name : sc4snmp . IF - MIB . ifInUnknownProtos : 0 metric_name : sc4snmp . IF - MIB . ifLastChange : 454107556 metric_name : sc4snmp . IF - MIB . ifMtu : 1500 metric_name : sc4snmp . IF - MIB . ifOutDiscards : 0 metric_name : sc4snmp . IF - MIB . ifOutErrors : 0 metric_name : sc4snmp . IF - MIB . ifOutOctets : 1738565177 metric_name : sc4snmp . IF - MIB . ifOutUcastPkts : 44295751 metric_name : sc4snmp . IF - MIB . ifSpeed : 1000000000 } Clearly we can see the textual part: ifAdminStatus: up ifDescr: GigabitEthernet1 ifIndex: 1 ifOperStatus: up ifPhysAddress: 0a:aa:ef:53:67:15 ifType: ethernetCsmacd And a metric one: metric_name : sc4snmp . IF - MIB . ifInDiscards : 0 metric_name : sc4snmp . IF - MIB . ifInErrors : 0 metric_name : sc4snmp . IF - MIB . ifInOctets : 3873878708 metric_name : sc4snmp . IF - MIB . ifInUcastPkts : 47512921 metric_name : sc4snmp . IF - MIB . ifInUnknownProtos : 0 metric_name : sc4snmp . IF - MIB . ifLastChange : 454107556 metric_name : sc4snmp . IF - MIB . ifMtu : 1500 metric_name : sc4snmp . IF - MIB . ifOutDiscards : 0 metric_name : sc4snmp . IF - MIB . ifOutErrors : 0 metric_name : sc4snmp . IF - MIB . ifOutOctets : 1738565177 metric_name : sc4snmp . IF - MIB . ifOutUcastPkts : 44295751 metric_name : sc4snmp . IF - MIB . ifSpeed : 1000000000 To which Splunk index will my data go? \u00b6 Metric index \u00b6 The rule is, if we poll a profile with AT LEAST one metric value, it will go to the metric index and will be enriched with all the textual fields we have for this object. For example, when polling: profile_with_one_metric : frequency : 100 varBinds : - [ 'IF-MIB' , 'ifOutUcastPkts' ] - [ 'IF-MIB' , 'ifInUcastPkts' ] The record that we\u2019ll see in Splunk | mpreview index=net* for the same case as above would be: ifAdminStatus : up ifDescr : GigabitEthernet1 ifIndex : 1 ifOperStatus : up ifPhysAddress : 0 a : aa : ef : 53 : 67 : 15 ifType : ethernetCsmacd metric_name : sc4snmp . IF - MIB . ifOutUcastPkts : 44295751 metric_name : sc4snmp . IF - MIB . ifInUcastPkts : 47512921 Note, that only fields specified in varBinds are actively polled form the device. In case of profile_with_one_metric shown above, the textual fields ifAdminStatus , ifDescr , ifIndex , ifOperStatus and ifPhysAddress are taken from the database cache, which is updated on every walk process. This is fine for the most of the cases, as things like MAC address, interface type or interface status shouldn\u2019t change frequently if ever. If you want to keep ifOperStatus and ifAdminStatus up to date all the time, define profile like: profile_with_one_metric : frequency : 100 varBinds : - [ 'IF-MIB' , 'ifOutUcastPkts' ] - [ 'IF-MIB' , 'ifInUcastPkts' ] - [ 'IF-MIB' , 'ifOperStatus' ] - [ 'IF-MIB' , 'ifAdminStatus' ] The result in Splunk will look the same, but ifOperStatus and ifAdminStatus will be actively polled. Event index \u00b6 It is possible to create an event without a single metric value, in such scenario it will go to an event index. An example of such profile would be: profile_with_only_textual_fields : frequency : 100 varBinds : - [ 'IF-MIB' , 'ifDescr' ] - [ 'IF-MIB' , 'ifName' ] - [ 'IF-MIB' , 'ifOperStatus' ] In this case no additional enrichment will be done. The events in event index index=netops of Splunk will look like: { [ - ] IF - MIB . ifDescr: { [ - ] name: IF - MIB . ifDescr oid: 1 . 3 . 6 . 1 . 2 . 1 . 2 . 2 . 1 . 2 . 5 time: 1676302789 . 9729967 type: f value: VirtualPortGroup0 } IF - MIB . ifName: { [ - ] name: IF - MIB . ifName oid: 1 . 3 . 6 . 1 . 2 . 1 . 31 . 1 . 1 . 1 . 1 . 5 time: 1676302789 . 6655216 type: f value: Vi0 } IF - MIB . ifOperStatus: { [ - ] name: IF - MIB . ifOperStatus oid: 1 . 3 . 6 . 1 . 2 . 1 . 2 . 2 . 1 . 8 . 5 time: 1676302789 . 6655502 type: g value: up } }","title":"SNMP data format"},{"location":"configuration/snmp-data-format/#snmp-data-format","text":"SC4SNMP classifies SNMP data elements as metrics or textual fields. We assume that the metric types are the indicators worth monitoring, that changes dynamically, and textual fields are the context helpful to understand what an SNMP object really means. SC4SNMP classify the data element as a metric when its type is one of: Unsigned Counter TimeTicks Gauge Integer Every other type is interpreted as a field value. Sometimes, the MIB file indicates a field as an INTEGER , but there is also some mapping defined, like for example in case of IF-MIB.ifOperStatus : ifOperStatus OBJECT - TYPE SYNTAX INTEGER { up ( 1 ) , -- ready to pass packets down ( 2 ) , testing ( 3 ) , -- in some test mode unknown ( 4 ) , -- status can not be determined -- for some reason . dormant ( 5 ) , notPresent ( 6 ) , -- some component is missing lowerLayerDown ( 7 ) -- down due to state of -- lower - layer interface ( s ) } source Here we expect some numeric value, but actually what SNMP Agents gets from the device is a string value, like up . To avoid setting textual value as a metrics, SC4SNMP does an additional check and tries to cast the numeric value to float. If the check fails, the values is classified as a textual field. Let\u2019s go through a simple example. We\u2019ve just added a device and didn\u2019t configure anything special. The data from a walk in Splunk\u2019s metrics index is: { ifAdminStatus : up ifDescr : GigabitEthernet1 ifIndex : 1 ifOperStatus : up ifPhysAddress : 0 a : aa : ef : 53 : 67 : 15 ifType : ethernetCsmacd metric_name : sc4snmp . IF - MIB . ifInDiscards : 0 metric_name : sc4snmp . IF - MIB . ifInErrors : 0 metric_name : sc4snmp . IF - MIB . ifInOctets : 3873878708 metric_name : sc4snmp . IF - MIB . ifInUcastPkts : 47512921 metric_name : sc4snmp . IF - MIB . ifInUnknownProtos : 0 metric_name : sc4snmp . IF - MIB . ifLastChange : 454107556 metric_name : sc4snmp . IF - MIB . ifMtu : 1500 metric_name : sc4snmp . IF - MIB . ifOutDiscards : 0 metric_name : sc4snmp . IF - MIB . ifOutErrors : 0 metric_name : sc4snmp . IF - MIB . ifOutOctets : 1738565177 metric_name : sc4snmp . IF - MIB . ifOutUcastPkts : 44295751 metric_name : sc4snmp . IF - MIB . ifSpeed : 1000000000 } Clearly we can see the textual part: ifAdminStatus: up ifDescr: GigabitEthernet1 ifIndex: 1 ifOperStatus: up ifPhysAddress: 0a:aa:ef:53:67:15 ifType: ethernetCsmacd And a metric one: metric_name : sc4snmp . IF - MIB . ifInDiscards : 0 metric_name : sc4snmp . IF - MIB . ifInErrors : 0 metric_name : sc4snmp . IF - MIB . ifInOctets : 3873878708 metric_name : sc4snmp . IF - MIB . ifInUcastPkts : 47512921 metric_name : sc4snmp . IF - MIB . ifInUnknownProtos : 0 metric_name : sc4snmp . IF - MIB . ifLastChange : 454107556 metric_name : sc4snmp . IF - MIB . ifMtu : 1500 metric_name : sc4snmp . IF - MIB . ifOutDiscards : 0 metric_name : sc4snmp . IF - MIB . ifOutErrors : 0 metric_name : sc4snmp . IF - MIB . ifOutOctets : 1738565177 metric_name : sc4snmp . IF - MIB . ifOutUcastPkts : 44295751 metric_name : sc4snmp . IF - MIB . ifSpeed : 1000000000","title":"SNMP Data Format"},{"location":"configuration/snmp-data-format/#to-which-splunk-index-will-my-data-go","text":"","title":"To which Splunk index will my data go?"},{"location":"configuration/snmp-data-format/#metric-index","text":"The rule is, if we poll a profile with AT LEAST one metric value, it will go to the metric index and will be enriched with all the textual fields we have for this object. For example, when polling: profile_with_one_metric : frequency : 100 varBinds : - [ 'IF-MIB' , 'ifOutUcastPkts' ] - [ 'IF-MIB' , 'ifInUcastPkts' ] The record that we\u2019ll see in Splunk | mpreview index=net* for the same case as above would be: ifAdminStatus : up ifDescr : GigabitEthernet1 ifIndex : 1 ifOperStatus : up ifPhysAddress : 0 a : aa : ef : 53 : 67 : 15 ifType : ethernetCsmacd metric_name : sc4snmp . IF - MIB . ifOutUcastPkts : 44295751 metric_name : sc4snmp . IF - MIB . ifInUcastPkts : 47512921 Note, that only fields specified in varBinds are actively polled form the device. In case of profile_with_one_metric shown above, the textual fields ifAdminStatus , ifDescr , ifIndex , ifOperStatus and ifPhysAddress are taken from the database cache, which is updated on every walk process. This is fine for the most of the cases, as things like MAC address, interface type or interface status shouldn\u2019t change frequently if ever. If you want to keep ifOperStatus and ifAdminStatus up to date all the time, define profile like: profile_with_one_metric : frequency : 100 varBinds : - [ 'IF-MIB' , 'ifOutUcastPkts' ] - [ 'IF-MIB' , 'ifInUcastPkts' ] - [ 'IF-MIB' , 'ifOperStatus' ] - [ 'IF-MIB' , 'ifAdminStatus' ] The result in Splunk will look the same, but ifOperStatus and ifAdminStatus will be actively polled.","title":"Metric index"},{"location":"configuration/snmp-data-format/#event-index","text":"It is possible to create an event without a single metric value, in such scenario it will go to an event index. An example of such profile would be: profile_with_only_textual_fields : frequency : 100 varBinds : - [ 'IF-MIB' , 'ifDescr' ] - [ 'IF-MIB' , 'ifName' ] - [ 'IF-MIB' , 'ifOperStatus' ] In this case no additional enrichment will be done. The events in event index index=netops of Splunk will look like: { [ - ] IF - MIB . ifDescr: { [ - ] name: IF - MIB . ifDescr oid: 1 . 3 . 6 . 1 . 2 . 1 . 2 . 2 . 1 . 2 . 5 time: 1676302789 . 9729967 type: f value: VirtualPortGroup0 } IF - MIB . ifName: { [ - ] name: IF - MIB . ifName oid: 1 . 3 . 6 . 1 . 2 . 1 . 31 . 1 . 1 . 1 . 1 . 5 time: 1676302789 . 6655216 type: f value: Vi0 } IF - MIB . ifOperStatus: { [ - ] name: IF - MIB . ifOperStatus oid: 1 . 3 . 6 . 1 . 2 . 1 . 2 . 2 . 1 . 8 . 5 time: 1676302789 . 6655502 type: g value: up } }","title":"Event index"},{"location":"configuration/snmpv3-configuration/","text":"Create SNMP v3 users \u00b6 Configuration of SNMP v3, when supported by the monitored devices, is the most secure choice available for authentication and data privacy. Each set of credentials will be stored as \u201cSecret\u201d objects in k8s, and will be referenced in values.yaml. This allows the secret to be created once, including automation by third-party password managers, then consumed without storing sensitive data in plain text. # <secretname>=Arbitrary name of the secret often the same as the username or prefixed with \"sc4snmp-\" # <namespace>=Namespace used to install sc4snmp # <username>=the SNMPv3 Username # <key>=key note must be at least 8 char long subject to target limitations # <authProtocol>=One of SHA (SHA1) or MD5 # <privProtocol>=One of AES or DES # Note MD5 and DES are considered insecure but must be supported for standards compliance microk8s kubectl create -n <namespace> secret generic <secretname> \\ --from-literal = userName = <username> \\ --from-literal = authKey = <key> \\ --from-literal = privKey = <key> \\ --from-literal = authProtocol = <authProtocol> \\ --from-literal = privProtocol = <privProtocol> Configured credentials can be used in poller and trap services. In service configuration, secretname needs to be provided.","title":"SNMPv3 configuration"},{"location":"configuration/snmpv3-configuration/#create-snmp-v3-users","text":"Configuration of SNMP v3, when supported by the monitored devices, is the most secure choice available for authentication and data privacy. Each set of credentials will be stored as \u201cSecret\u201d objects in k8s, and will be referenced in values.yaml. This allows the secret to be created once, including automation by third-party password managers, then consumed without storing sensitive data in plain text. # <secretname>=Arbitrary name of the secret often the same as the username or prefixed with \"sc4snmp-\" # <namespace>=Namespace used to install sc4snmp # <username>=the SNMPv3 Username # <key>=key note must be at least 8 char long subject to target limitations # <authProtocol>=One of SHA (SHA1) or MD5 # <privProtocol>=One of AES or DES # Note MD5 and DES are considered insecure but must be supported for standards compliance microk8s kubectl create -n <namespace> secret generic <secretname> \\ --from-literal = userName = <username> \\ --from-literal = authKey = <key> \\ --from-literal = privKey = <key> \\ --from-literal = authProtocol = <authProtocol> \\ --from-literal = privProtocol = <privProtocol> Configured credentials can be used in poller and trap services. In service configuration, secretname needs to be provided.","title":"Create SNMP v3 users"},{"location":"configuration/step-by-step-poll/","text":"An example of a polling scenario \u00b6 We have 4 hosts we want to poll from: 10.202.4.201:161 10.202.4.202:161 10.202.4.203:161 10.202.4.204:163 To retrieve data from the device efficiently, first determine the specific data needed. Instead of walking through the entire 1.3.6.1 , limit the walk to poll only the necessary data. Configure the IF-MIB family for interfaces and the UCD-SNMP-MIB for CPU-related statistics. In the scheduler section of values.yaml , define the target group and establish the polling parameters, known as the profile, to gather the desired data precisely: scheduler : logLevel : \"INFO\" profiles : | small_walk: condition: type: \"walk\" varBinds: - [\"IF-MIB\"] - [\"UCD-SNMP-MIB\"] switch_profile: frequency: 60 varBinds: - [\"IF-MIB\", \"ifDescr\"] - [\"IF-MIB\", \"ifAdminStatus\"] - [\"IF-MIB\", \"ifOperStatus\"] - [\"IF-MIB\", \"ifName\"] - [\"IF-MIB\", \"ifAlias\"] - [\"IF-MIB\", \"ifIndex\"] - [\"IF-MIB\", \"ifInDiscards\"] - [\"IF-MIB\", \"ifInErrors\"] - [\"IF-MIB\", \"ifInOctets\"] - [\"IF-MIB\", \"ifOutDiscards\"] - [\"IF-MIB\", \"ifOutErrors\"] - [\"IF-MIB\", \"ifOutOctets\"] - [\"IF-MIB\", \"ifOutQLen\"] - [\"UCD-SNMP-MIB\"] groups : | switch_group: - address: 10.202.4.201 - address: 10.202.4.202 - address: 10.202.4.203 - address: 10.202.4.204 port: 163 Then it is required to pass the proper instruction of what to do for SC4SNMP instance. This can be done by appending a new row to poller.inventory : poller : logLevel : \"WARN\" inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete switch_group,,2c,public,,,2000,small_walk;switch_profile,, The provided configuration will make: Walk devices from switch_group with IF-MIB and UCD-SNMP-MIB every 2000 seconds Poll specific IF-MIB fields and the whole UCD-SNMP-MIB every 60 seconds Note: you could as well limit walk profile even more if you want to enhance the performance. It makes sense to put in the walk the textual values that don\u2019t required to be constantly monitored, and monitor only the metrics you\u2019re interested in: small_walk : condition : type : \"walk\" varBinds : - [ \"IF-MIB\" , \"ifDescr\" ] - [ \"IF-MIB\" , \"ifAdminStatus\" ] - [ \"IF-MIB\" , \"ifOperStatus\" ] - [ \"IF-MIB\" , \"ifName\" ] - [ \"IF-MIB\" , \"ifAlias\" ] - [ \"IF-MIB\" , \"ifIndex\" ] switch_profile : frequency : 60 varBinds : - [ \"IF-MIB\" , \"ifInDiscards\" ] - [ \"IF-MIB\" , \"ifInErrors\" ] - [ \"IF-MIB\" , \"ifInOctets\" ] - [ \"IF-MIB\" , \"ifOutDiscards\" ] - [ \"IF-MIB\" , \"ifOutErrors\" ] - [ \"IF-MIB\" , \"ifOutOctets\" ] - [ \"IF-MIB\" , \"ifOutQLen\" ] Then every metric object will be enriched with the textual values gathered from a walk process. Learn more about SNMP format here . Now we\u2019re ready to reload SC4SNMP. We run the helm3 upgrade command: microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace We should see the new pod with Running -> Completed state: microk8s kubectl get pods -n sc4snmp -w Example output: NAME READY STATUS RESTARTS AGE snmp-splunk-connect-for-snmp-worker-sender-5bc5cf864b-cwmfw 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-worker-poller-76dcfb5896-d55pd 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-worker-trap-68fb6476db-zl9rb 1/1 Running 0 5h52m snmp-mibserver-58b558f5b4-zqf85 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-scheduler-57c5878444-k4qv4 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-worker-poller-76dcfb5896-bzgrm 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-trap-6cb76fcb49-l62f9 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-trap-6cb76fcb49-d7c88 1/1 Running 0 5h52m snmp-mongodb-869cc8586f-kw67q 2/2 Running 0 5h52m snmp-redis-master-0 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-inventory-g4bs7 1/1 Running 0 3s snmp-splunk-connect-for-snmp-inventory-g4bs7 0/1 Completed 0 5s snmp-splunk-connect-for-snmp-inventory-g4bs7 0/1 Completed 0 6s snmp-splunk-connect-for-snmp-inventory-g4bs7 0/1 Completed 0 7s We can check the pod\u2019s logs to make sure everything was reloaded right, with: microk8s kubectl logs -f snmp-splunk-connect-for-snmp-inventory-g4bs7 -n sc4snmp Example output: Successfully connected to redis://snmp-redis-headless:6379/0 Successfully connected to redis://snmp-redis-headless:6379/1 Successfully connected to mongodb://snmp-mongodb:27017 Successfully connected to http://snmp-mibserver/index.csv {\"message\" : \"Loading inventory from /app/inventory/inventory.csv\" , \"time\" : \"2022-09-05T14:30:30.605420\" , \"level\" : \"INFO\" } { \"message\" : \"New Record address='10.202.4.201' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\" , \"time\" : \"2022-09-05T14:30:30.607641\" , \"level\" : \"INFO\" } { \"message\" : \"New Record address='10.202.4.202' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\" , \"time\" : \"2022-09-05T14:30:30.607641\" , \"level\" : \"INFO\" } { \"message\" : \"New Record address='10.202.4.203' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\" , \"time\" : \"2022-09-05T14:30:30.607641\" , \"level\" : \"INFO\" } { \"message\" : \"New Record address='10.202.4.204' port=163 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\" , \"time\" : \"2022-09-05T14:30:30.607641\" , \"level\" : \"INFO\" } In some time (depending on how long the walk takes), we\u2019ll see events under: | mpreview index=netmetrics | search profiles=switch_profile query in Splunk. When groups are used, we can also use querying by the group name: | mpreview index=netmetrics | search group=switch_group Keep in mind, that querying by profiles/group in Splunk is only possible in the metrics index. Every piece of data being sent by SC4SNMP is formed based on the MIB file\u2019s definition of the SNMP object\u2019s index. The object is forwarded to an event index only if it doesn\u2019t have any metric value inside. The raw metrics in Splunk example is: { \"frequency\" : \"60\" , \"group\" : \"switch_group\" , \"ifAdminStatus\" : \"up\" , \"ifAlias\" : \"1\" , \"ifDescr\" : \"lo\" , \"ifIndex\" : \"1\" , \"ifName\" : \"lo\" , \"ifOperStatus\" : \"up\" , \"ifPhysAddress\" : \"1\" , \"ifType\" : \"softwareLoopback\" , \"profiles\" : \"switch_profile\" , \"metric_name:sc4snmp.IF-MIB.ifInDiscards\" : 21877 , \"metric_name:sc4snmp.IF-MIB.ifInErrors\" : 21840 , \"metric_name:sc4snmp.IF-MIB.ifInNUcastPkts\" : 14152789 , \"metric_name:sc4snmp.IF-MIB.ifInOctets\" : 1977814270 , \"metric_name:sc4snmp.IF-MIB.ifInUcastPkts\" : 220098191 , \"metric_name:sc4snmp.IF-MIB.ifInUnknownProtos\" : 1488029 , \"metric_name:sc4snmp.IF-MIB.ifLastChange\" : 124000001 , \"metric_name:sc4snmp.IF-MIB.ifMtu\" : 16436 , \"metric_name:sc4snmp.IF-MIB.ifOutDiscards\" : 21862 , \"metric_name:sc4snmp.IF-MIB.ifOutErrors\" : 21836 , \"metric_name:sc4snmp.IF-MIB.ifOutNUcastPkts\" : 14774727 , \"metric_name:sc4snmp.IF-MIB.ifOutOctets\" : 1346799625 , \"metric_name:sc4snmp.IF-MIB.ifOutQLen\" : 4294967295 , \"metric_name:sc4snmp.IF-MIB.ifOutUcastPkts\" : 74003841 , \"metric_name:sc4snmp.IF-MIB.ifSpeed\" : 10000000 } or { \"frequency\" : \"60\" , \"group\" : \"switch_group\" , \"laNames\" : \"Load-1\" , \"profiles\" : \"switch_profile\" , \"metric_name:sc4snmp.UCD-SNMP-MIB.laIndex\" : 1 }","title":"Step by Step polling example"},{"location":"configuration/step-by-step-poll/#an-example-of-a-polling-scenario","text":"We have 4 hosts we want to poll from: 10.202.4.201:161 10.202.4.202:161 10.202.4.203:161 10.202.4.204:163 To retrieve data from the device efficiently, first determine the specific data needed. Instead of walking through the entire 1.3.6.1 , limit the walk to poll only the necessary data. Configure the IF-MIB family for interfaces and the UCD-SNMP-MIB for CPU-related statistics. In the scheduler section of values.yaml , define the target group and establish the polling parameters, known as the profile, to gather the desired data precisely: scheduler : logLevel : \"INFO\" profiles : | small_walk: condition: type: \"walk\" varBinds: - [\"IF-MIB\"] - [\"UCD-SNMP-MIB\"] switch_profile: frequency: 60 varBinds: - [\"IF-MIB\", \"ifDescr\"] - [\"IF-MIB\", \"ifAdminStatus\"] - [\"IF-MIB\", \"ifOperStatus\"] - [\"IF-MIB\", \"ifName\"] - [\"IF-MIB\", \"ifAlias\"] - [\"IF-MIB\", \"ifIndex\"] - [\"IF-MIB\", \"ifInDiscards\"] - [\"IF-MIB\", \"ifInErrors\"] - [\"IF-MIB\", \"ifInOctets\"] - [\"IF-MIB\", \"ifOutDiscards\"] - [\"IF-MIB\", \"ifOutErrors\"] - [\"IF-MIB\", \"ifOutOctets\"] - [\"IF-MIB\", \"ifOutQLen\"] - [\"UCD-SNMP-MIB\"] groups : | switch_group: - address: 10.202.4.201 - address: 10.202.4.202 - address: 10.202.4.203 - address: 10.202.4.204 port: 163 Then it is required to pass the proper instruction of what to do for SC4SNMP instance. This can be done by appending a new row to poller.inventory : poller : logLevel : \"WARN\" inventory : | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete switch_group,,2c,public,,,2000,small_walk;switch_profile,, The provided configuration will make: Walk devices from switch_group with IF-MIB and UCD-SNMP-MIB every 2000 seconds Poll specific IF-MIB fields and the whole UCD-SNMP-MIB every 60 seconds Note: you could as well limit walk profile even more if you want to enhance the performance. It makes sense to put in the walk the textual values that don\u2019t required to be constantly monitored, and monitor only the metrics you\u2019re interested in: small_walk : condition : type : \"walk\" varBinds : - [ \"IF-MIB\" , \"ifDescr\" ] - [ \"IF-MIB\" , \"ifAdminStatus\" ] - [ \"IF-MIB\" , \"ifOperStatus\" ] - [ \"IF-MIB\" , \"ifName\" ] - [ \"IF-MIB\" , \"ifAlias\" ] - [ \"IF-MIB\" , \"ifIndex\" ] switch_profile : frequency : 60 varBinds : - [ \"IF-MIB\" , \"ifInDiscards\" ] - [ \"IF-MIB\" , \"ifInErrors\" ] - [ \"IF-MIB\" , \"ifInOctets\" ] - [ \"IF-MIB\" , \"ifOutDiscards\" ] - [ \"IF-MIB\" , \"ifOutErrors\" ] - [ \"IF-MIB\" , \"ifOutOctets\" ] - [ \"IF-MIB\" , \"ifOutQLen\" ] Then every metric object will be enriched with the textual values gathered from a walk process. Learn more about SNMP format here . Now we\u2019re ready to reload SC4SNMP. We run the helm3 upgrade command: microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace=sc4snmp --create-namespace We should see the new pod with Running -> Completed state: microk8s kubectl get pods -n sc4snmp -w Example output: NAME READY STATUS RESTARTS AGE snmp-splunk-connect-for-snmp-worker-sender-5bc5cf864b-cwmfw 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-worker-poller-76dcfb5896-d55pd 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-worker-trap-68fb6476db-zl9rb 1/1 Running 0 5h52m snmp-mibserver-58b558f5b4-zqf85 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-scheduler-57c5878444-k4qv4 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-worker-poller-76dcfb5896-bzgrm 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-trap-6cb76fcb49-l62f9 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-trap-6cb76fcb49-d7c88 1/1 Running 0 5h52m snmp-mongodb-869cc8586f-kw67q 2/2 Running 0 5h52m snmp-redis-master-0 1/1 Running 0 5h52m snmp-splunk-connect-for-snmp-inventory-g4bs7 1/1 Running 0 3s snmp-splunk-connect-for-snmp-inventory-g4bs7 0/1 Completed 0 5s snmp-splunk-connect-for-snmp-inventory-g4bs7 0/1 Completed 0 6s snmp-splunk-connect-for-snmp-inventory-g4bs7 0/1 Completed 0 7s We can check the pod\u2019s logs to make sure everything was reloaded right, with: microk8s kubectl logs -f snmp-splunk-connect-for-snmp-inventory-g4bs7 -n sc4snmp Example output: Successfully connected to redis://snmp-redis-headless:6379/0 Successfully connected to redis://snmp-redis-headless:6379/1 Successfully connected to mongodb://snmp-mongodb:27017 Successfully connected to http://snmp-mibserver/index.csv {\"message\" : \"Loading inventory from /app/inventory/inventory.csv\" , \"time\" : \"2022-09-05T14:30:30.605420\" , \"level\" : \"INFO\" } { \"message\" : \"New Record address='10.202.4.201' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\" , \"time\" : \"2022-09-05T14:30:30.607641\" , \"level\" : \"INFO\" } { \"message\" : \"New Record address='10.202.4.202' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\" , \"time\" : \"2022-09-05T14:30:30.607641\" , \"level\" : \"INFO\" } { \"message\" : \"New Record address='10.202.4.203' port=161 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\" , \"time\" : \"2022-09-05T14:30:30.607641\" , \"level\" : \"INFO\" } { \"message\" : \"New Record address='10.202.4.204' port=163 version='2c' community='public' secret=None security_engine=None walk_interval=2000 profiles=['switch_profile'] smart_profiles=True delete=False\" , \"time\" : \"2022-09-05T14:30:30.607641\" , \"level\" : \"INFO\" } In some time (depending on how long the walk takes), we\u2019ll see events under: | mpreview index=netmetrics | search profiles=switch_profile query in Splunk. When groups are used, we can also use querying by the group name: | mpreview index=netmetrics | search group=switch_group Keep in mind, that querying by profiles/group in Splunk is only possible in the metrics index. Every piece of data being sent by SC4SNMP is formed based on the MIB file\u2019s definition of the SNMP object\u2019s index. The object is forwarded to an event index only if it doesn\u2019t have any metric value inside. The raw metrics in Splunk example is: { \"frequency\" : \"60\" , \"group\" : \"switch_group\" , \"ifAdminStatus\" : \"up\" , \"ifAlias\" : \"1\" , \"ifDescr\" : \"lo\" , \"ifIndex\" : \"1\" , \"ifName\" : \"lo\" , \"ifOperStatus\" : \"up\" , \"ifPhysAddress\" : \"1\" , \"ifType\" : \"softwareLoopback\" , \"profiles\" : \"switch_profile\" , \"metric_name:sc4snmp.IF-MIB.ifInDiscards\" : 21877 , \"metric_name:sc4snmp.IF-MIB.ifInErrors\" : 21840 , \"metric_name:sc4snmp.IF-MIB.ifInNUcastPkts\" : 14152789 , \"metric_name:sc4snmp.IF-MIB.ifInOctets\" : 1977814270 , \"metric_name:sc4snmp.IF-MIB.ifInUcastPkts\" : 220098191 , \"metric_name:sc4snmp.IF-MIB.ifInUnknownProtos\" : 1488029 , \"metric_name:sc4snmp.IF-MIB.ifLastChange\" : 124000001 , \"metric_name:sc4snmp.IF-MIB.ifMtu\" : 16436 , \"metric_name:sc4snmp.IF-MIB.ifOutDiscards\" : 21862 , \"metric_name:sc4snmp.IF-MIB.ifOutErrors\" : 21836 , \"metric_name:sc4snmp.IF-MIB.ifOutNUcastPkts\" : 14774727 , \"metric_name:sc4snmp.IF-MIB.ifOutOctets\" : 1346799625 , \"metric_name:sc4snmp.IF-MIB.ifOutQLen\" : 4294967295 , \"metric_name:sc4snmp.IF-MIB.ifOutUcastPkts\" : 74003841 , \"metric_name:sc4snmp.IF-MIB.ifSpeed\" : 10000000 } or { \"frequency\" : \"60\" , \"group\" : \"switch_group\" , \"laNames\" : \"Load-1\" , \"profiles\" : \"switch_profile\" , \"metric_name:sc4snmp.UCD-SNMP-MIB.laIndex\" : 1 }","title":"An example of a polling scenario"},{"location":"configuration/trap-configuration/","text":"Trap Configuration \u00b6 A trap service is a simple server that can handle SNMP traps sent by SNMP devices like routers or switches. Trap configuration file \u00b6 The trap configuration is kept in the values.yaml file in section traps. values.yaml is used during the installation process for configuring Kubernetes values. Trap example configuration: traps : communities : 1 : - public 2c : - public - homelab usernameSecrets : - secretv3 - sc4snmp-homesecure-sha-des # Overrides the image tag whose default is the chart appVersion. logLevel : \"WARN\" # replicas: Number of replicas for trap container should be 2x number of nodes replicas : 2 #loadBalancerIP: The IP address in the metallb pool loadBalancerIP : 10.202.4.202 resources : limits : cpu : 500m memory : 512Mi requests : cpu : 200m memory : 256Mi Define communities \u00b6 communities define a version of SNMP protocol and SNMP community string, which should be used. communities key is split by protocol version, supported values are 1 and 2c . Under the version section, SNMP community string can be defined. Example: traps : communities : 1 : - public 2c : - public - homelab Configure user secrets for SNMPv3 \u00b6 The usernameSecrets key in the traps section define SNMPv3 secrets for trap messages sent by SNMP device. usernameSecrets define which secrets in \u201cSecret\u201d objects in k8s should be used, as a value it needs the name of \u201cSecret\u201d objects. More information on how to define the \u201cSecret\u201d object for SNMPv3 can be found in SNMPv3 Configuration . Example: traps : usernameSecrets : - sc4snmp-homesecure-sha-aes - sc4snmp-homesecure-sha-des Define security engines ID for SNMPv3 \u00b6 SNMPv3 TRAPs require the configuration SNMP Engine ID of the TRAP sending application for the USM users table of the TRAP receiving application for each USM user. The SNMP Engine ID is usually unique for the device, and the SC4SNMP as a trap receiver has to be aware of which security engine IDs to accept. Define all of them under traps.securityEngineId in values.yaml . By default, it is set to one-element list: [80003a8c04] . Example: traps : securityEngineId : - \"80003a8c04\" Security engine ID is a substitute of the -e variable in snmptrap . An example of SNMPv3 trap is: snmptrap -v3 -e 80003a8c04 -l authPriv -u snmp-poller -a SHA -A PASSWORD1 -x AES -X PASSWORD1 10.202.13.233 '' 1.3.6.1.2.1.2.2.1.1.1 Define external gateway for traps \u00b6 If you use SC4SNMP standalone, configure loadBalancerIP . loadBalancerIP is the IP address in the metallb pool. Example: traps : loadBalancerIP : 10.202.4.202 If you want to use SC4SNMP trap receiver in K8S cluster, configure NodePort instead. The snippet of config is: traps : service : type : NodePort externalTrafficPolicy : Cluster nodePort : 30000 Using this method, SNMP trap will always be forwarded to one of the trap receiver pods listening on port 30000 (as in the example above, remember - you can configure any other port). So doesn\u2019t matter IP address of which node you use, adding nodePort will make it end up in a correct place everytime. Here, good practice is to create IP floating address/Anycast pointing to the healthy nodes, so the traffic is forwarded in case of the failover. The best way is to create external LoadBalancer which balance the traffic between nodes. Define number of traps server replica \u00b6 replicaCount defines that the number of replicas for trap container should be 2x number of nodes. The default value is 2 . Example: traps : #For production deployments the value should be at least 2x the number of nodes # Minimum 2 for a single node # Minimum 6 for multi-node HA replicaCount : 2 Define log level \u00b6 The log level for trap can be set by changing the value for the logLevel key. The allowed values are: DEBUG , INFO , WARNING , ERROR . The default value is WARNING . Define annotations \u00b6 In case you need to append some annotations to the trap service, you can do so by setting traps.service.annotations , for ex.: traps : service : annotations : annotation_key : annotation_value","title":"Traps"},{"location":"configuration/trap-configuration/#trap-configuration","text":"A trap service is a simple server that can handle SNMP traps sent by SNMP devices like routers or switches.","title":"Trap Configuration"},{"location":"configuration/trap-configuration/#trap-configuration-file","text":"The trap configuration is kept in the values.yaml file in section traps. values.yaml is used during the installation process for configuring Kubernetes values. Trap example configuration: traps : communities : 1 : - public 2c : - public - homelab usernameSecrets : - secretv3 - sc4snmp-homesecure-sha-des # Overrides the image tag whose default is the chart appVersion. logLevel : \"WARN\" # replicas: Number of replicas for trap container should be 2x number of nodes replicas : 2 #loadBalancerIP: The IP address in the metallb pool loadBalancerIP : 10.202.4.202 resources : limits : cpu : 500m memory : 512Mi requests : cpu : 200m memory : 256Mi","title":"Trap configuration file"},{"location":"configuration/trap-configuration/#define-communities","text":"communities define a version of SNMP protocol and SNMP community string, which should be used. communities key is split by protocol version, supported values are 1 and 2c . Under the version section, SNMP community string can be defined. Example: traps : communities : 1 : - public 2c : - public - homelab","title":"Define communities"},{"location":"configuration/trap-configuration/#configure-user-secrets-for-snmpv3","text":"The usernameSecrets key in the traps section define SNMPv3 secrets for trap messages sent by SNMP device. usernameSecrets define which secrets in \u201cSecret\u201d objects in k8s should be used, as a value it needs the name of \u201cSecret\u201d objects. More information on how to define the \u201cSecret\u201d object for SNMPv3 can be found in SNMPv3 Configuration . Example: traps : usernameSecrets : - sc4snmp-homesecure-sha-aes - sc4snmp-homesecure-sha-des","title":"Configure user secrets for SNMPv3"},{"location":"configuration/trap-configuration/#define-security-engines-id-for-snmpv3","text":"SNMPv3 TRAPs require the configuration SNMP Engine ID of the TRAP sending application for the USM users table of the TRAP receiving application for each USM user. The SNMP Engine ID is usually unique for the device, and the SC4SNMP as a trap receiver has to be aware of which security engine IDs to accept. Define all of them under traps.securityEngineId in values.yaml . By default, it is set to one-element list: [80003a8c04] . Example: traps : securityEngineId : - \"80003a8c04\" Security engine ID is a substitute of the -e variable in snmptrap . An example of SNMPv3 trap is: snmptrap -v3 -e 80003a8c04 -l authPriv -u snmp-poller -a SHA -A PASSWORD1 -x AES -X PASSWORD1 10.202.13.233 '' 1.3.6.1.2.1.2.2.1.1.1","title":"Define security engines ID for SNMPv3"},{"location":"configuration/trap-configuration/#define-external-gateway-for-traps","text":"If you use SC4SNMP standalone, configure loadBalancerIP . loadBalancerIP is the IP address in the metallb pool. Example: traps : loadBalancerIP : 10.202.4.202 If you want to use SC4SNMP trap receiver in K8S cluster, configure NodePort instead. The snippet of config is: traps : service : type : NodePort externalTrafficPolicy : Cluster nodePort : 30000 Using this method, SNMP trap will always be forwarded to one of the trap receiver pods listening on port 30000 (as in the example above, remember - you can configure any other port). So doesn\u2019t matter IP address of which node you use, adding nodePort will make it end up in a correct place everytime. Here, good practice is to create IP floating address/Anycast pointing to the healthy nodes, so the traffic is forwarded in case of the failover. The best way is to create external LoadBalancer which balance the traffic between nodes.","title":"Define external gateway for traps"},{"location":"configuration/trap-configuration/#define-number-of-traps-server-replica","text":"replicaCount defines that the number of replicas for trap container should be 2x number of nodes. The default value is 2 . Example: traps : #For production deployments the value should be at least 2x the number of nodes # Minimum 2 for a single node # Minimum 6 for multi-node HA replicaCount : 2","title":"Define number of traps server replica"},{"location":"configuration/trap-configuration/#define-log-level","text":"The log level for trap can be set by changing the value for the logLevel key. The allowed values are: DEBUG , INFO , WARNING , ERROR . The default value is WARNING .","title":"Define log level"},{"location":"configuration/trap-configuration/#define-annotations","text":"In case you need to append some annotations to the trap service, you can do so by setting traps.service.annotations , for ex.: traps : service : annotations : annotation_key : annotation_value","title":"Define annotations"},{"location":"configuration/worker-configuration/","text":"Worker Configuration \u00b6 The worker is a kubernetes pod which is responsible for the actual execution of polling, processing trap messages, and sending data to Splunk. Worker types \u00b6 SC4SNMP has two base functionalities: monitoring traps and polling. These operations are handled by 3 types of workers: The trap worker consumes all the trap related tasks produced by the trap pod. The poller worker consumes all the tasks related to polling. The sender worker handles sending data to Splunk. You need to always have at least one sender pod running. Worker configuration file \u00b6 Worker configuration is kept in the values.yaml file in the worker section. worker has 3 subsections: poller , sender , or trap , that refer to the workers\u2019 types. values.yaml is used during the installation process for configuring Kubernetes values. The worker default configuration is: worker : # There are 3 types of workers trap : # replicaCount: number of trap-worker pods which consumes trap tasks replicaCount : 2 #autoscaling: use it instead of replicaCount in order to make pods scalable by itself #autoscaling: # enabled: true # minReplicas: 2 # maxReplicas: 10 # targetCPUUtilizationPercentage: 80 poller : # replicaCount: number of poller-worker pods which consumes polling tasks replicaCount : 2 #autoscaling: use it instead of replicaCount in order to make pods scalable by itself #autoscaling: # enabled: true # minReplicas: 2 # maxReplicas: 10 # targetCPUUtilizationPercentage: 80 sender : # replicaCount: number of sender-worker pods which consumes sending tasks replicaCount : 1 # autoscaling: use it instead of replicaCount in order to make pods scalable by itself #autoscaling: # enabled: true # minReplicas: 2 # maxReplicas: 10 # targetCPUUtilizationPercentage: 80 # udpConnectionTimeout: timeout in seconds for SNMP operations #udpConnectionTimeout: 5 logLevel : \"INFO\" All parameters are described in the Worker parameters section. Worker scaling \u00b6 You can adjust worker pods in two ways: set fixed value in replicaCount , or enable autoscaling , which scales pods automatically. Real life scenario: I use SC4SNMP for only trap monitoring, I want to use my resources effectively. \u00b6 If you don\u2019t use polling at all, set worker.poller.replicaCount to 0 . If you\u2019ll want to use polling in the future, you need to increase replicaCount . To monitor traps, adjust worker.trap.replicaCount depending on your needs and worker.sender.replicaCount to send traps to Splunk. Usually you need much less sender pods than trap ones. This is the example of values.yaml without using autoscaling: worker : trap : replicaCount : 4 sender : replicaCount : 1 poller : replicaCount : 0 logLevel : \"WARNING\" This is the example of values.yaml with autoscaling: worker : trap : autoscaling : enabled : true minReplicas : 4 maxReplicas : 10 targetCPUUtilizationPercentage : 80 sender : autoscaling : enabled : true minReplicas : 2 maxReplicas : 5 targetCPUUtilizationPercentage : 80 poller : replicaCount : 0 logLevel : \"WARNING\" In the example above both trap and sender pods are autoscaled. During an upgrade process, the number of pods is created through minReplicas , and then new ones are created only if the CPU threshold exceeds the targetCPUUtilizationPercentage , which by default is 80%. This solution helps you to keep resources usage adjusted to what you actually need. After helm upgrade process, you will see horizontalpodautoscaler in microk8s kubectl get all -n sc4snmp : NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE horizontalpodautoscaler.autoscaling/snmp-mibserver Deployment/snmp-mibserver 1%/80% 1 3 1 97m horizontalpodautoscaler.autoscaling/snmp-splunk-connect-for-snmp-worker-sender Deployment/snmp-splunk-connect-for-snmp-worker-sender 1%/80% 2 5 2 28m horizontalpodautoscaler.autoscaling/snmp-splunk-connect-for-snmp-worker-trap Deployment/snmp-splunk-connect-for-snmp-worker-trap 1%/80% 4 10 4 28m If you see <unknown>/80% in TARGETS section instead of the CPU percentage, you probably don\u2019t have the metrics-server add-on enabled. Enable it using microk8s enable metrics-server . Real life scenario: I have a significant delay in polling \u00b6 Sometimes when polling is configured to be run frequently and on many devices, workers get overloaded and there is a delay in delivering data to Splunk. To avoid such situations, we can scale poller and sender pods. Because of the walk cycles (walk is a costly operation ran once in a while), poller workers require more resources for a short time. For this reason, enabling autoscaling is recommended. This is the example of values.yaml with autoscaling: worker : trap : autoscaling : enabled : true minReplicas : 4 maxReplicas : 10 targetCPUUtilizationPercentage : 80 sender : autoscaling : enabled : true minReplicas : 2 maxReplicas : 5 targetCPUUtilizationPercentage : 80 poller : autoscaling : enabled : true minReplicas : 2 maxReplicas : 20 targetCPUUtilizationPercentage : 80 logLevel : \"WARNING\" Remember, that the system won\u2019t scale itself infinitely, there is a finite amount of resources that you can allocate. By default, every worker has configured the following resources: resources : limits : cpu : 500m requests : cpu : 250m I have autoscaling enabled and experience problems with Mongo and Redis pod \u00b6 If MongoDB and Redis pods are crushing, and some of the pods are in infinite Pending state, that means you\u2019re over your resources and SC4SNMP cannot scale more. You should decrease the number of maxReplicas in workers, so that it\u2019s not going beyond the available CPU. I don\u2019t know how to set autoscaling parameters and how many replicas I need \u00b6 The best way to see if pods are overloaded is to run: microk8s kubectl top pods -n sc4snmp NAME CPU(cores) MEMORY(bytes) snmp-mibserver-7f879c5b7c-nnlfj 1m 3Mi snmp-mongodb-869cc8586f-q8lkm 18m 225Mi snmp-redis-master-0 10m 2Mi snmp-splunk-connect-for-snmp-scheduler-558dccfb54-nb97j 2m 136Mi snmp-splunk-connect-for-snmp-trap-5878f89bbf-24wrz 2m 129Mi snmp-splunk-connect-for-snmp-trap-5878f89bbf-z9gd5 2m 129Mi snmp-splunk-connect-for-snmp-worker-poller-599c7fdbfb-cfqjm 260m 354Mi snmp-splunk-connect-for-snmp-worker-poller-599c7fdbfb-ztf7l 312m 553Mi snmp-splunk-connect-for-snmp-worker-sender-579f796bbd-vmw88 14m 257Mi snmp-splunk-connect-for-snmp-worker-trap-5474db6fc6-46zhf 3m 259Mi snmp-splunk-connect-for-snmp-worker-trap-5474db6fc6-mjtpv 4m 259Mi Here you can see how much CPU and Memory is being used by the pods. If the CPU is close to 500m (which is the limit for one pod by default), you should enable autoscaling/increase maxReplicas or increase replicaCount with autoscaling off. Here you can read about Horizontal Autoscaling and how to adjust maximum replica value to the resources you have: Horizontal Autoscaling. Worker parameters \u00b6 variable description default worker.taskTimeout task timeout in seconds (usually necessary when walk process takes a long time) 2400 worker.walkRetryMaxInterval maximum time interval between walk attempts 600 worker.poller.replicaCount number of poller worker replicas 2 worker.poller.autoscaling.enabled enabling autoscaling for poller worker pods false worker.poller.autoscaling.minReplicas minimum number of running poller worker pods when autoscaling is enabled 2 worker.poller.autoscaling.maxReplicas maximum number of running poller worker pods when autoscaling is enabled 40 worker.poller.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on poller worker pods to spawn another replica 80 worker.poller.resources.limits the resources limits for poller worker container {} worker.poller.resources.requests the requested resources for poller worker container {} worker.trap.replicaCount number of trap worker replicas 2 worker.trap.autoscaling.enabled enabling autoscaling for trap worker pods false worker.trap.autoscaling.minReplicas minimum number of running trap worker pods when autoscaling is enabled 2 worker.trap.autoscaling.maxReplicas maximum number of running trap worker pods when autoscaling is enabled 40 worker.trap.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on trap worker pods to spawn another replica 80 worker.trap.resources.limits the resources limits for poller worker container {} worker.trap.resources.requests the requested resources for poller worker container {} worker.sender.replicaCount number of sender worker replicas 2 worker.sender.autoscaling.enabled enabling autoscaling for sender worker pods false worker.sender.autoscaling.minReplicas minimum number of running sender worker pods when autoscaling is enabled 2 worker.sender.autoscaling.maxReplicas maximum number of running sender worker pods when autoscaling is enabled 40 worker.sender.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on sender worker pods to spawn another replica 80 worker.sender.resources.limits the resources limits for poller worker container {} worker.sender.resources.requests the requested resources for poller worker container {}","title":"Worker"},{"location":"configuration/worker-configuration/#worker-configuration","text":"The worker is a kubernetes pod which is responsible for the actual execution of polling, processing trap messages, and sending data to Splunk.","title":"Worker Configuration"},{"location":"configuration/worker-configuration/#worker-types","text":"SC4SNMP has two base functionalities: monitoring traps and polling. These operations are handled by 3 types of workers: The trap worker consumes all the trap related tasks produced by the trap pod. The poller worker consumes all the tasks related to polling. The sender worker handles sending data to Splunk. You need to always have at least one sender pod running.","title":"Worker types"},{"location":"configuration/worker-configuration/#worker-configuration-file","text":"Worker configuration is kept in the values.yaml file in the worker section. worker has 3 subsections: poller , sender , or trap , that refer to the workers\u2019 types. values.yaml is used during the installation process for configuring Kubernetes values. The worker default configuration is: worker : # There are 3 types of workers trap : # replicaCount: number of trap-worker pods which consumes trap tasks replicaCount : 2 #autoscaling: use it instead of replicaCount in order to make pods scalable by itself #autoscaling: # enabled: true # minReplicas: 2 # maxReplicas: 10 # targetCPUUtilizationPercentage: 80 poller : # replicaCount: number of poller-worker pods which consumes polling tasks replicaCount : 2 #autoscaling: use it instead of replicaCount in order to make pods scalable by itself #autoscaling: # enabled: true # minReplicas: 2 # maxReplicas: 10 # targetCPUUtilizationPercentage: 80 sender : # replicaCount: number of sender-worker pods which consumes sending tasks replicaCount : 1 # autoscaling: use it instead of replicaCount in order to make pods scalable by itself #autoscaling: # enabled: true # minReplicas: 2 # maxReplicas: 10 # targetCPUUtilizationPercentage: 80 # udpConnectionTimeout: timeout in seconds for SNMP operations #udpConnectionTimeout: 5 logLevel : \"INFO\" All parameters are described in the Worker parameters section.","title":"Worker configuration file"},{"location":"configuration/worker-configuration/#worker-scaling","text":"You can adjust worker pods in two ways: set fixed value in replicaCount , or enable autoscaling , which scales pods automatically.","title":"Worker scaling"},{"location":"configuration/worker-configuration/#real-life-scenario-i-use-sc4snmp-for-only-trap-monitoring-i-want-to-use-my-resources-effectively","text":"If you don\u2019t use polling at all, set worker.poller.replicaCount to 0 . If you\u2019ll want to use polling in the future, you need to increase replicaCount . To monitor traps, adjust worker.trap.replicaCount depending on your needs and worker.sender.replicaCount to send traps to Splunk. Usually you need much less sender pods than trap ones. This is the example of values.yaml without using autoscaling: worker : trap : replicaCount : 4 sender : replicaCount : 1 poller : replicaCount : 0 logLevel : \"WARNING\" This is the example of values.yaml with autoscaling: worker : trap : autoscaling : enabled : true minReplicas : 4 maxReplicas : 10 targetCPUUtilizationPercentage : 80 sender : autoscaling : enabled : true minReplicas : 2 maxReplicas : 5 targetCPUUtilizationPercentage : 80 poller : replicaCount : 0 logLevel : \"WARNING\" In the example above both trap and sender pods are autoscaled. During an upgrade process, the number of pods is created through minReplicas , and then new ones are created only if the CPU threshold exceeds the targetCPUUtilizationPercentage , which by default is 80%. This solution helps you to keep resources usage adjusted to what you actually need. After helm upgrade process, you will see horizontalpodautoscaler in microk8s kubectl get all -n sc4snmp : NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE horizontalpodautoscaler.autoscaling/snmp-mibserver Deployment/snmp-mibserver 1%/80% 1 3 1 97m horizontalpodautoscaler.autoscaling/snmp-splunk-connect-for-snmp-worker-sender Deployment/snmp-splunk-connect-for-snmp-worker-sender 1%/80% 2 5 2 28m horizontalpodautoscaler.autoscaling/snmp-splunk-connect-for-snmp-worker-trap Deployment/snmp-splunk-connect-for-snmp-worker-trap 1%/80% 4 10 4 28m If you see <unknown>/80% in TARGETS section instead of the CPU percentage, you probably don\u2019t have the metrics-server add-on enabled. Enable it using microk8s enable metrics-server .","title":"Real life scenario: I use SC4SNMP for only trap monitoring, I want to use my resources effectively."},{"location":"configuration/worker-configuration/#real-life-scenario-i-have-a-significant-delay-in-polling","text":"Sometimes when polling is configured to be run frequently and on many devices, workers get overloaded and there is a delay in delivering data to Splunk. To avoid such situations, we can scale poller and sender pods. Because of the walk cycles (walk is a costly operation ran once in a while), poller workers require more resources for a short time. For this reason, enabling autoscaling is recommended. This is the example of values.yaml with autoscaling: worker : trap : autoscaling : enabled : true minReplicas : 4 maxReplicas : 10 targetCPUUtilizationPercentage : 80 sender : autoscaling : enabled : true minReplicas : 2 maxReplicas : 5 targetCPUUtilizationPercentage : 80 poller : autoscaling : enabled : true minReplicas : 2 maxReplicas : 20 targetCPUUtilizationPercentage : 80 logLevel : \"WARNING\" Remember, that the system won\u2019t scale itself infinitely, there is a finite amount of resources that you can allocate. By default, every worker has configured the following resources: resources : limits : cpu : 500m requests : cpu : 250m","title":"Real life scenario: I have a significant delay in polling"},{"location":"configuration/worker-configuration/#i-have-autoscaling-enabled-and-experience-problems-with-mongo-and-redis-pod","text":"If MongoDB and Redis pods are crushing, and some of the pods are in infinite Pending state, that means you\u2019re over your resources and SC4SNMP cannot scale more. You should decrease the number of maxReplicas in workers, so that it\u2019s not going beyond the available CPU.","title":"I have autoscaling enabled and experience problems with Mongo and Redis pod"},{"location":"configuration/worker-configuration/#i-dont-know-how-to-set-autoscaling-parameters-and-how-many-replicas-i-need","text":"The best way to see if pods are overloaded is to run: microk8s kubectl top pods -n sc4snmp NAME CPU(cores) MEMORY(bytes) snmp-mibserver-7f879c5b7c-nnlfj 1m 3Mi snmp-mongodb-869cc8586f-q8lkm 18m 225Mi snmp-redis-master-0 10m 2Mi snmp-splunk-connect-for-snmp-scheduler-558dccfb54-nb97j 2m 136Mi snmp-splunk-connect-for-snmp-trap-5878f89bbf-24wrz 2m 129Mi snmp-splunk-connect-for-snmp-trap-5878f89bbf-z9gd5 2m 129Mi snmp-splunk-connect-for-snmp-worker-poller-599c7fdbfb-cfqjm 260m 354Mi snmp-splunk-connect-for-snmp-worker-poller-599c7fdbfb-ztf7l 312m 553Mi snmp-splunk-connect-for-snmp-worker-sender-579f796bbd-vmw88 14m 257Mi snmp-splunk-connect-for-snmp-worker-trap-5474db6fc6-46zhf 3m 259Mi snmp-splunk-connect-for-snmp-worker-trap-5474db6fc6-mjtpv 4m 259Mi Here you can see how much CPU and Memory is being used by the pods. If the CPU is close to 500m (which is the limit for one pod by default), you should enable autoscaling/increase maxReplicas or increase replicaCount with autoscaling off. Here you can read about Horizontal Autoscaling and how to adjust maximum replica value to the resources you have: Horizontal Autoscaling.","title":"I don't know how to set autoscaling parameters and how many replicas I need"},{"location":"configuration/worker-configuration/#worker-parameters","text":"variable description default worker.taskTimeout task timeout in seconds (usually necessary when walk process takes a long time) 2400 worker.walkRetryMaxInterval maximum time interval between walk attempts 600 worker.poller.replicaCount number of poller worker replicas 2 worker.poller.autoscaling.enabled enabling autoscaling for poller worker pods false worker.poller.autoscaling.minReplicas minimum number of running poller worker pods when autoscaling is enabled 2 worker.poller.autoscaling.maxReplicas maximum number of running poller worker pods when autoscaling is enabled 40 worker.poller.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on poller worker pods to spawn another replica 80 worker.poller.resources.limits the resources limits for poller worker container {} worker.poller.resources.requests the requested resources for poller worker container {} worker.trap.replicaCount number of trap worker replicas 2 worker.trap.autoscaling.enabled enabling autoscaling for trap worker pods false worker.trap.autoscaling.minReplicas minimum number of running trap worker pods when autoscaling is enabled 2 worker.trap.autoscaling.maxReplicas maximum number of running trap worker pods when autoscaling is enabled 40 worker.trap.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on trap worker pods to spawn another replica 80 worker.trap.resources.limits the resources limits for poller worker container {} worker.trap.resources.requests the requested resources for poller worker container {} worker.sender.replicaCount number of sender worker replicas 2 worker.sender.autoscaling.enabled enabling autoscaling for sender worker pods false worker.sender.autoscaling.minReplicas minimum number of running sender worker pods when autoscaling is enabled 2 worker.sender.autoscaling.maxReplicas maximum number of running sender worker pods when autoscaling is enabled 40 worker.sender.autoscaling.targetCPUUtilizationPercentage CPU % threshold that must be exceeded on sender worker pods to spawn another replica 80 worker.sender.resources.limits the resources limits for poller worker container {} worker.sender.resources.requests the requested resources for poller worker container {}","title":"Worker parameters"},{"location":"gettingstarted/sc4snmp-installation/","text":"SC4SNMP Helm installation \u00b6 The basic installation and configuration process discussed in this section is typical for single node non-HA deployments. It does not have resource requests and limits. See the mongo, redis, scheduler, worker, and traps configuration sections for guidance on production configuration. Installation process \u00b6 Offline installation \u00b6 For offline installation instructions see this page . Online installation \u00b6 Add SC4SNMP repository \u00b6 microk8s helm3 repo add splunk - connect - for - snmp https : // splunk . github . io / splunk - connect - for - snmp microk8s helm3 repo update Now the package should be visible in helm3 search command result: microk8s helm3 search repo snmp Example output: NAME CHART VERSION APP VERSION DESCRIPTION splunk - connect - for - snmp / splunk - connect - for - snmp 1 . 0 . 0 1 . 0 . 0 A Helm chart for SNMP Connect for SNMP Download and modify values.yaml \u00b6 The installation of SC4SNMP requires the creation of a values.yaml file, which serves as the configuration file. To configure this file, follow these steps: Start with checking out the basic configuration template Review the examples to determine which areas require configuration. For more advanced configuration options, refer to the complete default values.yaml or download it directly from Helm using the command microk8s helm3 show values splunk-connect-for-snmp/splunk-connect-for-snmp In order to learn more about each of the config parts, check configuration section. It is recommended to start by completing the base template and gradually add additional configurations as needed. Install SC4SNMP \u00b6 After the values.yaml creation, you can proceed with the SC4SNMP installation: microk8s helm3 install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace = sc4snmp --create-namespace From now on, when editing SC4SNMP configuration, the configuration change must be inserted in the corresponding section of values.yaml . For more details check configuration section. Use the following command to propagate configuration changes: microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace = sc4snmp --create-namespace Verification of the deployment \u00b6 In a few minutes, all pods should be up and running. It can be verified with: microk8s kubectl get pods -n sc4snmp Example output: NAME READY STATUS RESTARTS AGE snmp - splunk - connect - for - snmp - scheduler - 7 ddbc8d75 - bljsj 1 / 1 Running 0 133 m snmp - splunk - connect - for - snmp - worker - poller - 57 cd8f4665 - 9 z9vx 1 / 1 Running 0 133 m snmp - splunk - connect - for - snmp - worker - sender - 5 c44cbb9c5 - ppmb5 1 / 1 Running 0 133 m snmp - splunk - connect - for - snmp - worker - trap - 549766 d4 - 28 qzh 1 / 1 Running 0 133 m snmp - mibserver - 7 f879c5b7c - hz9tz 1 / 1 Running 0 133 m snmp - mongodb - 869 cc8586f - vvr9f 2 / 2 Running 0 133 m snmp - redis - master - 0 1 / 1 Running 0 133 m snmp - splunk - connect - for - snmp - trap - 78759 bfc8b - 79 m6d 1 / 1 Running 0 99 m snmp - splunk - connect - for - snmp - inventory - mjccw 0 / 1 Completed 0 6 s The output may vary depending on the configuration. In the above example, both polling and traps are configured, and the data is being sent to Splunk. If you have traps configured, you should see EXTERNAL-IP in snmp-splunk-connect-for-snmp-trap service. Check it using the command: microk8s kubectl get svc -n sc4snmp Here is an example of the correct setup: NAME TYPE CLUSTER - IP EXTERNAL - IP PORT ( S ) AGE snmp - redis - headless ClusterIP None < none > 6379 / TCP 33 h snmp - mongodb ClusterIP 10 . 152 . 183 . 147 < none > 27017 / TCP 33 h snmp - mibserver ClusterIP 10 . 152 . 183 . 253 < none > 80 / TCP 33 h snmp - redis - master ClusterIP 10 . 152 . 183 . 135 < none > 6379 / TCP 33 h snmp - mongodb - metrics ClusterIP 10 . 152 . 183 . 217 < none > 9216 / TCP 33 h snmp - splunk - connect - for - snmp - trap LoadBalancer 10 . 152 . 183 . 33 10 . 202 . 9 . 21 162 : 30161 / UDP 33 h If there\u2019s <pending> communicate instead of the IP address, that means you either provided the wrong IP address in traps.loadBalancerIP or there\u2019s something wrong with the metallb microk8s addon. For the sake of the example, let\u2019s assume we haven\u2019t changed the default indexes names and the metric data goes to netmetrics and the events goes to netops . Test SNMP Traps \u00b6 Simulate the event. On a Linux system, you can download snmpd package for its purpose and run: apt update apt-get install snmpd snmptrap -v2c -c public EXTERNAL-IP 123 1 .3.6.1.2.1.1.4 1 .3.6.1.2.1.1.4 s test Remember to replace EXTERNAL-IP with the ip address of the snmp-splunk-connect-for-snmp-trap service from the above. Search Splunk: You should see one event per trap command with the host value of the test machine EXTERNAL-IP IP address. index = \"netops\" sourcetype = \"sc4snmp:traps\" Test SNMP Poller \u00b6 To test SNMP poller, you can either use the device you already have, or configure snmpd on your Linux system. Snmpd needs to be configured to listen on the external IP. To enable listening snmpd to external IP, go to the /etc/snmp/snmpd.conf configuration file, and replace the IP address 10.0.101.22 with the server IP address where snmpd is configured: agentaddress 10.0.101.22,127.0.0.1,[::1] . Restart snmpd through the execute command: service snmpd stop service snmpd start Configure SC4SNMP Poller to test and add the IP address which you want to poll. Add the configuration entry into the values.yaml file by replacing the IP address 10.0.101.22 with the server IP address where the snmpd was configured. poller: inventory: | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10 .0.101.22,,2c,public,,,42000,,, Load values.yaml file into SC4SNMP microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace = sc4snmp --create-namespace Verify if the records appeared in Splunk: index = \"netops\" sourcetype = \"sc4snmp:event\" | mpreview index = \"netmetrics\" | search sourcetype = \"sc4snmp:metric\" NOTE: Before polling starts, SC4SNMP must perform SNMP WALK process on the device. It is run first time after configuring the new device, and then the run time in every walk_interval . Its purpose is to gather all the data and provide meaningful context for the polling records. For example, it might report that your device is so large that the walk takes too long, so the scope of walking needs to be limited. In such cases, enable the small walk. See: walk takes too much time . When the walk finishes, events appear in Splunk. Next Steps \u00b6 A good way to start with SC4SNMP polling is to follow the Step by Step guide for polling . Advanced configuration of polling is available in Poller configuration section. SNMP data format is explained in SNMP data format section. For advanced trap configuration, check the Traps configuration section.","title":"Install SC4SNMP"},{"location":"gettingstarted/sc4snmp-installation/#sc4snmp-helm-installation","text":"The basic installation and configuration process discussed in this section is typical for single node non-HA deployments. It does not have resource requests and limits. See the mongo, redis, scheduler, worker, and traps configuration sections for guidance on production configuration.","title":"SC4SNMP Helm installation"},{"location":"gettingstarted/sc4snmp-installation/#installation-process","text":"","title":"Installation process"},{"location":"gettingstarted/sc4snmp-installation/#offline-installation","text":"For offline installation instructions see this page .","title":"Offline installation"},{"location":"gettingstarted/sc4snmp-installation/#online-installation","text":"","title":"Online installation"},{"location":"gettingstarted/sc4snmp-installation/#add-sc4snmp-repository","text":"microk8s helm3 repo add splunk - connect - for - snmp https : // splunk . github . io / splunk - connect - for - snmp microk8s helm3 repo update Now the package should be visible in helm3 search command result: microk8s helm3 search repo snmp Example output: NAME CHART VERSION APP VERSION DESCRIPTION splunk - connect - for - snmp / splunk - connect - for - snmp 1 . 0 . 0 1 . 0 . 0 A Helm chart for SNMP Connect for SNMP","title":"Add SC4SNMP repository"},{"location":"gettingstarted/sc4snmp-installation/#download-and-modify-valuesyaml","text":"The installation of SC4SNMP requires the creation of a values.yaml file, which serves as the configuration file. To configure this file, follow these steps: Start with checking out the basic configuration template Review the examples to determine which areas require configuration. For more advanced configuration options, refer to the complete default values.yaml or download it directly from Helm using the command microk8s helm3 show values splunk-connect-for-snmp/splunk-connect-for-snmp In order to learn more about each of the config parts, check configuration section. It is recommended to start by completing the base template and gradually add additional configurations as needed.","title":"Download and modify values.yaml"},{"location":"gettingstarted/sc4snmp-installation/#install-sc4snmp","text":"After the values.yaml creation, you can proceed with the SC4SNMP installation: microk8s helm3 install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace = sc4snmp --create-namespace From now on, when editing SC4SNMP configuration, the configuration change must be inserted in the corresponding section of values.yaml . For more details check configuration section. Use the following command to propagate configuration changes: microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace = sc4snmp --create-namespace","title":"Install SC4SNMP"},{"location":"gettingstarted/sc4snmp-installation/#verification-of-the-deployment","text":"In a few minutes, all pods should be up and running. It can be verified with: microk8s kubectl get pods -n sc4snmp Example output: NAME READY STATUS RESTARTS AGE snmp - splunk - connect - for - snmp - scheduler - 7 ddbc8d75 - bljsj 1 / 1 Running 0 133 m snmp - splunk - connect - for - snmp - worker - poller - 57 cd8f4665 - 9 z9vx 1 / 1 Running 0 133 m snmp - splunk - connect - for - snmp - worker - sender - 5 c44cbb9c5 - ppmb5 1 / 1 Running 0 133 m snmp - splunk - connect - for - snmp - worker - trap - 549766 d4 - 28 qzh 1 / 1 Running 0 133 m snmp - mibserver - 7 f879c5b7c - hz9tz 1 / 1 Running 0 133 m snmp - mongodb - 869 cc8586f - vvr9f 2 / 2 Running 0 133 m snmp - redis - master - 0 1 / 1 Running 0 133 m snmp - splunk - connect - for - snmp - trap - 78759 bfc8b - 79 m6d 1 / 1 Running 0 99 m snmp - splunk - connect - for - snmp - inventory - mjccw 0 / 1 Completed 0 6 s The output may vary depending on the configuration. In the above example, both polling and traps are configured, and the data is being sent to Splunk. If you have traps configured, you should see EXTERNAL-IP in snmp-splunk-connect-for-snmp-trap service. Check it using the command: microk8s kubectl get svc -n sc4snmp Here is an example of the correct setup: NAME TYPE CLUSTER - IP EXTERNAL - IP PORT ( S ) AGE snmp - redis - headless ClusterIP None < none > 6379 / TCP 33 h snmp - mongodb ClusterIP 10 . 152 . 183 . 147 < none > 27017 / TCP 33 h snmp - mibserver ClusterIP 10 . 152 . 183 . 253 < none > 80 / TCP 33 h snmp - redis - master ClusterIP 10 . 152 . 183 . 135 < none > 6379 / TCP 33 h snmp - mongodb - metrics ClusterIP 10 . 152 . 183 . 217 < none > 9216 / TCP 33 h snmp - splunk - connect - for - snmp - trap LoadBalancer 10 . 152 . 183 . 33 10 . 202 . 9 . 21 162 : 30161 / UDP 33 h If there\u2019s <pending> communicate instead of the IP address, that means you either provided the wrong IP address in traps.loadBalancerIP or there\u2019s something wrong with the metallb microk8s addon. For the sake of the example, let\u2019s assume we haven\u2019t changed the default indexes names and the metric data goes to netmetrics and the events goes to netops .","title":"Verification of the deployment"},{"location":"gettingstarted/sc4snmp-installation/#test-snmp-traps","text":"Simulate the event. On a Linux system, you can download snmpd package for its purpose and run: apt update apt-get install snmpd snmptrap -v2c -c public EXTERNAL-IP 123 1 .3.6.1.2.1.1.4 1 .3.6.1.2.1.1.4 s test Remember to replace EXTERNAL-IP with the ip address of the snmp-splunk-connect-for-snmp-trap service from the above. Search Splunk: You should see one event per trap command with the host value of the test machine EXTERNAL-IP IP address. index = \"netops\" sourcetype = \"sc4snmp:traps\"","title":"Test SNMP Traps"},{"location":"gettingstarted/sc4snmp-installation/#test-snmp-poller","text":"To test SNMP poller, you can either use the device you already have, or configure snmpd on your Linux system. Snmpd needs to be configured to listen on the external IP. To enable listening snmpd to external IP, go to the /etc/snmp/snmpd.conf configuration file, and replace the IP address 10.0.101.22 with the server IP address where snmpd is configured: agentaddress 10.0.101.22,127.0.0.1,[::1] . Restart snmpd through the execute command: service snmpd stop service snmpd start Configure SC4SNMP Poller to test and add the IP address which you want to poll. Add the configuration entry into the values.yaml file by replacing the IP address 10.0.101.22 with the server IP address where the snmpd was configured. poller: inventory: | address,port,version,community,secret,security_engine,walk_interval,profiles,smart_profiles,delete 10 .0.101.22,,2c,public,,,42000,,, Load values.yaml file into SC4SNMP microk8s helm3 upgrade --install snmp -f values.yaml splunk-connect-for-snmp/splunk-connect-for-snmp --namespace = sc4snmp --create-namespace Verify if the records appeared in Splunk: index = \"netops\" sourcetype = \"sc4snmp:event\" | mpreview index = \"netmetrics\" | search sourcetype = \"sc4snmp:metric\" NOTE: Before polling starts, SC4SNMP must perform SNMP WALK process on the device. It is run first time after configuring the new device, and then the run time in every walk_interval . Its purpose is to gather all the data and provide meaningful context for the polling records. For example, it might report that your device is so large that the walk takes too long, so the scope of walking needs to be limited. In such cases, enable the small walk. See: walk takes too much time . When the walk finishes, events appear in Splunk.","title":"Test SNMP Poller"},{"location":"gettingstarted/sc4snmp-installation/#next-steps","text":"A good way to start with SC4SNMP polling is to follow the Step by Step guide for polling . Advanced configuration of polling is available in Poller configuration section. SNMP data format is explained in SNMP data format section. For advanced trap configuration, check the Traps configuration section.","title":"Next Steps"},{"location":"gettingstarted/sck-installation/","text":"Splunk OpenTelemetry Collector for Kubernetes installation \u00b6 Splunk OpenTelemetry Collector for Kubernetes is not required for SC4SNMP installation. This is the tool that sends logs and metrics from a k8s cluster to a Splunk instance, which makes SC4SNMP easier to debug. You can do the same using the microk8s kubectl logs command on instances you\u2019re interested in, but if you\u2019re not proficient in Kubernetes, Splunk OpenTelemetry Collector for Kubernetes is strongly advised. The below steps are sufficient for a Splunk OpenTelemetry Collector installation for the SC4SNMP project with Splunk Enterprise/Enterprise Cloud. In order to learn more about Splunk OpenTelemetry Collector, visit Splunk OpenTelemetry Collector . Offline installation \u00b6 For offline installation instructions see this page . Add Splunk OpenTelemetry Collector repository to HELM \u00b6 microk8s helm3 repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart Install Splunk OpenTelemetry Collector with HELM for a Splunk Platform \u00b6 In order to run Splunk OpenTelemetry Collector on your environment, replace <> variables according to the description presented below microk8s helm3 upgrade --install sck \\ --set = \"clusterName=<cluster_name>\" \\ --set = \"splunkPlatform.endpoint=<splunk_endpoint>\" \\ --set = \"splunkPlatform.insecureSkipVerify=<insecure_skip_verify>\" \\ --set = \"splunkPlatform.token=<splunk_token>\" \\ --set = \"logsEngine=otel\" \\ --set = \"splunkPlatform.metricsEnabled=true\" \\ --set = \"splunkPlatform.metricsIndex=em_metrics\" \\ --set = \"splunkPlatform.index=em_logs\" \\ splunk-otel-collector-chart/splunk-otel-collector Variables description \u00b6 Placeholder Description Example splunk_endpoint host address of splunk instance https://endpoint.example.com:8088/services/collector insecure_skip_verify is insecure ssl allowed false splunk_token Splunk HTTP Event Collector token 450a69af-16a9-4f87-9628-c26f04ad3785 cluster_name name of the cluster my-cluster An example of filled up command is: microk8s helm3 upgrade --install sck \\ --set = \"clusterName=my-cluster\" \\ --set = \"splunkPlatform.endpoint=https://endpoint.example.com/services/collector\" \\ --set = \"splunkPlatform.insecureSkipVerify=false\" \\ --set = \"splunkPlatform.token=4d22911c-18d9-4706-ae7b-dd1b976ca6f7\" \\ --set = \"splunkPlatform.metricsEnabled=true\" \\ --set = \"splunkPlatform.metricsIndex=em_metrics\" \\ --set = \"splunkPlatform.index=em_logs\" \\ splunk-otel-collector-chart/splunk-otel-collector Install Splunk OpenTelemetry Collector with HELM for Splunk Observability for Kubernetes \u00b6 To run Splunk OpenTelemetry Collector on your environment, replace the <> variables according to the description presented below: microk8s helm3 upgrade --install sck --set = \"clusterName=<cluster_name>\" --set = \"splunkObservability.realm=<realm>\" --set = \"splunkObservability.accessToken=<token>\" --set = \"splunkObservability.ingestUrl=<ingest_url>\" --set = \"splunkObservability.apiUrl=<api_url>\" --set = \"splunkObservability.metricsEnabled=true\" --set = \"splunkObservability.tracesEnabled=false\" --set = \"splunkObservability.logsEnabled=false\" splunk-otel-collector-chart/splunk-otel-collector Variables description \u00b6 Placeholder Description Example cluster_name name of the cluster my_cluster realm Realm obtained from the Splunk Observability Cloud environment us0 token Token obtained from the Splunk Observability Cloud environment BCwaJ_Ands4Xh7Nrg ingest_url Ingest URL from the Splunk Observability Cloud environment https://ingest..signalfx.com api_url API URL from the Splunk Observability Cloud environment https://api..signalfx.com An example of a filled up command is: microk8s helm3 upgrade --install sck --set = \"clusterName=my_cluster\" --set = \"splunkObservability.realm=us0\" --set = \"splunkObservability.accessToken=BCwaJ_Ands4Xh7Nrg\" --set = \"splunkObservability.ingestUrl=https://ingest..signalfx.com\" --set = \"splunkObservability.apiUrl=https://api..signalfx.com\" --set = \"splunkObservability.metricsEnabled=true\" --set = \"splunkObservability.tracesEnabled=false\" --set = \"splunkObservability.logsEnabled=false\" splunk-otel-collector-chart/splunk-otel-collector","title":"Install Splunk OpenTelemetry Collector for Kubernetes"},{"location":"gettingstarted/sck-installation/#splunk-opentelemetry-collector-for-kubernetes-installation","text":"Splunk OpenTelemetry Collector for Kubernetes is not required for SC4SNMP installation. This is the tool that sends logs and metrics from a k8s cluster to a Splunk instance, which makes SC4SNMP easier to debug. You can do the same using the microk8s kubectl logs command on instances you\u2019re interested in, but if you\u2019re not proficient in Kubernetes, Splunk OpenTelemetry Collector for Kubernetes is strongly advised. The below steps are sufficient for a Splunk OpenTelemetry Collector installation for the SC4SNMP project with Splunk Enterprise/Enterprise Cloud. In order to learn more about Splunk OpenTelemetry Collector, visit Splunk OpenTelemetry Collector .","title":"Splunk OpenTelemetry Collector for Kubernetes installation"},{"location":"gettingstarted/sck-installation/#offline-installation","text":"For offline installation instructions see this page .","title":"Offline installation"},{"location":"gettingstarted/sck-installation/#add-splunk-opentelemetry-collector-repository-to-helm","text":"microk8s helm3 repo add splunk-otel-collector-chart https://signalfx.github.io/splunk-otel-collector-chart","title":"Add Splunk OpenTelemetry Collector repository to HELM"},{"location":"gettingstarted/sck-installation/#install-splunk-opentelemetry-collector-with-helm-for-a-splunk-platform","text":"In order to run Splunk OpenTelemetry Collector on your environment, replace <> variables according to the description presented below microk8s helm3 upgrade --install sck \\ --set = \"clusterName=<cluster_name>\" \\ --set = \"splunkPlatform.endpoint=<splunk_endpoint>\" \\ --set = \"splunkPlatform.insecureSkipVerify=<insecure_skip_verify>\" \\ --set = \"splunkPlatform.token=<splunk_token>\" \\ --set = \"logsEngine=otel\" \\ --set = \"splunkPlatform.metricsEnabled=true\" \\ --set = \"splunkPlatform.metricsIndex=em_metrics\" \\ --set = \"splunkPlatform.index=em_logs\" \\ splunk-otel-collector-chart/splunk-otel-collector","title":"Install Splunk OpenTelemetry Collector with HELM for a Splunk Platform"},{"location":"gettingstarted/sck-installation/#variables-description","text":"Placeholder Description Example splunk_endpoint host address of splunk instance https://endpoint.example.com:8088/services/collector insecure_skip_verify is insecure ssl allowed false splunk_token Splunk HTTP Event Collector token 450a69af-16a9-4f87-9628-c26f04ad3785 cluster_name name of the cluster my-cluster An example of filled up command is: microk8s helm3 upgrade --install sck \\ --set = \"clusterName=my-cluster\" \\ --set = \"splunkPlatform.endpoint=https://endpoint.example.com/services/collector\" \\ --set = \"splunkPlatform.insecureSkipVerify=false\" \\ --set = \"splunkPlatform.token=4d22911c-18d9-4706-ae7b-dd1b976ca6f7\" \\ --set = \"splunkPlatform.metricsEnabled=true\" \\ --set = \"splunkPlatform.metricsIndex=em_metrics\" \\ --set = \"splunkPlatform.index=em_logs\" \\ splunk-otel-collector-chart/splunk-otel-collector","title":"Variables description"},{"location":"gettingstarted/sck-installation/#install-splunk-opentelemetry-collector-with-helm-for-splunk-observability-for-kubernetes","text":"To run Splunk OpenTelemetry Collector on your environment, replace the <> variables according to the description presented below: microk8s helm3 upgrade --install sck --set = \"clusterName=<cluster_name>\" --set = \"splunkObservability.realm=<realm>\" --set = \"splunkObservability.accessToken=<token>\" --set = \"splunkObservability.ingestUrl=<ingest_url>\" --set = \"splunkObservability.apiUrl=<api_url>\" --set = \"splunkObservability.metricsEnabled=true\" --set = \"splunkObservability.tracesEnabled=false\" --set = \"splunkObservability.logsEnabled=false\" splunk-otel-collector-chart/splunk-otel-collector","title":"Install Splunk OpenTelemetry Collector with HELM for Splunk Observability for Kubernetes"},{"location":"gettingstarted/sck-installation/#variables-description_1","text":"Placeholder Description Example cluster_name name of the cluster my_cluster realm Realm obtained from the Splunk Observability Cloud environment us0 token Token obtained from the Splunk Observability Cloud environment BCwaJ_Ands4Xh7Nrg ingest_url Ingest URL from the Splunk Observability Cloud environment https://ingest..signalfx.com api_url API URL from the Splunk Observability Cloud environment https://api..signalfx.com An example of a filled up command is: microk8s helm3 upgrade --install sck --set = \"clusterName=my_cluster\" --set = \"splunkObservability.realm=us0\" --set = \"splunkObservability.accessToken=BCwaJ_Ands4Xh7Nrg\" --set = \"splunkObservability.ingestUrl=https://ingest..signalfx.com\" --set = \"splunkObservability.apiUrl=https://api..signalfx.com\" --set = \"splunkObservability.metricsEnabled=true\" --set = \"splunkObservability.tracesEnabled=false\" --set = \"splunkObservability.logsEnabled=false\" splunk-otel-collector-chart/splunk-otel-collector","title":"Variables description"},{"location":"gettingstarted/splunk-requirements/","text":"Splunk requirements \u00b6 Prepare Splunk \u00b6 See the following prerequisites for the Splunk Connect for SNMP. Requirements (Splunk Enterprise/Enterprise Cloud) \u00b6 Manually create the following indexes in Splunk: Indexes for logs and metrics from SC4SNMP Connector: em_metrics (metrics type) em_logs (event type) Indexes where SNMP Data will be forwarded: netmetrics (metrics type) netops (event type) Note: netmetrics and netops are the default names of SC4SNMP indexes. You can use the index names of your choice and reference it in the values.yaml file later on. See parameters and instructions for details: SC4SNMP Parameters . Create or obtain a new Splunk HTTP Event Collector token and the correct HTTPS endpoint. Verify the token using curl . Note: The endpoint must use a publicly trusted certificate authority. The SHARED IP address to be used for SNMP Traps. Note Simple and POC deployments will use the same IP as the host server. If HA deployment will be used, the IP must be in addition to the management interface of each cluster member. Obtain the IP address of an internal DNS server that can resolve the Splunk Endpoint. Requirements (Splunk Infrastructure Monitoring) \u00b6 Obtain the following from your Splunk Observability Cloud environment: Realm Token","title":"Splunk Requirements"},{"location":"gettingstarted/splunk-requirements/#splunk-requirements","text":"","title":"Splunk requirements"},{"location":"gettingstarted/splunk-requirements/#prepare-splunk","text":"See the following prerequisites for the Splunk Connect for SNMP.","title":"Prepare Splunk"},{"location":"gettingstarted/splunk-requirements/#requirements-splunk-enterpriseenterprise-cloud","text":"Manually create the following indexes in Splunk: Indexes for logs and metrics from SC4SNMP Connector: em_metrics (metrics type) em_logs (event type) Indexes where SNMP Data will be forwarded: netmetrics (metrics type) netops (event type) Note: netmetrics and netops are the default names of SC4SNMP indexes. You can use the index names of your choice and reference it in the values.yaml file later on. See parameters and instructions for details: SC4SNMP Parameters . Create or obtain a new Splunk HTTP Event Collector token and the correct HTTPS endpoint. Verify the token using curl . Note: The endpoint must use a publicly trusted certificate authority. The SHARED IP address to be used for SNMP Traps. Note Simple and POC deployments will use the same IP as the host server. If HA deployment will be used, the IP must be in addition to the management interface of each cluster member. Obtain the IP address of an internal DNS server that can resolve the Splunk Endpoint.","title":"Requirements (Splunk Enterprise/Enterprise Cloud)"},{"location":"gettingstarted/splunk-requirements/#requirements-splunk-infrastructure-monitoring","text":"Obtain the following from your Splunk Observability Cloud environment: Realm Token","title":"Requirements (Splunk Infrastructure Monitoring)"},{"location":"gettingstarted/mk8s/k8s-microk8s/","text":"Splunk Connect for SNMP using MicroK8s \u00b6 See the following requirements to use any Linux deployment of Microk8s to support SC4SMP. The minimum requirements below are suitable for proof of value and small installations, and actual requirements will differ. Single node minimum: 4 cores 8 GB of memory per node 50 GB mounted as / Three node minimum per node: 4 cores 8 GB of memory per node 50 GB mounted / MicroK8s installation on Ubuntu \u00b6 The following quick start guidance is based on Ubuntu 20.04LTS with MicroK8s with internet access. Other deployment options may be found in the MicroK8s documentation including offline and with proxy. Install MicroK8s using Snap \u00b6 sudo snap install microk8s --classic --channel = 1 .25/stable Add a user to the microk8s group so the sudo command is no longer necessary: sudo usermod -a -G microk8s $USER sudo chown -f -R $USER ~/.kube su - $USER Wait for Installation of Mk8S to complete: microk8s status --wait-ready Add additional nodes (optional) \u00b6 Repeat the steps above for each additional node (minimum total 3) On the first node issue the following to return the instructions to join: microk8s add-node On each additional node, use the output from the command above Install basic services required for sc4snmp \u00b6 The following commands can be issued from any one node in a cluster: sudo systemctl enable iscsid microk8s enable helm3 microk8s enable hostpath-storage microk8s enable rbac microk8s enable metrics-server microk8s status --wait-ready Install the DNS server for mk8s and configure the forwarding DNS servers. Replace the IP addressed below (opendns) with allowed values for your network: microk8s enable dns:208.67.222.222,208.67.220.220 microk8s status --wait-ready Install Metallb \u00b6 Note: when installing Metallb you will be prompted for one or more IPs to use as entry points into the cluster. If your plan to enable clustering, this IP should not be assigned to the host (floats). If you do not plan to cluster, then this IP should be the IP of your host. Note2: a single IP in cidr format is x.x.x.x/32. Use CIDR or range syntax for single server installations. This can be the same as the primary IP. microk8s enable metallb microk8s status --wait-ready","title":"Platform Microk8s"},{"location":"gettingstarted/mk8s/k8s-microk8s/#splunk-connect-for-snmp-using-microk8s","text":"See the following requirements to use any Linux deployment of Microk8s to support SC4SMP. The minimum requirements below are suitable for proof of value and small installations, and actual requirements will differ. Single node minimum: 4 cores 8 GB of memory per node 50 GB mounted as / Three node minimum per node: 4 cores 8 GB of memory per node 50 GB mounted /","title":"Splunk Connect for SNMP using MicroK8s"},{"location":"gettingstarted/mk8s/k8s-microk8s/#microk8s-installation-on-ubuntu","text":"The following quick start guidance is based on Ubuntu 20.04LTS with MicroK8s with internet access. Other deployment options may be found in the MicroK8s documentation including offline and with proxy.","title":"MicroK8s installation on Ubuntu"},{"location":"gettingstarted/mk8s/k8s-microk8s/#install-microk8s-using-snap","text":"sudo snap install microk8s --classic --channel = 1 .25/stable Add a user to the microk8s group so the sudo command is no longer necessary: sudo usermod -a -G microk8s $USER sudo chown -f -R $USER ~/.kube su - $USER Wait for Installation of Mk8S to complete: microk8s status --wait-ready","title":"Install MicroK8s using Snap"},{"location":"gettingstarted/mk8s/k8s-microk8s/#add-additional-nodes-optional","text":"Repeat the steps above for each additional node (minimum total 3) On the first node issue the following to return the instructions to join: microk8s add-node On each additional node, use the output from the command above","title":"Add additional nodes (optional)"},{"location":"gettingstarted/mk8s/k8s-microk8s/#install-basic-services-required-for-sc4snmp","text":"The following commands can be issued from any one node in a cluster: sudo systemctl enable iscsid microk8s enable helm3 microk8s enable hostpath-storage microk8s enable rbac microk8s enable metrics-server microk8s status --wait-ready Install the DNS server for mk8s and configure the forwarding DNS servers. Replace the IP addressed below (opendns) with allowed values for your network: microk8s enable dns:208.67.222.222,208.67.220.220 microk8s status --wait-ready","title":"Install basic services required for sc4snmp"},{"location":"gettingstarted/mk8s/k8s-microk8s/#install-metallb","text":"Note: when installing Metallb you will be prompted for one or more IPs to use as entry points into the cluster. If your plan to enable clustering, this IP should not be assigned to the host (floats). If you do not plan to cluster, then this IP should be the IP of your host. Note2: a single IP in cidr format is x.x.x.x/32. Use CIDR or range syntax for single server installations. This can be the same as the primary IP. microk8s enable metallb microk8s status --wait-ready","title":"Install Metallb"},{"location":"offlineinstallation/offline-microk8s/","text":"Offline Microk8s installation issues \u00b6 Offline installation of Microk8s is described here , but there are additional steps to install microk8s offline. Importing images \u00b6 After running: snap ack microk8s_{microk8s_version}.assert snap install microk8s_{microk8s_version}.snap --classic You should check if the microk8s instance is healthy. Do it with: microk8s kubectl get pods -A The output will probably look like: NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-7c9c8dd885-fg8f2 0/1 Pending 0 14m kube-system calico-node-zg4c4 0/1 Init:0/3 0 23s The pods are in the Pending / Init state because they\u2019re trying to download images, which is impossible to do offline. In order to make them work you need to download all the images on a different server with an internet connection, pack it up, and import it to a microk8s image registry on your offline server. Packing up images for offline environment \u00b6 You need to monitor microk8s kubectl get events -A to see if microk8s fails to pull images, and then import anything it needs. An example of such information is: kube - system 0 s Warning Failed pod / calico - node - sc784 Failed to pull image \"docker.io/calico/cni:v3.21.4\" : rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/calico/cni:v3.21.4\" : failed to resolve reference \"docker.io/calico/cni:v3.21.4\" : failed to do request: Head \"https://registry-1.docker.io/v2/calico/cni/manifests/v3.21.4\" : dial tcp 54.83.42.45 : 443 : i / o timeout kube - system 0 s Warning Failed pod / calico - node - sc784 Error: ErrImagePull This shows you that you lack a docker.io/calico/cni:v3.21.4 image, and need to import it in order to fix the issue. The process of such action is always: docker pull <needed_image> docker save <needed_image> > image.tar Transfer package to the offline lab and execute: microk8s ctr image import image.tar Example of the offline installation \u00b6 For example, microk8s version 3597 requires these images to work correctly: docker pull docker . io / calico / kube - controllers : v3 . 21 . 4 docker pull docker . io / calico / node : v3 . 21 . 4 docker pull docker . io / calico / pod2daemon - flexvol : v3 . 21 . 4 docker pull docker . io / calico / cni : v3 . 21 . 4 docker pull k8s . gcr . io / pause : 3 . 1 docker pull k8s . gcr . io / metrics - server / metrics - server : v0 . 5 . 2 You should issue the above commands on your instance connected to the internet, then save it to tar packages: docker save docker . io / calico / kube - controllers : v3 . 21 . 4 > kube - controllers . tar docker save docker . io / calico / node : v3 . 21 . 4 > node . tar docker save docker . io / calico / pod2daemon - flexvol : v3 . 21 . 4 > pod2daemon - flexvol . tar docker save docker . io / calico / cni : v3 . 21 . 4 > cni . tar docker save k8s . gcr . io / pause : 3 . 1 > pause . tar docker save cdkbot / hostpath - provisioner : 1 . 2 . 0 > cdkbot . tar docker save k8s . gcr . io / metrics - server / metrics - server : v0 . 5 . 2 > metrics . tar After that, scp those packages to your offline server and import it to its microk8s image registry: microk8s ctr image import kube - controllers . tar microk8s ctr image import node.tar microk8s ctr image import pod2daemon - flexvol . tar microk8s ctr image import cni.tar microk8s ctr image import pause.tar microk8s ctr image import metrics.tar NOTE: for other versions of microk8s , tags of images may differ. The healthy instance of microk8s, after running: microk8s enable hostpath-storage microk8s enable rbac microk8s enable metrics-server should look like this: NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-7c9c8dd885-wxms9 1/1 Running 0 3h21m kube-system calico-node-8cxsq 1/1 Running 0 3h21m kube-system hostpath-provisioner-f57964d5f-zs4sj 1/1 Running 0 5m41s kube-system metrics-server-5f8f64cb86-x7k29 1/1 Running 0 2m15s Enabling DNS and Metallb \u00b6 The dns and metallb don\u2019t require importing any images, so you can enable them simply by: microk8s enable dns microk8s enable metallb More on metallb here . Installing helm3 \u00b6 The additional problem is the installation of helm3 add-on. You need to do a few things to make it work. Check your server\u2019s platform with: dpkg --print-architecture The output would be for ex.: amd64 . You need the platform to download the correct version of helm. Download the helm package from https://get.helm.sh/helm-v3.8.0-linux-{{arch}}.tar.gz , where {{arch}} should be replaced with the result from the previous command. Example: https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz Rename package to helm.tar.gz and send it to an offline lab. Create tmp directory in /var/snap/microk8s/current and copy the package there: sudo mkdir - p / var / snap / microk8s / current / tmp / helm3 sudo cp helm . tar . gz / var / snap / microk8s / current / tmp / helm3 Go to the directory containing enable script for helm3 : cd / var / snap / microk8s / common / addons / core / addons / helm3 Open enable file with vi, nano, or some other editor. Comment this line: #fetch_as $SOURCE_URI /helm- $HELM_VERSION -linux- ${ SNAP_ARCH } .tar.gz \" $SNAP_DATA /tmp/helm3/helm.tar.gz\" Save file. Run microk8s enable helm3 Verify your instance \u00b6 Check if all the add-ons were installed successfully with command: microk8s status --wait-ready . An example of a correct output is: microk8s is running high - availability : no datastore master nodes : 127.0.0.1 : 19001 datastore standby nodes : none addons : enabled : dns # ( core ) CoreDNS ha - cluster # ( core ) Configure high availability on the current node helm3 # ( core ) Helm 3 - Kubernetes package manager hostpath - storage # ( core ) Storage class ; allocates storage from host directory metallb # ( core ) Loadbalancer for your Kubernetes cluster metrics - server # ( core ) K8s Metrics Server for API access to service metrics rbac # ( core ) Role - Based Access Control for authorisation storage # ( core ) Alias to hostpath - storage add - on , deprecated disabled : community # ( core ) The community addons repository dashboard # ( core ) The Kubernetes dashboard gpu # ( core ) Automatic enablement of Nvidia CUDA helm # ( core ) Helm 2 - the package manager for Kubernetes host - access # ( core ) Allow Pods connecting to Host services smoothly ingress # ( core ) Ingress controller for external access mayastor # ( core ) OpenEBS MayaStor prometheus # ( core ) Prometheus operator for monitoring and logging registry # ( core ) Private image registry exposed on localhost : 32000","title":"Install Microk8s"},{"location":"offlineinstallation/offline-microk8s/#offline-microk8s-installation-issues","text":"Offline installation of Microk8s is described here , but there are additional steps to install microk8s offline.","title":"Offline Microk8s installation issues"},{"location":"offlineinstallation/offline-microk8s/#importing-images","text":"After running: snap ack microk8s_{microk8s_version}.assert snap install microk8s_{microk8s_version}.snap --classic You should check if the microk8s instance is healthy. Do it with: microk8s kubectl get pods -A The output will probably look like: NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-7c9c8dd885-fg8f2 0/1 Pending 0 14m kube-system calico-node-zg4c4 0/1 Init:0/3 0 23s The pods are in the Pending / Init state because they\u2019re trying to download images, which is impossible to do offline. In order to make them work you need to download all the images on a different server with an internet connection, pack it up, and import it to a microk8s image registry on your offline server.","title":"Importing images"},{"location":"offlineinstallation/offline-microk8s/#packing-up-images-for-offline-environment","text":"You need to monitor microk8s kubectl get events -A to see if microk8s fails to pull images, and then import anything it needs. An example of such information is: kube - system 0 s Warning Failed pod / calico - node - sc784 Failed to pull image \"docker.io/calico/cni:v3.21.4\" : rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/calico/cni:v3.21.4\" : failed to resolve reference \"docker.io/calico/cni:v3.21.4\" : failed to do request: Head \"https://registry-1.docker.io/v2/calico/cni/manifests/v3.21.4\" : dial tcp 54.83.42.45 : 443 : i / o timeout kube - system 0 s Warning Failed pod / calico - node - sc784 Error: ErrImagePull This shows you that you lack a docker.io/calico/cni:v3.21.4 image, and need to import it in order to fix the issue. The process of such action is always: docker pull <needed_image> docker save <needed_image> > image.tar Transfer package to the offline lab and execute: microk8s ctr image import image.tar","title":"Packing up images for offline environment"},{"location":"offlineinstallation/offline-microk8s/#example-of-the-offline-installation","text":"For example, microk8s version 3597 requires these images to work correctly: docker pull docker . io / calico / kube - controllers : v3 . 21 . 4 docker pull docker . io / calico / node : v3 . 21 . 4 docker pull docker . io / calico / pod2daemon - flexvol : v3 . 21 . 4 docker pull docker . io / calico / cni : v3 . 21 . 4 docker pull k8s . gcr . io / pause : 3 . 1 docker pull k8s . gcr . io / metrics - server / metrics - server : v0 . 5 . 2 You should issue the above commands on your instance connected to the internet, then save it to tar packages: docker save docker . io / calico / kube - controllers : v3 . 21 . 4 > kube - controllers . tar docker save docker . io / calico / node : v3 . 21 . 4 > node . tar docker save docker . io / calico / pod2daemon - flexvol : v3 . 21 . 4 > pod2daemon - flexvol . tar docker save docker . io / calico / cni : v3 . 21 . 4 > cni . tar docker save k8s . gcr . io / pause : 3 . 1 > pause . tar docker save cdkbot / hostpath - provisioner : 1 . 2 . 0 > cdkbot . tar docker save k8s . gcr . io / metrics - server / metrics - server : v0 . 5 . 2 > metrics . tar After that, scp those packages to your offline server and import it to its microk8s image registry: microk8s ctr image import kube - controllers . tar microk8s ctr image import node.tar microk8s ctr image import pod2daemon - flexvol . tar microk8s ctr image import cni.tar microk8s ctr image import pause.tar microk8s ctr image import metrics.tar NOTE: for other versions of microk8s , tags of images may differ. The healthy instance of microk8s, after running: microk8s enable hostpath-storage microk8s enable rbac microk8s enable metrics-server should look like this: NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-7c9c8dd885-wxms9 1/1 Running 0 3h21m kube-system calico-node-8cxsq 1/1 Running 0 3h21m kube-system hostpath-provisioner-f57964d5f-zs4sj 1/1 Running 0 5m41s kube-system metrics-server-5f8f64cb86-x7k29 1/1 Running 0 2m15s","title":"Example of the offline installation"},{"location":"offlineinstallation/offline-microk8s/#enabling-dns-and-metallb","text":"The dns and metallb don\u2019t require importing any images, so you can enable them simply by: microk8s enable dns microk8s enable metallb More on metallb here .","title":"Enabling DNS and Metallb"},{"location":"offlineinstallation/offline-microk8s/#installing-helm3","text":"The additional problem is the installation of helm3 add-on. You need to do a few things to make it work. Check your server\u2019s platform with: dpkg --print-architecture The output would be for ex.: amd64 . You need the platform to download the correct version of helm. Download the helm package from https://get.helm.sh/helm-v3.8.0-linux-{{arch}}.tar.gz , where {{arch}} should be replaced with the result from the previous command. Example: https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz Rename package to helm.tar.gz and send it to an offline lab. Create tmp directory in /var/snap/microk8s/current and copy the package there: sudo mkdir - p / var / snap / microk8s / current / tmp / helm3 sudo cp helm . tar . gz / var / snap / microk8s / current / tmp / helm3 Go to the directory containing enable script for helm3 : cd / var / snap / microk8s / common / addons / core / addons / helm3 Open enable file with vi, nano, or some other editor. Comment this line: #fetch_as $SOURCE_URI /helm- $HELM_VERSION -linux- ${ SNAP_ARCH } .tar.gz \" $SNAP_DATA /tmp/helm3/helm.tar.gz\" Save file. Run microk8s enable helm3","title":"Installing helm3"},{"location":"offlineinstallation/offline-microk8s/#verify-your-instance","text":"Check if all the add-ons were installed successfully with command: microk8s status --wait-ready . An example of a correct output is: microk8s is running high - availability : no datastore master nodes : 127.0.0.1 : 19001 datastore standby nodes : none addons : enabled : dns # ( core ) CoreDNS ha - cluster # ( core ) Configure high availability on the current node helm3 # ( core ) Helm 3 - Kubernetes package manager hostpath - storage # ( core ) Storage class ; allocates storage from host directory metallb # ( core ) Loadbalancer for your Kubernetes cluster metrics - server # ( core ) K8s Metrics Server for API access to service metrics rbac # ( core ) Role - Based Access Control for authorisation storage # ( core ) Alias to hostpath - storage add - on , deprecated disabled : community # ( core ) The community addons repository dashboard # ( core ) The Kubernetes dashboard gpu # ( core ) Automatic enablement of Nvidia CUDA helm # ( core ) Helm 2 - the package manager for Kubernetes host - access # ( core ) Allow Pods connecting to Host services smoothly ingress # ( core ) Ingress controller for external access mayastor # ( core ) OpenEBS MayaStor prometheus # ( core ) Prometheus operator for monitoring and logging registry # ( core ) Private image registry exposed on localhost : 32000","title":"Verify your instance"},{"location":"offlineinstallation/offline-sc4snmp/","text":"Offline SC4SNMP installation \u00b6 Local machine with internet access \u00b6 To install the SC4SNMP offline, first, some packages must be downloaded from the Github release and then moved to the SC4SNMP installation server. Those packages are: dependencies-images.tar splunk-connect-for-snmp-chart.tar Additionally, you\u2019ll need pull_mibserver.sh script to easily pull and export mibserver image. Moreover, SC4SNMP Docker image must be pulled, saved as a .tar package, and then moved to the server as well. This process requires Docker to be installed locally. Images can be pulled from the following repository: ghcr.io/splunk/splunk-connect-for-snmp/container:<tag> . The latest tag can be found here under the Releases section with the label latest . Example of docker pull command: docker pull ghcr.io/splunk/splunk-connect-for-snmp/container:<tag> Then save the image. Directory where this image will be saved can be specified after the > sign: docker save ghcr.io/splunk/splunk-connect-for-snmp/container:<tag> > snmp_image.tar Another package you have to pull is the mibserver image. You can do it by executing pull_mibserver.sh script from the Release section, or copy-pasting its content. chmod a+x pull_mibserver.sh # you'll probably need to make file executable ./pull_mibserver.sh This script should produce mibserver.tar with the image of the mibserver inside. All four packages, mibserver.tar , snmp_image.tar , dependencies-images.tar , and splunk-connect-for-snmp-chart.tar , must be moved to the SC4SNMP installation server. Installation on the server \u00b6 On the server, all the images must be imported to the microk8s cluster. This can be done with the following command: microk8s ctr image import <name_of_tar_image> In case of this installation the following commands must be run: microk8s ctr image import dependencies-images.tar microk8s ctr image import snmp_image.tar microk8s ctr image import mibserver.tar Then create values.yaml . It\u2019s a little different from values.yaml used in an online installation. The difference between the two files is the following, which is used for automatic image pulling: image : pullPolicy : \"Never\" Example values.yaml file can be found here . The next step is to unpack the chart package splunk-connect-for-snmp-chart.tar . It will result in creating the splunk-connect-for-snmp directory: tar -xvf splunk-connect-for-snmp-chart.tar --exclude = '._*' Finally, run the helm install command in the directory where both the values.yaml and splunk-connect-for-snmp directories are located: microk8s helm3 install snmp -f values.yaml splunk-connect-for-snmp --namespace = sc4snmp --create-namespace","title":"Install SC4SNMP"},{"location":"offlineinstallation/offline-sc4snmp/#offline-sc4snmp-installation","text":"","title":"Offline SC4SNMP installation"},{"location":"offlineinstallation/offline-sc4snmp/#local-machine-with-internet-access","text":"To install the SC4SNMP offline, first, some packages must be downloaded from the Github release and then moved to the SC4SNMP installation server. Those packages are: dependencies-images.tar splunk-connect-for-snmp-chart.tar Additionally, you\u2019ll need pull_mibserver.sh script to easily pull and export mibserver image. Moreover, SC4SNMP Docker image must be pulled, saved as a .tar package, and then moved to the server as well. This process requires Docker to be installed locally. Images can be pulled from the following repository: ghcr.io/splunk/splunk-connect-for-snmp/container:<tag> . The latest tag can be found here under the Releases section with the label latest . Example of docker pull command: docker pull ghcr.io/splunk/splunk-connect-for-snmp/container:<tag> Then save the image. Directory where this image will be saved can be specified after the > sign: docker save ghcr.io/splunk/splunk-connect-for-snmp/container:<tag> > snmp_image.tar Another package you have to pull is the mibserver image. You can do it by executing pull_mibserver.sh script from the Release section, or copy-pasting its content. chmod a+x pull_mibserver.sh # you'll probably need to make file executable ./pull_mibserver.sh This script should produce mibserver.tar with the image of the mibserver inside. All four packages, mibserver.tar , snmp_image.tar , dependencies-images.tar , and splunk-connect-for-snmp-chart.tar , must be moved to the SC4SNMP installation server.","title":"Local machine with internet access"},{"location":"offlineinstallation/offline-sc4snmp/#installation-on-the-server","text":"On the server, all the images must be imported to the microk8s cluster. This can be done with the following command: microk8s ctr image import <name_of_tar_image> In case of this installation the following commands must be run: microk8s ctr image import dependencies-images.tar microk8s ctr image import snmp_image.tar microk8s ctr image import mibserver.tar Then create values.yaml . It\u2019s a little different from values.yaml used in an online installation. The difference between the two files is the following, which is used for automatic image pulling: image : pullPolicy : \"Never\" Example values.yaml file can be found here . The next step is to unpack the chart package splunk-connect-for-snmp-chart.tar . It will result in creating the splunk-connect-for-snmp directory: tar -xvf splunk-connect-for-snmp-chart.tar --exclude = '._*' Finally, run the helm install command in the directory where both the values.yaml and splunk-connect-for-snmp directories are located: microk8s helm3 install snmp -f values.yaml splunk-connect-for-snmp --namespace = sc4snmp --create-namespace","title":"Installation on the server"},{"location":"offlineinstallation/offline-sck/","text":"Splunk OpenTelemetry Collector for Kubernetes offline installation \u00b6 Local machine with internet access \u00b6 To install Splunk OpenTelemetry Collector offline first one must download packed chart splunk-otel-collector-<tag>.tgz and the otel image otel_image.tar from github release where <tag> is the current OpenTelemetry release tag. Both packages must be later moved to the installation server. Installation on the server \u00b6 Otel image has to be imported to the microk8s registry with: microk8s ctr image import otel_image.tar Imported package must be unpacked with the following command : tar -xvf splunk-otel-collector-<tag>.tgz --exclude = '._*' In order to run Splunk OpenTelemetry Collector on your environment, replace <> variables according to the description presented below microk8s helm3 install sck \\ --set = \"clusterName=<cluster_name>\" \\ --set = \"splunkPlatform.endpoint=<splunk_endpoint>\" \\ --set = \"splunkPlatform.insecureSkipVerify=<insecure_skip_verify>\" \\ --set = \"splunkPlatform.token=<splunk_token>\" \\ --set = \"logsEngine=otel\" \\ --set = \"splunkPlatform.metricsEnabled=true\" \\ --set = \"splunkPlatform.metricsIndex=em_metrics\" \\ --set = \"splunkPlatform.index=em_logs\" \\ splunk-otel-collector Variables description \u00b6 Placeholder Description Example splunk_endpoint host address of splunk instance https://endpoint.example.com:8088/services/collector insecure_skip_verify is insecure ssl allowed false splunk_token Splunk HTTP Event Collector token 450a69af-16a9-4f87-9628-c26f04ad3785 cluster_name name of the cluster my-cluster An example of filled up command is: microk8s helm3 install sck \\ --set = \"clusterName=my-cluster\" \\ --set = \"splunkPlatform.endpoint=https://endpoint.example.com/services/collector\" \\ --set = \"splunkPlatform.insecureSkipVerify=false\" \\ --set = \"splunkPlatform.token=4d22911c-18d9-4706-ae7b-dd1b976ca6f7\" \\ --set = \"splunkPlatform.metricsEnabled=true\" \\ --set = \"splunkPlatform.metricsIndex=em_metrics\" \\ --set = \"splunkPlatform.index=em_logs\" \\ splunk-otel-collector Install Splunk OpenTelemetry Collector with HELM for Splunk Observability for Kubernetes \u00b6 To run Splunk OpenTelemetry Collector on your environment, replace <> variables according to the description presented below microk8s helm3 install sck --set = \"clusterName=<cluster_name>\" --set = \"splunkObservability.realm=<realm>\" --set = \"splunkObservability.accessToken=<token>\" --set = \"splunkObservability.ingestUrl=<ingest_url>\" --set = \"splunkObservability.apiUrl=<api_url>\" --set = \"splunkObservability.metricsEnabled=true\" --set = \"splunkObservability.tracesEnabled=false\" --set = \"splunkObservability.logsEnabled=false\" splunk-otel-collector Variables description \u00b6 Placeholder Description Example cluster_name name of the cluster my_cluster realm Realm obtained from the Splunk Observability Cloud environment us0 token Token obtained from the Splunk Observability Cloud environment BCwaJ_Ands4Xh7Nrg ingest_url Ingest URL from the Splunk Observability Cloud environment https://ingest..signalfx.com api_url API URL from the Splunk Observability Cloud environment https://api..signalfx.com An example of filled up command is: microk8s helm3 install sck --set = \"clusterName=my_cluster\" --set = \"splunkObservability.realm=us0\" --set = \"splunkObservability.accessToken=BCwaJ_Ands4Xh7Nrg\" --set = \"splunkObservability.ingestUrl=https://ingest..signalfx.com\" --set = \"splunkObservability.apiUrl=https://api..signalfx.com\" --set = \"splunkObservability.metricsEnabled=true\" --set = \"splunkObservability.tracesEnabled=false\" --set = \"splunkObservability.logsEnabled=false\" splunk-otel-collector","title":"Install Splunk OpenTelemetry Collector for Kubernetes"},{"location":"offlineinstallation/offline-sck/#splunk-opentelemetry-collector-for-kubernetes-offline-installation","text":"","title":"Splunk OpenTelemetry Collector for Kubernetes offline installation"},{"location":"offlineinstallation/offline-sck/#local-machine-with-internet-access","text":"To install Splunk OpenTelemetry Collector offline first one must download packed chart splunk-otel-collector-<tag>.tgz and the otel image otel_image.tar from github release where <tag> is the current OpenTelemetry release tag. Both packages must be later moved to the installation server.","title":"Local machine with internet access"},{"location":"offlineinstallation/offline-sck/#installation-on-the-server","text":"Otel image has to be imported to the microk8s registry with: microk8s ctr image import otel_image.tar Imported package must be unpacked with the following command : tar -xvf splunk-otel-collector-<tag>.tgz --exclude = '._*' In order to run Splunk OpenTelemetry Collector on your environment, replace <> variables according to the description presented below microk8s helm3 install sck \\ --set = \"clusterName=<cluster_name>\" \\ --set = \"splunkPlatform.endpoint=<splunk_endpoint>\" \\ --set = \"splunkPlatform.insecureSkipVerify=<insecure_skip_verify>\" \\ --set = \"splunkPlatform.token=<splunk_token>\" \\ --set = \"logsEngine=otel\" \\ --set = \"splunkPlatform.metricsEnabled=true\" \\ --set = \"splunkPlatform.metricsIndex=em_metrics\" \\ --set = \"splunkPlatform.index=em_logs\" \\ splunk-otel-collector","title":"Installation on the server"},{"location":"offlineinstallation/offline-sck/#variables-description","text":"Placeholder Description Example splunk_endpoint host address of splunk instance https://endpoint.example.com:8088/services/collector insecure_skip_verify is insecure ssl allowed false splunk_token Splunk HTTP Event Collector token 450a69af-16a9-4f87-9628-c26f04ad3785 cluster_name name of the cluster my-cluster An example of filled up command is: microk8s helm3 install sck \\ --set = \"clusterName=my-cluster\" \\ --set = \"splunkPlatform.endpoint=https://endpoint.example.com/services/collector\" \\ --set = \"splunkPlatform.insecureSkipVerify=false\" \\ --set = \"splunkPlatform.token=4d22911c-18d9-4706-ae7b-dd1b976ca6f7\" \\ --set = \"splunkPlatform.metricsEnabled=true\" \\ --set = \"splunkPlatform.metricsIndex=em_metrics\" \\ --set = \"splunkPlatform.index=em_logs\" \\ splunk-otel-collector","title":"Variables description"},{"location":"offlineinstallation/offline-sck/#install-splunk-opentelemetry-collector-with-helm-for-splunk-observability-for-kubernetes","text":"To run Splunk OpenTelemetry Collector on your environment, replace <> variables according to the description presented below microk8s helm3 install sck --set = \"clusterName=<cluster_name>\" --set = \"splunkObservability.realm=<realm>\" --set = \"splunkObservability.accessToken=<token>\" --set = \"splunkObservability.ingestUrl=<ingest_url>\" --set = \"splunkObservability.apiUrl=<api_url>\" --set = \"splunkObservability.metricsEnabled=true\" --set = \"splunkObservability.tracesEnabled=false\" --set = \"splunkObservability.logsEnabled=false\" splunk-otel-collector","title":"Install Splunk OpenTelemetry Collector with HELM for Splunk Observability for Kubernetes"},{"location":"offlineinstallation/offline-sck/#variables-description_1","text":"Placeholder Description Example cluster_name name of the cluster my_cluster realm Realm obtained from the Splunk Observability Cloud environment us0 token Token obtained from the Splunk Observability Cloud environment BCwaJ_Ands4Xh7Nrg ingest_url Ingest URL from the Splunk Observability Cloud environment https://ingest..signalfx.com api_url API URL from the Splunk Observability Cloud environment https://api..signalfx.com An example of filled up command is: microk8s helm3 install sck --set = \"clusterName=my_cluster\" --set = \"splunkObservability.realm=us0\" --set = \"splunkObservability.accessToken=BCwaJ_Ands4Xh7Nrg\" --set = \"splunkObservability.ingestUrl=https://ingest..signalfx.com\" --set = \"splunkObservability.apiUrl=https://api..signalfx.com\" --set = \"splunkObservability.metricsEnabled=true\" --set = \"splunkObservability.tracesEnabled=false\" --set = \"splunkObservability.logsEnabled=false\" splunk-otel-collector","title":"Variables description"}]}